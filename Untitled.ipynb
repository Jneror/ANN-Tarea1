{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n",
      "       Unnamed: 0           0          1          2          3          4  \\\n",
      "0               0   73.516695  17.817765  12.469551  12.458130  12.454607   \n",
      "1               1   73.516695  20.649126  18.527789  17.891535  17.887995   \n",
      "2               2   73.516695  17.830377  12.512263  12.404775  12.394493   \n",
      "3               3   73.516695  17.875810  17.871259  17.862402  17.850920   \n",
      "4               4   73.516695  17.883818  17.868256  17.864221  17.818540   \n",
      "5               5   53.358707  17.038820  16.981436  16.167446  16.137631   \n",
      "6               6   53.358707  17.040919  16.975955  16.168874  16.131888   \n",
      "7               7   53.358707  15.190748  15.134397  15.078282  13.721467   \n",
      "8               8   73.516695  20.648642  18.559611  17.674347  16.152675   \n",
      "9               9   73.516695  17.563342  17.562598  12.653657  12.540799   \n",
      "10             10   73.516695  18.593826  17.902116  17.791648  17.709349   \n",
      "11             11   73.516695  17.893322  17.892435  17.891329  17.835056   \n",
      "12             12   73.516695  17.892036  17.835452  17.796401  12.544441   \n",
      "13             13  388.023441  66.102901  35.415029  35.414463  21.068417   \n",
      "14             14   73.516695  19.145558  17.855414  17.779134  12.927104   \n",
      "15             15   73.516695  20.686211  18.654076  18.029737  16.104124   \n",
      "16             16   73.516695  23.621854  23.620385  20.741845  18.712919   \n",
      "17             17   73.516695  20.882498  20.704776  18.830990  18.239768   \n",
      "18             18   53.358707  17.152774  16.979009  16.165980  16.131888   \n",
      "19             19   73.516695  20.792001  20.781995  18.699353  15.970705   \n",
      "20             20   73.516695  20.752137  19.148052  18.575154  17.872719   \n",
      "21             21  388.023441  29.794358  29.450794  19.838451  19.640707   \n",
      "22             22   73.516695  20.707815  17.725040  17.690824  12.899468   \n",
      "23             23   53.358707  17.139133  16.062083  15.810563  15.258986   \n",
      "24             24   53.358707  15.192917  15.192453  15.191957  13.696268   \n",
      "25             25   73.516695  20.722774  18.596453  17.866674  17.743940   \n",
      "26             26   73.516695  20.765497  19.143140  18.645819  17.736610   \n",
      "27             27   73.516695  20.800040  12.574005  12.558918  12.517330   \n",
      "28             28  388.023441  46.500552  46.493093  34.676868  29.085904   \n",
      "29             29   53.358707  18.092880  15.249478  15.177545  13.653715   \n",
      "...           ...         ...        ...        ...        ...        ...   \n",
      "16212       16243  388.023441  29.658517  29.655842  23.552305  23.550885   \n",
      "16213       16244   73.516695  18.635498  17.804251  15.228614  13.653764   \n",
      "16214       16245   53.358707  19.148633  13.653248  13.652988  13.652850   \n",
      "16215       16246   73.516695  20.698171  15.924907  15.855703  13.654009   \n",
      "16216       16247   73.516695  18.643484  18.640762  17.802607  17.802277   \n",
      "16217       16248   73.516695  17.870020  15.238941  15.176526  15.176523   \n",
      "16218       16249   53.358707  19.150693  16.405490  15.022080  14.895449   \n",
      "16219       16250   36.858105  14.107138  12.674530  12.544437  12.505901   \n",
      "16220       16251   73.516695  20.698855  20.698855  15.893020  15.892046   \n",
      "16221       16252   53.358707  15.754498  15.753712  13.653954  13.653736   \n",
      "16222       16253   73.516695  17.677578  12.519029  12.479941  12.477739   \n",
      "16223       16254   73.516695  20.743557  20.741361  18.598127  18.595527   \n",
      "16224       16255   73.516695  20.801457  20.801457  18.634187  18.634183   \n",
      "16225       16256   73.516695  20.581963  15.924696  15.751649  15.452521   \n",
      "16226       16257   53.358707  16.359301  16.358704  15.148839  13.825358   \n",
      "16227       16258   53.358707  16.435036  16.433452  15.192854  15.192072   \n",
      "16228       16259   36.858105  14.159026  13.653760  13.653143  13.653143   \n",
      "16229       16260   53.358707  16.525299  16.435115  14.202458  13.827072   \n",
      "16230       16261   73.516695  20.731531  20.731285  16.422832  16.422798   \n",
      "16231       16262   73.516695  20.221994  18.690270  18.632741  18.405014   \n",
      "16232       16263   73.516695  17.860880  17.780884  17.779987  12.511356   \n",
      "16233       16264  388.023441  46.723458  46.722364  28.505545  28.472771   \n",
      "16234       16265   73.516695  23.607539  23.606041  21.378625  17.851697   \n",
      "16235       16266   73.516695  20.643181  20.588968  18.703556  18.673772   \n",
      "16236       16267   73.516695  20.610314  20.554562  18.683667  18.653061   \n",
      "16237       16268   73.516695  20.753166  18.624076  17.872009  17.851690   \n",
      "16238       16269   73.516695  20.724740  18.579933  17.741621  14.716676   \n",
      "16239       16270   53.358707  20.820797  19.150234  19.148721  15.135514   \n",
      "16240       16271   53.358707  15.707759  15.707644  13.653838  13.653570   \n",
      "16241       16272   53.358707  15.708752  15.708094  13.653893  13.653176   \n",
      "\n",
      "               5          6          7          8    ...      1267  1268  \\\n",
      "0      12.447345  12.433065  12.426926  12.387474    ...       0.0   0.0   \n",
      "1      17.871731  17.852586  17.729842  15.864270    ...       0.0   0.0   \n",
      "2      12.391564  12.324461  12.238106  10.423249    ...       0.0   0.0   \n",
      "3      17.850440  12.558105  12.557645  12.517583    ...       0.0   0.0   \n",
      "4      12.508657  12.490519  12.450098  10.597068    ...       0.0   0.0   \n",
      "5      16.053239  15.713944  15.432893  15.421116    ...       0.0   0.0   \n",
      "6      16.073074  15.843838  15.638061  15.160532    ...       0.0   0.0   \n",
      "7      13.720334  13.671396  13.655370  13.654554    ...       0.0   0.0   \n",
      "8      14.266867  13.666125  13.657868  13.642132    ...       0.0   0.0   \n",
      "9      12.539160  12.536825  12.508203  12.489843    ...       0.0   0.0   \n",
      "10     17.659515  17.569676  17.188025  17.152301    ...       0.0   0.0   \n",
      "11     17.797049  17.796887  17.796494  12.519782    ...       0.0   0.0   \n",
      "12     12.520178  12.520042  12.501077  12.205003    ...       0.0   0.0   \n",
      "13     20.972084  18.855496  18.854617  18.833136    ...       0.0   0.0   \n",
      "14     12.337087  12.336829  12.276143  12.205558    ...       0.0   0.0   \n",
      "15     15.732605  15.382404  14.561514  13.653314    ...       0.0   0.0   \n",
      "16     15.755895  15.645016  15.533408  15.392356    ...       0.0   0.0   \n",
      "17     18.026817  14.817389  14.804453  14.229886    ...       0.0   0.0   \n",
      "18     16.122930  16.090475  15.157053  12.520047    ...       0.0   0.0   \n",
      "19     15.420394  14.874331  12.647200  12.647035    ...       0.0   0.0   \n",
      "20     17.850143  17.739271  14.677496  12.976874    ...       0.0   0.0   \n",
      "21     18.080196  15.228163  15.179768  15.179574    ...       0.0   0.0   \n",
      "22     12.809439  12.740222  12.667372  12.576495    ...       0.0   0.0   \n",
      "23     15.258296  13.676434  13.675309  13.654199    ...       0.0   0.0   \n",
      "24     13.695720  13.655091  13.653141  13.653066    ...       0.0   0.0   \n",
      "25     14.701212  13.740966  13.738704  13.654970    ...       0.0   0.0   \n",
      "26     17.699663  17.687518  14.701643  14.216043    ...       0.0   0.0   \n",
      "27     12.384754  12.355934  12.344451  12.316290    ...       0.0   0.0   \n",
      "28     19.000431  18.995270  18.631019  18.630794    ...       0.0   0.0   \n",
      "29     13.653147  13.652852  13.652687  13.652476    ...       0.0   0.0   \n",
      "...          ...        ...        ...        ...    ...       ...   ...   \n",
      "16212  23.446952  23.446603  23.424316  22.152848    ...       0.0   0.0   \n",
      "16213  13.653754  13.653413  13.652802  13.652570    ...       0.0   0.0   \n",
      "16214  13.652754  13.652495  13.652167  12.965136    ...       0.0   0.0   \n",
      "16215  13.653321  13.653014  13.652836  13.652436    ...       0.0   0.0   \n",
      "16216  13.653867  13.653335  13.652740  13.652676    ...       0.0   0.0   \n",
      "16217  12.525824  12.525722  12.507239  12.484568    ...       0.0   0.0   \n",
      "16218  12.521816  12.521367  12.488777  12.488520    ...       0.0   0.0   \n",
      "16219  12.480945  12.469779  12.345929  12.286274    ...       0.0   0.0   \n",
      "16220  15.875667  15.875528  13.653570  13.652995    ...       0.0   0.0   \n",
      "16221  13.653736  13.653482  13.653142  13.653000    ...       0.0   0.0   \n",
      "16222  12.452035  12.426147  12.421406  12.308018    ...       0.0   0.0   \n",
      "16223  14.582338  14.576939  12.993091  12.611977    ...       0.0   0.0   \n",
      "16224  14.566950  14.566947  14.254289  14.252941    ...       0.0   0.0   \n",
      "16225  14.852202  13.654823  13.653847  13.653692    ...       0.0   0.0   \n",
      "16226  13.825283  13.654598  13.653577  13.653275    ...       0.0   0.0   \n",
      "16227  15.191944  13.836318  13.787761  13.744369    ...       0.0   0.0   \n",
      "16228  13.652803  13.652756  13.652713  13.092874    ...       0.0   0.0   \n",
      "16229  13.743711  13.695585  13.694933  12.759605    ...       0.0   0.0   \n",
      "16230  16.136916  16.135040  15.893446  15.853311    ...       0.0   0.0   \n",
      "16231  17.735798  14.707738  13.842602  13.689474    ...       0.0   0.0   \n",
      "16232  12.510997  12.481791  12.475366  10.571260    ...       0.0   0.0   \n",
      "16233  19.796532  19.731292  14.209230  14.144322    ...       0.0   0.0   \n",
      "16234  15.649065  15.422739  13.654045  13.653121    ...       0.0   0.0   \n",
      "16235  14.895437  14.894662  13.684300  13.683890    ...       0.0   0.0   \n",
      "16236  14.895197  14.894752  13.684066  13.683760    ...       0.0   0.0   \n",
      "16237  17.851254  17.742176  14.655754  12.706683    ...       0.0   0.0   \n",
      "16238  13.697829  13.697558  13.653512  13.652942    ...       0.0   0.0   \n",
      "16239  15.123685  12.942704  12.938162  12.488633    ...       0.0   0.0   \n",
      "16240  13.653314  13.652591  13.652585  13.652550    ...       0.0   0.0   \n",
      "16241  13.653120  13.652930  13.652528  13.652322    ...       0.0   0.0   \n",
      "\n",
      "       1269  1270  1271  1272  1273  1274  pubchem_id        Eat  \n",
      "0       0.5   0.0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
      "1       0.0   0.0   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
      "2       0.0   0.0   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
      "3       0.0   0.0   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
      "4       0.0   0.0   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
      "5       0.0   0.0   0.0   0.0   0.0   0.0       25017 -16.169604  \n",
      "6       0.0   0.0   0.0   0.0   0.0   0.0       25019 -17.378477  \n",
      "7       0.0   0.0   0.0   0.0   0.0   0.0       25023 -15.673737  \n",
      "8       0.0   0.0   0.0   0.0   0.0   0.0       25032 -10.427851  \n",
      "9       0.0   0.0   0.0   0.0   0.0   0.0       25042  -8.744178  \n",
      "10      0.0   0.0   0.0   0.0   0.0   0.0       25051 -12.244325  \n",
      "11      0.0   0.0   0.0   0.0   0.0   0.0       25054 -14.149376  \n",
      "12      0.0   0.0   0.0   0.0   0.0   0.0       25055  -8.572029  \n",
      "13      0.0   0.0   0.0   0.0   0.0   0.0       25057 -13.696046  \n",
      "14      0.0   0.0   0.0   0.0   0.0   0.0       25066  -8.293733  \n",
      "15      0.0   0.0   0.0   0.0   0.0   0.0       25072 -11.248055  \n",
      "16      0.0   0.0   0.0   0.0   0.0   0.0       25089 -10.170327  \n",
      "17      0.0   0.0   0.0   0.0   0.0   0.0       25090 -11.116150  \n",
      "18      0.0   0.0   0.0   0.0   0.0   0.0       25093 -11.642478  \n",
      "19      0.0   0.0   0.0   0.0   0.0   0.0       25098  -8.329572  \n",
      "20      0.0   0.0   0.0   0.0   0.0   0.0       25106  -8.170857  \n",
      "21      0.0   0.0   0.0   0.0   0.0   0.0       25118 -12.796349  \n",
      "22      0.0   0.0   0.0   0.0   0.0   0.0       25121 -10.500986  \n",
      "23      0.0   0.0   0.0   0.0   0.0   0.0       25123 -11.520089  \n",
      "24      0.0   0.0   0.0   0.0   0.0   0.0       25125 -11.197458  \n",
      "25      0.0   0.0   0.0   0.0   0.0   0.0       25129 -13.505988  \n",
      "26      0.0   0.0   0.0   0.0   0.0   0.0       25131 -14.299903  \n",
      "27      0.0   0.0   0.0   0.0   0.0   0.0       25132 -11.331558  \n",
      "28      0.0   0.0   0.0   0.0   0.0   0.0       25141  -9.290619  \n",
      "29      0.0   0.0   0.0   0.0   0.0   0.0       25142  -9.586835  \n",
      "...     ...   ...   ...   ...   ...   ...         ...        ...  \n",
      "16212   0.0   0.0   0.0   0.0   0.0   0.0       74895  -6.849553  \n",
      "16213   0.0   0.0   0.0   0.0   0.0   0.0       74896 -10.408032  \n",
      "16214   0.0   0.0   0.0   0.0   0.0   0.0       74897 -10.100940  \n",
      "16215   0.0   0.0   0.0   0.0   0.0   0.0       74903 -11.170072  \n",
      "16216   0.0   0.0   0.0   0.0   0.0   0.0       74904 -11.163202  \n",
      "16217   0.0   0.0   0.0   0.0   0.0   0.0       74905 -15.003575  \n",
      "16218   0.0   0.0   0.0   0.0   0.0   0.0       74906 -11.932200  \n",
      "16219   0.0   0.0   0.0   0.0   0.0   0.0       74910 -11.987681  \n",
      "16220   0.0   0.0   0.0   0.0   0.0   0.0       74911 -11.908604  \n",
      "16221   0.0   0.0   0.0   0.0   0.0   0.0       74912 -12.330795  \n",
      "16222   0.0   0.0   0.0   0.0   0.0   0.0       74917 -13.007444  \n",
      "16223   0.0   0.0   0.0   0.0   0.0   0.0       74918 -15.191880  \n",
      "16224   0.0   0.0   0.0   0.0   0.0   0.0       74919  -6.746408  \n",
      "16225   0.0   0.0   0.0   0.0   0.0   0.0       74921 -11.988657  \n",
      "16226   0.0   0.0   0.0   0.0   0.0   0.0       74922 -10.752393  \n",
      "16227   0.0   0.0   0.0   0.0   0.0   0.0       74927 -12.044004  \n",
      "16228   0.0   0.0   0.0   0.0   0.0   0.0       74928 -10.988833  \n",
      "16229   0.0   0.0   0.0   0.0   0.0   0.0       74935 -10.635767  \n",
      "16230   0.0   0.0   0.0   0.0   0.0   0.0       74947  -8.633728  \n",
      "16231   0.0   0.0   0.0   0.0   0.0   0.0       74953 -11.206702  \n",
      "16232   0.0   0.0   0.0   0.0   0.0   0.0       74956  -6.823134  \n",
      "16233   0.0   0.0   0.0   0.0   0.0   0.0       74963 -11.964498  \n",
      "16234   0.0   0.0   0.0   0.0   0.0   0.0       74968  -8.918597  \n",
      "16235   0.0   0.0   0.0   0.0   0.0   0.0       74971 -11.794917  \n",
      "16236   0.0   0.0   0.0   0.0   0.0   0.0       74974 -11.809176  \n",
      "16237   0.0   0.0   0.0   0.0   0.0   0.0       74976  -8.876123  \n",
      "16238   0.0   0.0   0.0   0.0   0.0   0.0       74977 -13.105268  \n",
      "16239   0.0   0.0   0.0   0.0   0.0   0.0       74978 -16.801464  \n",
      "16240   0.0   0.0   0.0   0.0   0.0   0.0       74979 -13.335088  \n",
      "16241   0.0   0.0   0.0   0.0   0.0   0.0       74980 -13.336696  \n",
      "\n",
      "[16242 rows x 1278 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "print(datos)\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_train)\n",
    "#Transform training\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Transform val\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "y_val_scaled = X_val_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Transform test\n",
    "X_Test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "y_Test_scaled = X_Test_scaled.pop('Eat').values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Para el mismo problema definido anteriormente ([sección 1](#primero)) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (*loss*) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de *gradients* (__[link](https://www.tensorflow.org/api_docs/python/tf/keras/backend/gradients)__) en el *backend* de Keras. Deberá generar un **histograma** para todos los pesos de cada capa antes y despues del entrenamiento con 250 *epochs*. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-845cd172fdae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m  \u001b[0;31m###calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlistOfVariableTensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistOfVariableTensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We can now calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    " ###calculate gradients\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
