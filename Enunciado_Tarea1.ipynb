{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Y1tHj3DNqhX"
   },
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Entrenamiento de redes *Feed-Forward* vı́a GD y variantes (SGD, mini-*batches*), *momentum*, regularización y tasa de aprendizaje adaptiva.\n",
    "* Evaluación de redes *Feed-Forward* vı́a validación cruzada (cross-validation).\n",
    "* Rol de capas ocultas y mayor profundidad (*Deep Learning*).\n",
    "* Identificar el gradiente desvaneciente.\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: 26 de Octubre.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<margarita.bugueno.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<cvalle@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea1-INF395-II-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "#### Paquetes instalación\n",
    "\n",
    "Para poder trabajar en el curso se necesitará instalar librerías para Python, por lo que se recomienda instalarlas a través de anaconda (para Windows y sistemas Unix) en un entorno virtual, donde podrán elegir su versión de Python. Se instalarán librerías como sklearn, una librería simple y de facil acceso para data science, keras en su versión con GPU (para cálculo acelerado a través de la tarjeta gráfica), además de que ésta utiliza como backend TensorFlow o Theano, por lo que habrá que instalar alguno de éstos, además de las librerías básicas de computer science como *numpy, matplotlib, pandas,* además de claramente *jupyter*.\n",
    "\n",
    "* Descargar anacona\n",
    "* Luego de instalar Anaconda y tenerla en el path de su computador crear un entorno virtual:\n",
    "```\n",
    "conda create -n redesneuronales python=version\n",
    "```\n",
    "con version, la version de Python que desea utilizar. Si está en Windows, se recomienda Python 3 debido a dependencias con una de las librerías a utilziar.\n",
    "\n",
    "* Acceder al ambiente creado\n",
    "```\n",
    "source activate redesneuronales\n",
    "```\n",
    "\n",
    "* Instalar los paquetes a utilizar\n",
    "```\n",
    "conda install jupyter sklearn numpy pandas matplotlib keras-gpu tensorflow-gpu\n",
    "```\n",
    "\n",
    "*  Para salir del entorno\n",
    "```\n",
    "source deactivate redesneuronales\n",
    "```\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Predicción de Entalpía de Atomización  \n",
    "[2.](#segundo) *Deep Networks*  \n",
    "[3.](#tercero) Entendimiento de imágenes de personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n",
      "       Unnamed: 0           0          1          2          3          4  \\\n",
      "0               0   73.516695  17.817765  12.469551  12.458130  12.454607   \n",
      "1               1   73.516695  20.649126  18.527789  17.891535  17.887995   \n",
      "2               2   73.516695  17.830377  12.512263  12.404775  12.394493   \n",
      "3               3   73.516695  17.875810  17.871259  17.862402  17.850920   \n",
      "4               4   73.516695  17.883818  17.868256  17.864221  17.818540   \n",
      "5               5   53.358707  17.038820  16.981436  16.167446  16.137631   \n",
      "6               6   53.358707  17.040919  16.975955  16.168874  16.131888   \n",
      "7               7   53.358707  15.190748  15.134397  15.078282  13.721467   \n",
      "8               8   73.516695  20.648642  18.559611  17.674347  16.152675   \n",
      "9               9   73.516695  17.563342  17.562598  12.653657  12.540799   \n",
      "10             10   73.516695  18.593826  17.902116  17.791648  17.709349   \n",
      "11             11   73.516695  17.893322  17.892435  17.891329  17.835056   \n",
      "12             12   73.516695  17.892036  17.835452  17.796401  12.544441   \n",
      "13             13  388.023441  66.102901  35.415029  35.414463  21.068417   \n",
      "14             14   73.516695  19.145558  17.855414  17.779134  12.927104   \n",
      "15             15   73.516695  20.686211  18.654076  18.029737  16.104124   \n",
      "16             16   73.516695  23.621854  23.620385  20.741845  18.712919   \n",
      "17             17   73.516695  20.882498  20.704776  18.830990  18.239768   \n",
      "18             18   53.358707  17.152774  16.979009  16.165980  16.131888   \n",
      "19             19   73.516695  20.792001  20.781995  18.699353  15.970705   \n",
      "20             20   73.516695  20.752137  19.148052  18.575154  17.872719   \n",
      "21             21  388.023441  29.794358  29.450794  19.838451  19.640707   \n",
      "22             22   73.516695  20.707815  17.725040  17.690824  12.899468   \n",
      "23             23   53.358707  17.139133  16.062083  15.810563  15.258986   \n",
      "24             24   53.358707  15.192917  15.192453  15.191957  13.696268   \n",
      "25             25   73.516695  20.722774  18.596453  17.866674  17.743940   \n",
      "26             26   73.516695  20.765497  19.143140  18.645819  17.736610   \n",
      "27             27   73.516695  20.800040  12.574005  12.558918  12.517330   \n",
      "28             28  388.023441  46.500552  46.493093  34.676868  29.085904   \n",
      "29             29   53.358707  18.092880  15.249478  15.177545  13.653715   \n",
      "...           ...         ...        ...        ...        ...        ...   \n",
      "16212       16243  388.023441  29.658517  29.655842  23.552305  23.550885   \n",
      "16213       16244   73.516695  18.635498  17.804251  15.228614  13.653764   \n",
      "16214       16245   53.358707  19.148633  13.653248  13.652988  13.652850   \n",
      "16215       16246   73.516695  20.698171  15.924907  15.855703  13.654009   \n",
      "16216       16247   73.516695  18.643484  18.640762  17.802607  17.802277   \n",
      "16217       16248   73.516695  17.870020  15.238941  15.176526  15.176523   \n",
      "16218       16249   53.358707  19.150693  16.405490  15.022080  14.895449   \n",
      "16219       16250   36.858105  14.107138  12.674530  12.544437  12.505901   \n",
      "16220       16251   73.516695  20.698855  20.698855  15.893020  15.892046   \n",
      "16221       16252   53.358707  15.754498  15.753712  13.653954  13.653736   \n",
      "16222       16253   73.516695  17.677578  12.519029  12.479941  12.477739   \n",
      "16223       16254   73.516695  20.743557  20.741361  18.598127  18.595527   \n",
      "16224       16255   73.516695  20.801457  20.801457  18.634187  18.634183   \n",
      "16225       16256   73.516695  20.581963  15.924696  15.751649  15.452521   \n",
      "16226       16257   53.358707  16.359301  16.358704  15.148839  13.825358   \n",
      "16227       16258   53.358707  16.435036  16.433452  15.192854  15.192072   \n",
      "16228       16259   36.858105  14.159026  13.653760  13.653143  13.653143   \n",
      "16229       16260   53.358707  16.525299  16.435115  14.202458  13.827072   \n",
      "16230       16261   73.516695  20.731531  20.731285  16.422832  16.422798   \n",
      "16231       16262   73.516695  20.221994  18.690270  18.632741  18.405014   \n",
      "16232       16263   73.516695  17.860880  17.780884  17.779987  12.511356   \n",
      "16233       16264  388.023441  46.723458  46.722364  28.505545  28.472771   \n",
      "16234       16265   73.516695  23.607539  23.606041  21.378625  17.851697   \n",
      "16235       16266   73.516695  20.643181  20.588968  18.703556  18.673772   \n",
      "16236       16267   73.516695  20.610314  20.554562  18.683667  18.653061   \n",
      "16237       16268   73.516695  20.753166  18.624076  17.872009  17.851690   \n",
      "16238       16269   73.516695  20.724740  18.579933  17.741621  14.716676   \n",
      "16239       16270   53.358707  20.820797  19.150234  19.148721  15.135514   \n",
      "16240       16271   53.358707  15.707759  15.707644  13.653838  13.653570   \n",
      "16241       16272   53.358707  15.708752  15.708094  13.653893  13.653176   \n",
      "\n",
      "               5          6          7          8    ...      1267  1268  \\\n",
      "0      12.447345  12.433065  12.426926  12.387474    ...       0.0   0.0   \n",
      "1      17.871731  17.852586  17.729842  15.864270    ...       0.0   0.0   \n",
      "2      12.391564  12.324461  12.238106  10.423249    ...       0.0   0.0   \n",
      "3      17.850440  12.558105  12.557645  12.517583    ...       0.0   0.0   \n",
      "4      12.508657  12.490519  12.450098  10.597068    ...       0.0   0.0   \n",
      "5      16.053239  15.713944  15.432893  15.421116    ...       0.0   0.0   \n",
      "6      16.073074  15.843838  15.638061  15.160532    ...       0.0   0.0   \n",
      "7      13.720334  13.671396  13.655370  13.654554    ...       0.0   0.0   \n",
      "8      14.266867  13.666125  13.657868  13.642132    ...       0.0   0.0   \n",
      "9      12.539160  12.536825  12.508203  12.489843    ...       0.0   0.0   \n",
      "10     17.659515  17.569676  17.188025  17.152301    ...       0.0   0.0   \n",
      "11     17.797049  17.796887  17.796494  12.519782    ...       0.0   0.0   \n",
      "12     12.520178  12.520042  12.501077  12.205003    ...       0.0   0.0   \n",
      "13     20.972084  18.855496  18.854617  18.833136    ...       0.0   0.0   \n",
      "14     12.337087  12.336829  12.276143  12.205558    ...       0.0   0.0   \n",
      "15     15.732605  15.382404  14.561514  13.653314    ...       0.0   0.0   \n",
      "16     15.755895  15.645016  15.533408  15.392356    ...       0.0   0.0   \n",
      "17     18.026817  14.817389  14.804453  14.229886    ...       0.0   0.0   \n",
      "18     16.122930  16.090475  15.157053  12.520047    ...       0.0   0.0   \n",
      "19     15.420394  14.874331  12.647200  12.647035    ...       0.0   0.0   \n",
      "20     17.850143  17.739271  14.677496  12.976874    ...       0.0   0.0   \n",
      "21     18.080196  15.228163  15.179768  15.179574    ...       0.0   0.0   \n",
      "22     12.809439  12.740222  12.667372  12.576495    ...       0.0   0.0   \n",
      "23     15.258296  13.676434  13.675309  13.654199    ...       0.0   0.0   \n",
      "24     13.695720  13.655091  13.653141  13.653066    ...       0.0   0.0   \n",
      "25     14.701212  13.740966  13.738704  13.654970    ...       0.0   0.0   \n",
      "26     17.699663  17.687518  14.701643  14.216043    ...       0.0   0.0   \n",
      "27     12.384754  12.355934  12.344451  12.316290    ...       0.0   0.0   \n",
      "28     19.000431  18.995270  18.631019  18.630794    ...       0.0   0.0   \n",
      "29     13.653147  13.652852  13.652687  13.652476    ...       0.0   0.0   \n",
      "...          ...        ...        ...        ...    ...       ...   ...   \n",
      "16212  23.446952  23.446603  23.424316  22.152848    ...       0.0   0.0   \n",
      "16213  13.653754  13.653413  13.652802  13.652570    ...       0.0   0.0   \n",
      "16214  13.652754  13.652495  13.652167  12.965136    ...       0.0   0.0   \n",
      "16215  13.653321  13.653014  13.652836  13.652436    ...       0.0   0.0   \n",
      "16216  13.653867  13.653335  13.652740  13.652676    ...       0.0   0.0   \n",
      "16217  12.525824  12.525722  12.507239  12.484568    ...       0.0   0.0   \n",
      "16218  12.521816  12.521367  12.488777  12.488520    ...       0.0   0.0   \n",
      "16219  12.480945  12.469779  12.345929  12.286274    ...       0.0   0.0   \n",
      "16220  15.875667  15.875528  13.653570  13.652995    ...       0.0   0.0   \n",
      "16221  13.653736  13.653482  13.653142  13.653000    ...       0.0   0.0   \n",
      "16222  12.452035  12.426147  12.421406  12.308018    ...       0.0   0.0   \n",
      "16223  14.582338  14.576939  12.993091  12.611977    ...       0.0   0.0   \n",
      "16224  14.566950  14.566947  14.254289  14.252941    ...       0.0   0.0   \n",
      "16225  14.852202  13.654823  13.653847  13.653692    ...       0.0   0.0   \n",
      "16226  13.825283  13.654598  13.653577  13.653275    ...       0.0   0.0   \n",
      "16227  15.191944  13.836318  13.787761  13.744369    ...       0.0   0.0   \n",
      "16228  13.652803  13.652756  13.652713  13.092874    ...       0.0   0.0   \n",
      "16229  13.743711  13.695585  13.694933  12.759605    ...       0.0   0.0   \n",
      "16230  16.136916  16.135040  15.893446  15.853311    ...       0.0   0.0   \n",
      "16231  17.735798  14.707738  13.842602  13.689474    ...       0.0   0.0   \n",
      "16232  12.510997  12.481791  12.475366  10.571260    ...       0.0   0.0   \n",
      "16233  19.796532  19.731292  14.209230  14.144322    ...       0.0   0.0   \n",
      "16234  15.649065  15.422739  13.654045  13.653121    ...       0.0   0.0   \n",
      "16235  14.895437  14.894662  13.684300  13.683890    ...       0.0   0.0   \n",
      "16236  14.895197  14.894752  13.684066  13.683760    ...       0.0   0.0   \n",
      "16237  17.851254  17.742176  14.655754  12.706683    ...       0.0   0.0   \n",
      "16238  13.697829  13.697558  13.653512  13.652942    ...       0.0   0.0   \n",
      "16239  15.123685  12.942704  12.938162  12.488633    ...       0.0   0.0   \n",
      "16240  13.653314  13.652591  13.652585  13.652550    ...       0.0   0.0   \n",
      "16241  13.653120  13.652930  13.652528  13.652322    ...       0.0   0.0   \n",
      "\n",
      "       1269  1270  1271  1272  1273  1274  pubchem_id        Eat  \n",
      "0       0.5   0.0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
      "1       0.0   0.0   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
      "2       0.0   0.0   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
      "3       0.0   0.0   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
      "4       0.0   0.0   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
      "5       0.0   0.0   0.0   0.0   0.0   0.0       25017 -16.169604  \n",
      "6       0.0   0.0   0.0   0.0   0.0   0.0       25019 -17.378477  \n",
      "7       0.0   0.0   0.0   0.0   0.0   0.0       25023 -15.673737  \n",
      "8       0.0   0.0   0.0   0.0   0.0   0.0       25032 -10.427851  \n",
      "9       0.0   0.0   0.0   0.0   0.0   0.0       25042  -8.744178  \n",
      "10      0.0   0.0   0.0   0.0   0.0   0.0       25051 -12.244325  \n",
      "11      0.0   0.0   0.0   0.0   0.0   0.0       25054 -14.149376  \n",
      "12      0.0   0.0   0.0   0.0   0.0   0.0       25055  -8.572029  \n",
      "13      0.0   0.0   0.0   0.0   0.0   0.0       25057 -13.696046  \n",
      "14      0.0   0.0   0.0   0.0   0.0   0.0       25066  -8.293733  \n",
      "15      0.0   0.0   0.0   0.0   0.0   0.0       25072 -11.248055  \n",
      "16      0.0   0.0   0.0   0.0   0.0   0.0       25089 -10.170327  \n",
      "17      0.0   0.0   0.0   0.0   0.0   0.0       25090 -11.116150  \n",
      "18      0.0   0.0   0.0   0.0   0.0   0.0       25093 -11.642478  \n",
      "19      0.0   0.0   0.0   0.0   0.0   0.0       25098  -8.329572  \n",
      "20      0.0   0.0   0.0   0.0   0.0   0.0       25106  -8.170857  \n",
      "21      0.0   0.0   0.0   0.0   0.0   0.0       25118 -12.796349  \n",
      "22      0.0   0.0   0.0   0.0   0.0   0.0       25121 -10.500986  \n",
      "23      0.0   0.0   0.0   0.0   0.0   0.0       25123 -11.520089  \n",
      "24      0.0   0.0   0.0   0.0   0.0   0.0       25125 -11.197458  \n",
      "25      0.0   0.0   0.0   0.0   0.0   0.0       25129 -13.505988  \n",
      "26      0.0   0.0   0.0   0.0   0.0   0.0       25131 -14.299903  \n",
      "27      0.0   0.0   0.0   0.0   0.0   0.0       25132 -11.331558  \n",
      "28      0.0   0.0   0.0   0.0   0.0   0.0       25141  -9.290619  \n",
      "29      0.0   0.0   0.0   0.0   0.0   0.0       25142  -9.586835  \n",
      "...     ...   ...   ...   ...   ...   ...         ...        ...  \n",
      "16212   0.0   0.0   0.0   0.0   0.0   0.0       74895  -6.849553  \n",
      "16213   0.0   0.0   0.0   0.0   0.0   0.0       74896 -10.408032  \n",
      "16214   0.0   0.0   0.0   0.0   0.0   0.0       74897 -10.100940  \n",
      "16215   0.0   0.0   0.0   0.0   0.0   0.0       74903 -11.170072  \n",
      "16216   0.0   0.0   0.0   0.0   0.0   0.0       74904 -11.163202  \n",
      "16217   0.0   0.0   0.0   0.0   0.0   0.0       74905 -15.003575  \n",
      "16218   0.0   0.0   0.0   0.0   0.0   0.0       74906 -11.932200  \n",
      "16219   0.0   0.0   0.0   0.0   0.0   0.0       74910 -11.987681  \n",
      "16220   0.0   0.0   0.0   0.0   0.0   0.0       74911 -11.908604  \n",
      "16221   0.0   0.0   0.0   0.0   0.0   0.0       74912 -12.330795  \n",
      "16222   0.0   0.0   0.0   0.0   0.0   0.0       74917 -13.007444  \n",
      "16223   0.0   0.0   0.0   0.0   0.0   0.0       74918 -15.191880  \n",
      "16224   0.0   0.0   0.0   0.0   0.0   0.0       74919  -6.746408  \n",
      "16225   0.0   0.0   0.0   0.0   0.0   0.0       74921 -11.988657  \n",
      "16226   0.0   0.0   0.0   0.0   0.0   0.0       74922 -10.752393  \n",
      "16227   0.0   0.0   0.0   0.0   0.0   0.0       74927 -12.044004  \n",
      "16228   0.0   0.0   0.0   0.0   0.0   0.0       74928 -10.988833  \n",
      "16229   0.0   0.0   0.0   0.0   0.0   0.0       74935 -10.635767  \n",
      "16230   0.0   0.0   0.0   0.0   0.0   0.0       74947  -8.633728  \n",
      "16231   0.0   0.0   0.0   0.0   0.0   0.0       74953 -11.206702  \n",
      "16232   0.0   0.0   0.0   0.0   0.0   0.0       74956  -6.823134  \n",
      "16233   0.0   0.0   0.0   0.0   0.0   0.0       74963 -11.964498  \n",
      "16234   0.0   0.0   0.0   0.0   0.0   0.0       74968  -8.918597  \n",
      "16235   0.0   0.0   0.0   0.0   0.0   0.0       74971 -11.794917  \n",
      "16236   0.0   0.0   0.0   0.0   0.0   0.0       74974 -11.809176  \n",
      "16237   0.0   0.0   0.0   0.0   0.0   0.0       74976  -8.876123  \n",
      "16238   0.0   0.0   0.0   0.0   0.0   0.0       74977 -13.105268  \n",
      "16239   0.0   0.0   0.0   0.0   0.0   0.0       74978 -16.801464  \n",
      "16240   0.0   0.0   0.0   0.0   0.0   0.0       74979 -13.335088  \n",
      "16241   0.0   0.0   0.0   0.0   0.0   0.0       74980 -13.336696  \n",
      "\n",
      "[16242 rows x 1278 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "print(datos)\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Predicción de Entalpía de Atomización\n",
    "\n",
    "\n",
    "Las simulaciones de propiedades moleculares son computacionalmente costosas y requieren de un arduo trabajo científico. El objetivo de esta sección corresponde a la utilización de métodos de aprendizaje automático supervisado (Redes Neuronales Artificiales) para predecir propiedades moleculares, en este caso la Energía de Atomización o Entalpía de Atomización, a partir de una base de datos de simulaciones obtenida mediante __[Quantum Espresso](http://www.quantum-espresso.org/)__. Si esto se lograse hacer con gran precisión, se abrirían muchas posibilidades en el diseño computacional y el descubrimiento de nuevas moléculas, compuestos y fármacos.\n",
    "\n",
    "<img src=\"https://pubs.rsc.org/services/images/RSCpubs.ePlatform.Service.FreeContent.ImageService.svc/ImageService/Articleimage/2012/NR/c2nr11543c/c2nr11543c-f4.gif\" title=\"Title text\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "La **entalpía de atomización** es la cantidad de variación de entalpía cuando los enlaces de un compuesto se rompen y los componentes se reducen a átomos individuales. Tal como se ha indicado, su tarea es la de predecir dicho nivel a partir de los atributos enunciados en el dataset puesto a vuestra disposición en *moodle*.\n",
    "\n",
    "> a) Construya un *dataframe* con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas.\n",
    "```python\n",
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "print(datos)\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante\n",
    "```\n",
    ">> a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. **Explique por qué se aconseja dicho preprocesamiento**\n",
    ">```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "...\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "...\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "```\n",
    "\n",
    " \n",
    ">b) Muestre en un gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de *epochs* de entrenamiento, para una red *feedforward* de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrene la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "```\n",
    "> c) Repita el paso anterior, utilizado ’**ReLU**’ como función de activación y compare con lo obtenido en b).  \n",
    "\n",
    "> d) Repita b) y c) variando la tasa de aprendizaje (*learning rate*) en un rango sensible. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento.\n",
    "```python\n",
    "import numpy as np\n",
    "n_lr = 20\n",
    "lear_rate = np.linspace(0,1,n_lr)\n",
    "```\n",
    "\n",
    "> e) Entrene los modelos considerados en b) y c) usando *progressive decay*. Compare y comente.\n",
    "```python\n",
    "n_decay = 10\n",
    "lear_decay = np.logspace(-6,0,n_decay)\n",
    "sgd = SGD(lr=0.2, decay=1e-6)\n",
    "```\n",
    "\n",
    "> f) Entrene los modelos considerados en b) y c) utilizando SGD en mini-*batches*. Experimente con diferentes tamaños del *batch*. Comente.\n",
    "```python\n",
    "n_batches = 21\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "```\n",
    "> g) Entrene los modelos obtenidos en b) y c) utilizando estrategias modernas para adaptar la tasa de aprendizaje. Compare los desempeños de adagrad, adadelta, RMSprop y adam. ¿Se observa en algún caso un mejor resultado final? ¿Se observa en algún caso una mayor velocidad de convergencia sobre el dataset de entrenamiento? ¿Sobre el dataset de validación?\n",
    "```python\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "moptimizer = Adagrad(lr=0.01)\n",
    "model.compile(optimizer=moptimizer)\n",
    "model.fit(X_train_scaled,y_train,batch_size=bs,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "```\n",
    "\n",
    "> h) Entrene los modelos obtenidos en b) y c) utilizando regularizadores $l_1$ y $l_2$ (*weight decay*). Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente. Además evalúe el efecto de regularizar solo la primera capa *vs* la segunda, comente.\n",
    "```python\n",
    "model = Sequential()\n",
    "...#la regularization se debe incorporar a cada capa separadamente\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(256,input_dim=idim,kernel_initializer='uniform',W_regularizer=l2(0.01)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=l2(0.01)))\n",
    "model.add(Activation('linear'))\n",
    "```\n",
    "\n",
    "> i) Entrene los modelos obtenidos en b) y c) utilizando *Dropout*. Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente.\n",
    "```python\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "...\n",
    "model.add(Dense(256,kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "...\n",
    "```\n",
    "\n",
    "> j) Fijando todos los demás hiper-parámetros del modelo definido en b) y en c), utilice validación cruzada con un número de *folds* igual a *K* = 5 y *K*=10 para determinar el mejor valor correspondiente a un parámetro que usted elija (tasa de aprendizaje, número de neuronas, parámetro de regularización, etc) ¿El mejor parámetro para la red con sigmoidal es distinto que para ReLU? ¿Porqué sucede? Además mida el error real del modelo sobre el conjunto de pruebas, compare y concluya.\n",
    "```python\n",
    "from sklearn import cross_validation\n",
    "Xm = X_train_scaled.values\n",
    "ym = y_train\n",
    "kfold = cross_validation.KFold(len(Xm), 10)\n",
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    ...# create model\n",
    "    model = #model with hiperparam\n",
    "    ...# Compile model\n",
    "    model.compile(optimizer=,loss='mean_squared_error')\n",
    "    ...# Fit the model\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    ...# evaluate the model\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFtCUBp9Nqhb"
   },
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n",
    "```python\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "...#transform val and test\n",
    "```\n",
    "\n",
    "> b) Para el mismo problema definido anteriormente ([sección 1](#primero)) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (*loss*) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de *gradients* (__[link](https://www.tensorflow.org/api_docs/python/tf/keras/backend/gradients)__) en el *backend* de Keras. Deberá generar un **histograma** para todos los pesos de cada capa antes y despues del entrenamiento con 250 *epochs*. Comente.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "- ###calculate gradients\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "```\n",
    "\n",
    "> c) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento pero ahora entrenando una red mucho más profunda de 6 capas, 5 capas escondidas y 1 de salida. Utilice el inicializador de pesos *uniform* el cual inicializa mediante una distribución uniforme entre $-1/\\sqrt{N}$ y $1/\\sqrt{N}$ para cada capa, con $N$ el número de neuronas de la capa anterior. Por simplicidad visualice las 3-4 primeras capas de la red. Comente si observa el efecto del *gradiente desvaneciente* antes y/o después de entrenar.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "> d) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento, pero ahora entrenando la red profunda con el inicializador de Glorot [[1]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/(N_{in}+N_{out})}$  y $\\sqrt{6/(N_{in}+N_{out})}$ . Por simplicidad visualice las 3-4 primeras capas de la red. Comente si el efecto del *gradiente desvaneciente* se amortigua antes y/o después de entrenar.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "> e) Vuelva a repetir la experimentación ahora cambiando la función de activación por ReLU, es decir, deberá visualizar los gradientes de los pesos de cada capa antes y después del entrenamiento, con inicialización *uniform* y comparar con la inicialización de He [[2]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/N_{in}}$ y $\\sqrt{6/N_{in}} $. Comente si ocurre el mismo fenómeno anterior (para función sigmoidal) sobre el efecto del *gradiente desvaneciente* para la función ReLU. Explique la importancia de la inicialización de los pesos dependiendo de la arquitectura.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "...\n",
    "or\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='relu')) #he\n",
    "...\n",
    "```\n",
    "> f) ¿Qué es lo que sucede con la red más profunda? ¿El modelo logra convergencia en su entrenamiento? Modifique aspectos estructurales (funciones de activación, inicializadores, regularización, *momentum*, variación de tasa de aprendizaje, entre otros) de la red profunda de 6 capas definida anteriormente (no modifique la profundidad ni el número de neuronas) para lograr un error cuadrático medio (*mse*) similar o menor al de una red no profunda, como la definida en b) en esta sección, sobre el conjunto de pruebas.\n",
    "\n",
    "> g) Experimente con la utilización de una función activación auxiliar (debido a que aproxima) a '**ReLU**' y que es continua derivable (**softplus**) ¿Cuál es el beneficio de ésta con respecto ReLU? Comente.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='softplus')) #softplus\n",
    "...\n",
    "```\n",
    "\n",
    "> h) Pruebe con utilizar una red *shallow* (poco profunda), es decir, sitúe todas las neuronas en una única capa ¿Qué sucede con la convergencia del algoritmo? ¿Por qué sucede este fenómeno?\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=X_train_scaled.shape[1], kernel_initializer='choose',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='choose',activation='linear'))\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "model.fit(X_train_scaled.values, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled.values, y_val_scaled))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLGc9XcDNqhi"
   },
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Entendimiento de imágenes de personas\n",
    "\n",
    "El problema de inferir ciertas características de una persona a través de una foto de ella puede resultar bastante dificil incluso para nosotros, como por ejemplo de qué país es, la emoción que expresa, la edad que tiene, o el género. La automatización de este proceso para que máquinas logren identificar ciertas características de una persona puede ser algo crucial para el futuro desarrollo de Inteligencia Artificial.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/6B072GE.jpg\" width=\"60%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "En esta actividad trabajaremos con unos datos (imágenes) con la tarea de predecir la **edad** (*target value*) de la persona en la imagen. Los datos con corresponden a 3640 imágenes de Flickr de rostros de personas, pero, debido a que trabajamos con redes *feed forward*, se trabajará con representaciones de características extraídas. Para ésto necesitará descargar los datos del siguiente __[link](http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html)__ en el extracto de *ageGenderClassification* o a través de la consola Unix.\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/ageGenderClassification.zip\n",
    "```\n",
    "\n",
    "Se trabajará con archivos *.mat* que pueden ser cargados de la siguiente manera:\n",
    "```python\n",
    "import scipy.io as sio\n",
    "sio.loadmat(\"file.mat\")\n",
    "```\n",
    "\n",
    "Para descripción sobre las columnas están en el archivo readme a través del siguiente __[link](http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt)__ o a través de la consola Unix:\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt\n",
    "```\n",
    "\n",
    "\n",
    "> a) Cargue los datos dos dataset de entrenamiento y de pruebas ¿Cuántos datos hay en cada conjunto?\n",
    "```python\n",
    "import scipy.io as sio\n",
    "mat_train = sio.loadmat(\"./eventrain.mat\")\n",
    "mat_test = sio.loadmat(\"./eventest.mat\")\n",
    "data_train= mat_train[\"trcoll\"][0][0]\n",
    "data_test= mat_test[\"tecoll\"][0][0]\n",
    "```\n",
    "\n",
    "> b) Eliga cuál representación utilizará para trabajar los datos y entregárselos como *input* al modelo neuronal denso. Además extraiga las etiquetas del problema. Describa los datos utilziados.\n",
    "```python\n",
    "genFeat = data[0]  #it can be used as representation: contextual features\n",
    "ageClass = data[1] #target\n",
    "ffcoefs = data[3]   #it can be used as representation: fisherface space\n",
    "faceGist = data[4]  #it can be used as representation\n",
    "```\n",
    "\n",
    "> c) Defina y entrene una modelo de red neuronal *feed forward* para la inferencia de la edad de la persona a través de la representación escogida. Intente llegar a un *mse* menor a 100 en el conjunto de pruebas. Recuerde que **NO** puede seleccionar modelos a través del conjunto de pruebas. Visualice sus resultados si estima conveniente.\n",
    "\n",
    "\n",
    "*Nota: Puede notar que la cantidad de edades presentes en el problema son pocas (1,  5, 10, 16, 28, 51 o 75 años), por lo que puede tratar al problema así como de regresión o clasificación (considerando cada edad como una clase)*\n",
    "\n",
    "\n",
    "#### Ayuda:\n",
    "> Para problemas de clasificación de múltiples clases es necesario transformar las etiquetas categóricas en *one hot vector*, donde cada columna del vector representará una categoría. Por ejemplo, si existen tres categorías (perro, gato, ratón), la categoría perro puede ser codificada como [1,0,0], y la categoría ratón puede ser codificada como [0,0,1]. Para ésto la librería *keras* nos ayuda:\n",
    "\n",
    "```python\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train,num_classes=edades_distintas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAYlvfT-Nqhw"
   },
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] Glorot, X., & Bengio, Y. (2010, March). *Understanding the difficulty of training deep feedforward neural networks*. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).    \n",
    "[2]  He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Delving deep into rectifiers: Surpassing human-level performance on imagenet classification*. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).  \n",
    "[3] Gallagher, A. C., & Chen, T. (2009, June). *Understanding images of groups of people*. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 256-263). IEEE."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Enunciado.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
