{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Construya un dataframe con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas**\n",
    "\n",
    "Las columnas 'Unnamed: 0' y 'pubchem_id' se eliminan por no tener datos relevante para los conjuntos de entrenamiento, validación y testeo. 'Unnamed: 0' es la columna que enumera las filas, por lo que no es necesaria debido a que el dataframe ya tiene una enumeración. 'pubchem_id' es una identificación del test, por lo que tampoco es relevante para las mediciones.\n"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "print(datos.describe())\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True) # con la propiedad drop elimina columnas\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante\n",
    "print (\"Imprimir dataset\")\n",
    "print (df_test)\n",
    "print (total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. Explique por qué se aconseja dicho preprocesamiento**\n",
    "\n",
    "Para un mejor funcionamiento de los algoritmos de Machine learning, hay que normalizar las variables de entrada del algoritmo, Normalizar, hace referencia a extender o comprimir los valores de una variable para estar en un rango definido. Es decir, realiza una ponderación de las caracterisiticas de una mejor manera y ademas se reduce el facor de escala. Sin embargo, realizar una mala eleccion del metodo de normalización puede alterar los resultados del analisis de datos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "             0         1         2         3         4         5         6  \\\n",
      "0    -0.351225 -0.509371 -1.033927 -0.919025 -0.816522 -0.768547 -0.658326   \n",
      "1    -0.351225 -0.173570 -0.217847 -0.065078  0.166757  0.531135  0.733115   \n",
      "2    -0.351225 -0.507875 -1.028174 -0.927411 -0.827401 -0.781912 -0.686210   \n",
      "3    -0.351225 -0.502487 -0.306285 -0.069657  0.160048  0.526034 -0.626222   \n",
      "4    -0.351225 -0.501537 -0.306690 -0.069371  0.154188 -0.753856 -0.643575   \n",
      "5    -0.534463 -0.601755 -0.426150 -0.336046 -0.150006  0.095425  0.184027   \n",
      "6    -0.534463 -0.601506 -0.426888 -0.335822 -0.151045  0.100177  0.217377   \n",
      "7    -0.534463 -0.820937 -0.674957 -0.507226 -0.587258 -0.463539 -0.340389   \n",
      "8    -0.351225 -0.173628 -0.213560 -0.099212 -0.147283 -0.332590 -0.341743   \n",
      "9    -0.351225 -0.539546 -0.347864 -0.888295 -0.800924 -0.746548 -0.631686   \n",
      "10   -0.351225 -0.417330 -0.302129 -0.080777  0.134428  0.480288  0.660479   \n",
      "11   -0.351225 -0.500410 -0.303433 -0.065110  0.157177  0.513242  0.718814   \n",
      "12   -0.351225 -0.500563 -0.311109 -0.080030 -0.800264 -0.751096 -0.635995   \n",
      "13    2.507677  5.217274  2.056964  2.688934  0.742317  1.273980  0.990608   \n",
      "14   -0.351225 -0.351894 -0.308420 -0.082743 -0.731014 -0.794965 -0.683034   \n",
      "15   -0.351225 -0.169172 -0.200835 -0.043357 -0.156070  0.018601  0.098905   \n",
      "16   -0.351225  0.178997  0.468156  0.382895  0.316044  0.024181  0.166330   \n",
      "17   -0.351225 -0.145892  0.075406  0.082573  0.230417  0.568294 -0.046160   \n",
      "18   -0.534463 -0.588240 -0.426477 -0.336277 -0.151045  0.112123  0.280700   \n",
      "19   -0.351225 -0.156625  0.085808  0.061884 -0.180214 -0.056205 -0.031540   \n",
      "20   -0.351225 -0.161353 -0.134294  0.042364  0.163993  0.525963  0.704022   \n",
      "21    2.507677  0.911060  1.253546  0.240911  0.483945  0.581084  0.059304   \n",
      "22   -0.351225 -0.166610 -0.325982 -0.096623 -0.736015 -0.681789 -0.579465   \n",
      "23   -0.534463 -0.589857 -0.549992 -0.392136 -0.309014 -0.095043 -0.339096   \n",
      "24   -0.534463 -0.820680 -0.667136 -0.489360 -0.591819 -0.469436 -0.344576   \n",
      "25   -0.351225 -0.164835 -0.208598 -0.068985  0.140688 -0.228521 -0.322527   \n",
      "26   -0.351225 -0.159769 -0.134956  0.053470  0.139361  0.489908  0.690734   \n",
      "27   -0.351225 -0.155672 -1.019857 -0.903185 -0.805171 -0.783544 -0.678129   \n",
      "28    2.507677  2.892424  3.549244  2.573009  2.193240  0.801572  1.026495   \n",
      "29   -0.534463 -0.476742 -0.659455 -0.491625 -0.599520 -0.479637 -0.345150   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9715 -0.684455 -0.933691 -0.802337 -0.714731 -0.582701 -0.484618 -0.350689   \n",
      "9716 -0.351225 -0.163275  0.079581 -0.407053 -0.226609 -0.023921  0.143120   \n",
      "9717 -0.351225 -0.162188  0.080837 -0.407268 -0.226783  0.013495  0.183258   \n",
      "9718 -0.351225 -0.207678 -0.202807  0.052175  0.299763  0.514990  0.720285   \n",
      "9719 -0.351225 -0.409581 -0.200421  0.004759  0.247587  0.023368  0.193879   \n",
      "9720 -0.351225 -0.411743 -0.202884 -0.078790  0.137728 -0.479615 -0.345099   \n",
      "9721 -0.351225  0.201179  0.493215  0.385074  0.290116  0.499511  0.094724   \n",
      "9722 -0.351225  0.180354  0.469766  0.836854  1.205680  1.219908  0.924445   \n",
      "9723 -0.684455 -0.934189 -0.870329 -0.726492 -0.599979 -0.480310 -0.345849   \n",
      "9724 -0.351225 -0.174620 -0.126828 -0.192075 -0.162707  0.091629 -0.301202   \n",
      "9725 -0.351225  0.151272  0.436857  0.060087  0.311096  0.506682  0.236193   \n",
      "9726 -0.351225 -0.159851 -0.200443 -0.089231 -0.408627 -0.348568 -0.334029   \n",
      "9727  2.507677  2.906152  3.565452  3.645874  2.144731  0.883628 -0.195340   \n",
      "9728 -0.351225 -0.161361 -0.213199 -0.068258 -0.418407 -0.348611 -0.576779   \n",
      "9729 -0.351225 -0.170316 -0.797930 -0.869909 -0.763142 -0.744967 -0.631459   \n",
      "9730 -0.351225 -0.174850 -0.202625  0.052480  0.151077 -0.479454 -0.345164   \n",
      "9731 -0.351225  0.211445 -0.534959 -0.714123 -0.580126 -0.456007 -0.319827   \n",
      "9732 -0.351225 -0.166947 -0.202178  0.052387  0.294888  0.500038  0.017953   \n",
      "9733 -0.534463 -0.352048 -0.134858 -0.638876 -0.582596 -0.457177 -0.349222   \n",
      "9734 -0.534463 -0.339614 -0.863016 -0.717830 -0.592791 -0.471654 -0.336678   \n",
      "9735  2.001553  2.387006  2.579635  3.278894  4.012602  0.694269  0.830041   \n",
      "9736  2.507677  4.555007  2.964855  3.272702  2.024727  2.610600  1.979779   \n",
      "9737 -0.351225 -0.158116 -0.190172 -0.523041 -0.360077 -0.192065 -0.665418   \n",
      "9738 -0.351225 -0.501572 -0.307187 -0.076611 -0.806857 -0.758159 -0.654060   \n",
      "9739 -0.351225 -0.174940  0.066169  0.361633 -0.168806  0.090686  0.255194   \n",
      "9740 -0.351225 -0.173096 -0.416677 -0.210399 -0.145471  0.107211  0.225062   \n",
      "9741 -0.351225 -0.172540 -0.219307 -0.090478 -0.194263 -0.075668  0.068266   \n",
      "9742 -0.684455 -0.992224 -0.861913 -0.716587 -0.582895 -0.482245 -0.348248   \n",
      "9743 -0.534463 -0.353026 -0.859814 -0.714251 -0.581742 -0.455970 -0.347925   \n",
      "9744  2.507677  5.051728  5.324370  3.224301  3.954586  5.472092  3.376124   \n",
      "\n",
      "             7         8         9    ...         1266      1267      1268  \\\n",
      "0    -0.535947 -0.413029 -0.287900    ...    -0.047539 -0.047539 -0.047534   \n",
      "1     0.937017  0.589367  0.585246    ...    -0.047539 -0.047539 -0.047534   \n",
      "2    -0.588395 -0.979335 -1.406819    ...    -0.047539 -0.047539 -0.047534   \n",
      "3    -0.499638 -0.375517 -0.264053    ...    -0.047539 -0.047539 -0.047534   \n",
      "4    -0.529511 -0.929221 -0.827982    ...    -0.047539 -0.047539 -0.047534   \n",
      "5     0.299005  0.461601  0.149546    ...    -0.047539 -0.047539 -0.047534   \n",
      "6     0.355994  0.386472  0.122840    ...    -0.047539 -0.047539 -0.047534   \n",
      "7    -0.194728 -0.047717  0.105164    ...    -0.047539 -0.047539 -0.047534   \n",
      "8    -0.194035 -0.051298  0.097718    ...    -0.047539 -0.047539 -0.047534   \n",
      "9    -0.513371 -0.383515 -0.417362    ...    -0.047539 -0.047539 -0.047534   \n",
      "10    0.786520  0.960719  0.904403    ...    -0.047539 -0.047539 -0.047534   \n",
      "11    0.955531 -0.374883 -0.241241    ...    -0.047539 -0.047539 -0.047534   \n",
      "12   -0.515351 -0.465637 -0.822164    ...    -0.047539 -0.047539 -0.047534   \n",
      "13    1.249441  1.445321  1.685100    ...    -0.047539 -0.047539 -0.047534   \n",
      "14   -0.577830 -0.465477 -0.780007    ...    -0.047539 -0.047539 -0.047534   \n",
      "15    0.056967 -0.048074  0.104811    ...    -0.047539 -0.047539 -0.047534   \n",
      "16    0.326925  0.453309  0.484556    ...    -0.047539 -0.047539 -0.047534   \n",
      "17    0.124447  0.118157  0.115386    ...    -0.047539 -0.047539 -0.047534   \n",
      "18    0.222387 -0.374807 -0.241042    ...    -0.047539 -0.047539 -0.047534   \n",
      "19   -0.474763 -0.338195 -0.212177    ...    -0.047539 -0.047539 -0.047534   \n",
      "20    0.089182 -0.243099 -0.210171    ...    -0.047539 -0.047539 -0.047534   \n",
      "21    0.228696  0.391962  0.166085    ...    -0.047539 -0.047539 -0.047534   \n",
      "22   -0.469160 -0.358532 -0.289817    ...    -0.047539 -0.047539 -0.047534   \n",
      "23   -0.189190 -0.047819  0.104739    ...    -0.047539 -0.047539 -0.047534   \n",
      "24   -0.195348 -0.048146  0.104449    ...    -0.047539 -0.047539 -0.047534   \n",
      "25   -0.171581 -0.047597  0.104841    ...    -0.047539 -0.047539 -0.047534   \n",
      "26    0.095890  0.114166  0.017277    ...    -0.047539 -0.047539 -0.047534   \n",
      "27   -0.558856 -0.433552 -0.306173    ...    -0.047539 -0.047539 -0.047534   \n",
      "28    1.187333  1.386984  0.557903    ...    -0.047539 -0.047539 -0.047534   \n",
      "29   -0.195473 -0.048316  0.104350    ...    -0.047539 -0.047539 -0.047534   \n",
      "...        ...       ...       ...    ...          ...       ...       ...   \n",
      "9715 -0.217930 -0.076561 -0.059317    ...    -0.047539 -0.047539 -0.047534   \n",
      "9716 -0.191418 -0.044328  0.103940    ...    -0.047539 -0.047539 -0.047534   \n",
      "9717 -0.191418 -0.044328  0.103937    ...    -0.047539 -0.047539 -0.047534   \n",
      "9718 -0.195290 -0.048162  0.104701    ...    -0.047539 -0.047539 -0.047534   \n",
      "9719 -0.163174 -0.014752  0.140059    ...    -0.047539 -0.047539 -0.047534   \n",
      "9720 -0.195429 -0.048254  0.104688    ...    -0.047539 -0.047539 -0.047534   \n",
      "9721  0.207529  0.248413 -0.193547    ...    -0.047539 -0.047539 -0.047534   \n",
      "9722  1.040198  0.518770  0.704331    ...    -0.047539 -0.047539 -0.047534   \n",
      "9723 -0.196244 -0.074500  0.076466    ...    -0.047539 -0.047539 -0.047534   \n",
      "9724 -0.148204 -0.047893  0.104756    ...    -0.047539 -0.047539 -0.047534   \n",
      "9725  0.296055  0.007444  0.163151    ...    -0.047539 -0.047539 -0.047534   \n",
      "9726 -0.183514 -0.048012  0.104835    ...    -0.047539 -0.047539 -0.047534   \n",
      "9727 -0.137288 -0.091790  0.057924    ...    -0.047539 -0.047539 -0.047534   \n",
      "9728 -0.458337 -0.326425 -0.240200    ...    -0.047539 -0.047539 -0.047534   \n",
      "9729 -1.022901 -0.953592 -1.284066    ...    -0.047539 -0.047539 -0.047534   \n",
      "9730 -0.195480 -0.048399  0.104528    ...    -0.047539 -0.047539 -0.047534   \n",
      "9731 -0.198279 -0.051381  0.094737    ...    -0.047539 -0.047539 -0.047534   \n",
      "9732  0.103984 -0.019058  0.135575    ...    -0.047539 -0.047539 -0.047534   \n",
      "9733 -0.199929 -0.067099  0.078257    ...    -0.047539 -0.047539 -0.047534   \n",
      "9734 -0.201607 -0.055101  0.097178    ...    -0.047539 -0.047539 -0.047534   \n",
      "9735  1.031907  1.196214  1.406823    ...    -0.047539 -0.047539 -0.047534   \n",
      "9736  2.126695  1.201734  1.409068    ...    -0.047539 -0.047539 -0.047534   \n",
      "9737 -0.542082 -0.431516 -0.819548    ...    -0.047539 -0.047539 -0.047534   \n",
      "9738 -1.044116 -0.929681 -0.861886    ...    -0.047539 -0.047539 -0.047534   \n",
      "9739  0.453743 -0.261493 -0.121019    ...    -0.047539 -0.047539 -0.047534   \n",
      "9740  0.418801 -0.043228  0.109609    ...    -0.047539 -0.047539 -0.047534   \n",
      "9741  0.119103 -0.028264  0.125328    ...    -0.047539 -0.047539 -0.047534   \n",
      "9742 -0.203589 -0.056945  0.075608    ...    -0.047539 -0.047539 -0.047534   \n",
      "9743 -0.198505 -0.057463  0.094728    ...    -0.047539 -0.047539 -0.047534   \n",
      "9744  2.306049  2.538892  2.742295    ...    -0.047539 -0.047539 -0.047534   \n",
      "\n",
      "           1269      1270      1271    1272      1273      1274       Eat  \n",
      "0     11.929326 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -2.157297  \n",
      "1     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.240078  \n",
      "2     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.452498  \n",
      "3     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.738999  \n",
      "4     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.679834  \n",
      "5     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.387082  \n",
      "6     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.714452  \n",
      "7     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.252799  \n",
      "8     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.167818  \n",
      "9     -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.623767  \n",
      "10    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.324094  \n",
      "11    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.839993  \n",
      "12    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.670386  \n",
      "13    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.717228  \n",
      "14    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.745750  \n",
      "15    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.054298  \n",
      "16    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.237557  \n",
      "17    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.018577  \n",
      "18    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.161110  \n",
      "19    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.736044  \n",
      "20    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.779025  \n",
      "21    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.473585  \n",
      "22    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.148013  \n",
      "23    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.127966  \n",
      "24    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.040596  \n",
      "25    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.665759  \n",
      "26    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.880756  \n",
      "27    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.076911  \n",
      "28    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.475787  \n",
      "29    -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.395570  \n",
      "...         ...       ...       ...     ...       ...       ...       ...  \n",
      "9715  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.327030  \n",
      "9716  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.435245  \n",
      "9717  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.275362  \n",
      "9718  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.110955  \n",
      "9719  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.918367  \n",
      "9720  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.179784  \n",
      "9721  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  1.515423  \n",
      "9722  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.374237  \n",
      "9723  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.186324  \n",
      "9724  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.006883  \n",
      "9725  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  1.182824  \n",
      "9726  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.068492  \n",
      "9727  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  1.613191  \n",
      "9728  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.309439  \n",
      "9729  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  1.082156  \n",
      "9730  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.000703  \n",
      "9731  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.203068  \n",
      "9732  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -1.015048  \n",
      "9733  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.010827  \n",
      "9734  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.486752  \n",
      "9735  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.500940  \n",
      "9736  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.698781  \n",
      "9737  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  1.392797  \n",
      "9738  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  1.000624  \n",
      "9739  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.636306  \n",
      "9740  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.046233  \n",
      "9741  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.949185  \n",
      "9742  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.382402  \n",
      "9743  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568 -0.308806  \n",
      "9744  -0.083827 -0.047525 -0.047528 -0.0642 -0.047528 -0.047568  0.294065  \n",
      "\n",
      "[9745 rows x 1276 columns]\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "print(scaler)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "...\n",
    "df_train_copy = df_train\n",
    "df_val_copy = df_val\n",
    "print(X_train_scaled)\n",
    "y_train = df_train_copy.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val_copy.pop('Eat').values.reshape(-1,1)\n",
    "...\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
