{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Entendimiento de imágenes de personas\n",
    "\n",
    "El problema de inferir ciertas características de una persona a través de una foto de ella puede resultar bastante dificil incluso para nosotros, como por ejemplo de qué país es, la emoción que expresa, la edad que tiene, o el género. La automatización de este proceso para que máquinas logren identificar ciertas características de una persona puede ser algo crucial para el futuro desarrollo de Inteligencia Artificial.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/6B072GE.jpg\" width=\"60%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "En esta actividad trabajaremos con unos datos (imágenes) con la tarea de predecir la **edad** (*target value*) de la persona en la imagen. Los datos con corresponden a 3640 imágenes de Flickr de rostros de personas, pero, debido a que trabajamos con redes *feed forward*, se trabajará con representaciones de características extraídas. Para ésto necesitará descargar los datos del siguiente __[link](http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html)__ en el extracto de *ageGenderClassification* o a través de la consola Unix.\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/ageGenderClassification.zip\n",
    "```\n",
    "\n",
    "Se trabajará con archivos *.mat* que pueden ser cargados de la siguiente manera:\n",
    "```python\n",
    "import scipy.io as sio\n",
    "sio.loadmat(\"file.mat\")\n",
    "```\n",
    "\n",
    "Para descripción sobre las columnas están en el archivo readme a través del siguiente __[link](http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt)__ o a través de la consola Unix:\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Cargue los datos dos dataset de entrenamiento y de pruebas ¿Cuántos datos hay en cada conjunto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de datos en el conjunto de entrenamiento es  38500\n",
      "La cantidad de datos en el conjunto de test es  11550\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "mat_train = sio.loadmat(\"./eventrain.mat\")\n",
    "mat_test = sio.loadmat(\"./eventest.mat\")\n",
    "data_train= mat_train[\"trcoll\"][0][0]\n",
    "data_test= mat_test[\"tecoll\"][0][0]\n",
    "row=len(data_train)\n",
    "columns=len(data_train[0])\n",
    "row2=len(data_test)\n",
    "columns2=len(data_test[0])\n",
    "print(\"La cantidad de datos en el conjunto de entrenamiento es \",(columns*row))\n",
    "print(\"La cantidad de datos en el conjunto de test es \",(columns2*row2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Eliga cuál representación utilizará para trabajar los datos y entregárselos como *input* al modelo neuronal denso. Además extraiga las etiquetas del problema. Describa los datos utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el problema de clasificación se utilizo la representación de genFeat, donde se tienen 14 caracteristicas de entrada. Estos serian los datos para ser presentados como entrada al modelo neuronal denso. Los nombres de cada una de estas caracteristicas se describen a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      xcenter  ycenter  minSpanningTreeDegree  SizeRelativeToNeighbor  \\\n",
      "0        87.0     35.0                    1.0                1.118034   \n",
      "1        32.0     60.0                    1.0                1.012220   \n",
      "2        41.0     65.0                    2.0                0.987927   \n",
      "3        29.0     33.0                    1.0                0.895533   \n",
      "4        38.0     31.0                    2.0                1.090062   \n",
      "5        78.0     34.0                    2.0                0.894427   \n",
      "6        60.0     35.0                    2.0                1.000000   \n",
      "7        53.0     32.0                    2.0                1.000000   \n",
      "8        43.0     35.0                    2.0                0.917379   \n",
      "9        27.0     32.0                    2.0                0.848875   \n",
      "10       42.0     28.0                    3.0                1.199251   \n",
      "11       37.0     19.0                    1.0                1.126793   \n",
      "12       37.0     34.0                    3.0                1.007782   \n",
      "13       31.0     20.0                    2.0                0.996546   \n",
      "14       42.0     44.0                    1.0                1.101415   \n",
      "15       23.0      9.0                    1.0                1.003466   \n",
      "16       60.0     13.0                    2.0                1.085931   \n",
      "17       19.0     36.0                    2.0                1.000000   \n",
      "18       30.0     39.0                    2.0                1.000000   \n",
      "19       83.0     24.0                    1.0                1.270730   \n",
      "20       14.0     16.0                    1.0                1.109823   \n",
      "21       39.0     17.0                    2.0                0.995037   \n",
      "22       84.0     17.0                    2.0                0.887954   \n",
      "23       26.0     22.0                    3.0                1.109823   \n",
      "24       50.0     20.0                    2.0                1.214747   \n",
      "25       21.0     20.0                    2.0                0.901044   \n",
      "26       42.0      7.0                    2.0                1.004988   \n",
      "27       51.0      8.0                    3.0                0.823217   \n",
      "28       78.0     47.0                    1.0                1.386750   \n",
      "29       20.0     37.0                    2.0                1.272792   \n",
      "...       ...      ...                    ...                     ...   \n",
      "1020     45.0     56.0                    2.0                0.916734   \n",
      "1021     44.0     68.0                    1.0                0.714998   \n",
      "1022     50.0     25.0                    2.0                0.688991   \n",
      "1023     58.0     39.0                    2.0                0.762862   \n",
      "1024     67.0     51.0                    1.0                0.822523   \n",
      "1025     49.0     46.0                    1.0                0.780869   \n",
      "1026     50.0     75.0                    1.0                0.601736   \n",
      "1027     64.0     60.0                    1.0                0.858137   \n",
      "1028     36.0     37.0                    1.0                0.848687   \n",
      "1029     34.0     77.0                    1.0                0.830582   \n",
      "1030     41.0     52.0                    1.0                0.786433   \n",
      "1031     41.0     36.0                    1.0                1.140429   \n",
      "1032     47.0     70.0                    1.0                0.907808   \n",
      "1033     46.0     52.0                    1.0                0.935674   \n",
      "1034     68.0     44.0                    2.0                0.711711   \n",
      "1035     67.0     29.0                    1.0                0.786646   \n",
      "1036     70.0     50.0                    1.0                1.195165   \n",
      "1037     37.0     20.0                    2.0                1.151311   \n",
      "1038     42.0     34.0                    1.0                1.160782   \n",
      "1039     60.0     29.0                    1.0                0.861488   \n",
      "1040     61.0     27.0                    1.0                1.433145   \n",
      "1041     40.0     67.0                    2.0                0.933053   \n",
      "1042     91.0     36.0                    2.0                1.049980   \n",
      "1043     20.0     38.0                    1.0                0.836216   \n",
      "1044     35.0     48.0                    1.0                1.076497   \n",
      "1045     39.0     20.0                    1.0                1.119105   \n",
      "1046     73.0     40.0                    1.0                0.887954   \n",
      "1047     34.0     59.0                    1.0                0.852955   \n",
      "1048     65.0     58.0                    1.0                1.240347   \n",
      "1049     33.0     11.0                    1.0                1.151751   \n",
      "\n",
      "          PosX       PosY  neiAngle   myAngle  SizeRelAverage  \\\n",
      "0     4.159086   0.268328 -0.000000 -0.179853        1.061500   \n",
      "1    -4.114365  -1.744133  0.090660  0.179853        1.061500   \n",
      "2     4.164644   1.765447  0.179853  0.090660        1.048685   \n",
      "3    -5.055556   0.722222 -0.099669 -0.000000        0.854491   \n",
      "4    -2.935360  -1.542308  0.218669 -0.099669        0.954170   \n",
      "5    -4.650000  -0.300000 -0.179853 -0.000000        0.949435   \n",
      "6     3.482630   1.194045 -0.099669 -0.099669        0.954170   \n",
      "7    -3.482630  -1.194045 -0.099669 -0.099669        0.954170   \n",
      "8     3.199724   1.681211 -0.099669  0.218669        0.875336   \n",
      "9    -2.642857   2.857143  0.244979 -0.000000        0.937019   \n",
      "10    2.093750   1.406250  0.226799 -0.000000        1.070879   \n",
      "11   -1.197342  -2.993355  0.226799 -0.066568        1.006177   \n",
      "12   -3.162886   2.604729 -0.000000  0.124355        1.079212   \n",
      "13    3.166667   3.291667  0.083141 -0.000000        1.055526   \n",
      "14   -4.479872   1.191893  0.090660  0.165149        1.070085   \n",
      "15   -3.155728  -3.280296 -0.000000  0.083141        1.059184   \n",
      "16   -4.561579  -3.085774  0.507099  0.179853        0.983428   \n",
      "17   -4.120739  -0.558744  0.211093 -0.211093        0.981207   \n",
      "18    4.120739   0.558744 -0.211093  0.211093        0.981207   \n",
      "19    2.030541  -1.496188 -0.090660  0.071307        0.961871   \n",
      "20   -3.432878  -1.393052 -0.110657 -0.099669        0.989426   \n",
      "21   -1.300000   3.950000 -0.099669 -0.000000        0.984516   \n",
      "22   -1.711689  -2.816004  0.197396  0.110657        0.891517   \n",
      "23    2.537345   0.696526 -0.110657 -0.099669        0.989426   \n",
      "24   -0.727273   3.954545 -0.110657 -0.000000        1.082968   \n",
      "25   -2.816004  -0.773021 -0.099669 -0.110657        0.891517   \n",
      "26    1.293548  -3.930397 -0.000000 -0.099669        0.989426   \n",
      "27    0.883452  -4.803771 -0.000000 -0.110657        0.891517   \n",
      "28    3.264659   3.175217  0.124355  0.179853        1.362011   \n",
      "29   -0.666667   4.277778 -0.141897 -0.000000        1.096398   \n",
      "...        ...        ...       ...       ...             ...   \n",
      "1020  2.256540   2.513998  0.027771  0.030294        0.989529   \n",
      "1021 -2.165134   5.377913 -0.049958 -0.211093        0.709795   \n",
      "1022 -4.481519   0.090536  0.062419 -0.090660        0.771201   \n",
      "1023 -3.978261   0.152174  0.099669 -0.000000        0.838701   \n",
      "1024  3.097683   1.608205 -0.106736 -0.239743        0.902620   \n",
      "1025 -3.355936   3.688756 -0.086738 -0.055499        0.842401   \n",
      "1026 -1.578947   6.315789  0.191184 -0.000000        0.646294   \n",
      "1027  5.887097   2.532258  0.083141 -0.000000        0.848918   \n",
      "1028 -3.356898   0.693037  0.298499  0.086738        0.918151   \n",
      "1029 -0.079174   4.963610 -0.450074  0.048742        0.834660   \n",
      "1030 -2.397500   1.603961 -0.186979  0.204018        0.880451   \n",
      "1031  1.466667  -4.466667  0.152649 -0.000000        1.065608   \n",
      "1032 -3.425575  14.068671 -0.066568 -0.147078        0.952174   \n",
      "1033 -2.966667   5.566667 -0.062419 -0.000000        0.951172   \n",
      "1034  5.653239   1.924765 -0.034469 -0.121352        0.808091   \n",
      "1035  2.293225  -1.986439  0.344647  0.566729        0.922244   \n",
      "1036  1.924838  -0.006850  0.049141 -0.165149        1.088907   \n",
      "1037  1.919355  -3.467742 -0.262995 -0.000000        1.057509   \n",
      "1038 -2.115707   0.360368  0.054002  0.023252        1.074409   \n",
      "1039  2.455874  -0.418308  0.023252  0.054002        0.925591   \n",
      "1040  3.369278  -2.722497 -0.129703  0.120624        1.178018   \n",
      "1041  3.986675   1.767289  0.076772  0.165149        1.069889   \n",
      "1042  1.036150   4.185499 -0.231091 -0.192048        0.964468   \n",
      "1043 -5.280307  -6.351242 -0.099669  0.397079        0.884027   \n",
      "1044 -2.006833   1.085408  0.471166  0.031240        1.042062   \n",
      "1045 -3.732143  -1.767857  0.039979 -0.000000        1.056205   \n",
      "1046  8.006286   3.920319  0.197396 -0.110657        0.927068   \n",
      "1047 -4.490320   1.208477 -0.244979  0.239232        0.920643   \n",
      "1048  3.467149  -0.581318  0.207496 -0.083141        1.140037   \n",
      "1049 -5.643580  -4.775337 -0.000000 -0.124355        1.070524   \n",
      "\n",
      "      x of [x y] position relative average.  \\\n",
      "0                                 16.114498   \n",
      "1                                -10.659564   \n",
      "2                                 -6.292163   \n",
      "3                                -12.083716   \n",
      "4                                 -7.763787   \n",
      "5                                 11.699626   \n",
      "6                                  3.154713   \n",
      "7                                 -0.168309   \n",
      "8                                 -4.962955   \n",
      "9                                -15.835009   \n",
      "10                                -5.728592   \n",
      "11                                -9.175482   \n",
      "12                                -9.142017   \n",
      "13                                -8.786275   \n",
      "14                                -4.212330   \n",
      "15                               -12.128773   \n",
      "16                                 3.836054   \n",
      "17                               -10.807567   \n",
      "18                                -6.764269   \n",
      "19                                11.601898   \n",
      "20                               -19.648869   \n",
      "21                                -7.145514   \n",
      "22                                15.153775   \n",
      "23                               -13.741772   \n",
      "24                                -1.878353   \n",
      "25                               -16.252288   \n",
      "26                                -5.865643   \n",
      "27                                -1.090740   \n",
      "28                                18.022037   \n",
      "29                               -18.159086   \n",
      "...                                     ...   \n",
      "1020                              -0.734314   \n",
      "1021                              -2.759632   \n",
      "1022                               0.389836   \n",
      "1023                               0.461893   \n",
      "1024                               1.398016   \n",
      "1025                              -0.295944   \n",
      "1026                              -0.014173   \n",
      "1027                               1.688708   \n",
      "1028                              -1.541070   \n",
      "1029                              -0.906531   \n",
      "1030                              -1.055440   \n",
      "1031                               0.781446   \n",
      "1032                              -1.534939   \n",
      "1033                              -1.395053   \n",
      "1034                               0.358684   \n",
      "1035                               2.801022   \n",
      "1036                               1.047984   \n",
      "1037                              -2.194616   \n",
      "1038                              -1.136567   \n",
      "1039                               1.136567   \n",
      "1040                               1.984536   \n",
      "1041                             -10.263104   \n",
      "1042                               8.579797   \n",
      "1043                             -10.262883   \n",
      "1044                              -2.508951   \n",
      "1045                              -1.970955   \n",
      "1046                               6.210905   \n",
      "1047                              -2.066991   \n",
      "1048                               7.218963   \n",
      "1049                              -3.020795   \n",
      "\n",
      "      y of [x y] position relative average.  Size Relative to Planar FaceFit.  \\\n",
      "0                                 -2.218225                          1.093432   \n",
      "1                                  6.801406                          0.974263   \n",
      "2                                  8.652804                          0.941440   \n",
      "3                                 -3.072716                          0.890515   \n",
      "4                                 -3.689849                          1.002888   \n",
      "5                                 -2.503055                          0.981788   \n",
      "6                                 -2.170753                          0.982241   \n",
      "7                                 -3.310075                          0.997645   \n",
      "8                                 -2.218225                          0.901667   \n",
      "9                                 -0.348340                          0.938159   \n",
      "10                                -2.490097                          1.080261   \n",
      "11                                -7.007866                          1.031387   \n",
      "12                                 0.320959                          1.078006   \n",
      "13                                -2.194125                          1.048008   \n",
      "14                                 5.634357                          1.090166   \n",
      "15                                -5.668564                          1.039913   \n",
      "16                                -4.393137                          0.969504   \n",
      "17                                -1.781792                          1.074197   \n",
      "18                                -1.233549                          1.043760   \n",
      "19                                -4.865664                          1.259642   \n",
      "20                                -2.080438                          1.022814   \n",
      "21                                -1.637406                          1.010477   \n",
      "22                                -1.686632                          0.915752   \n",
      "23                                -0.012954                          0.989628   \n",
      "24                                -0.751341                          1.095887   \n",
      "25                                -0.702115                          0.901448   \n",
      "26                                -5.526244                          1.083364   \n",
      "27                                -5.033986                          0.967973   \n",
      "28                                 3.813027                          1.255954   \n",
      "29                                -0.328919                          1.104443   \n",
      "...                                     ...                               ...   \n",
      "1020                               1.743370                          0.992728   \n",
      "1021                               3.437147                          1.024542   \n",
      "1022                               0.157098                          0.846788   \n",
      "1023                               0.212714                          0.898661   \n",
      "1024                               0.725799                          1.000000   \n",
      "1025                               2.351976                          0.992409   \n",
      "1026                               3.339187                          1.002404   \n",
      "1027                               2.596959                          0.990034   \n",
      "1028                               0.318156                          1.000000   \n",
      "1029                               2.953426                          1.009553   \n",
      "1030                               0.706104                          1.000000   \n",
      "1031                              -2.379857                          1.000000   \n",
      "1032                              10.977137                          0.986588   \n",
      "1033                               5.664760                          0.989144   \n",
      "1034                               1.669512                          0.911276   \n",
      "1035                              -1.721167                          0.922405   \n",
      "1036                              -0.003729                          1.000000   \n",
      "1037                              -1.637434                          0.994537   \n",
      "1038                               0.193591                          1.000000   \n",
      "1039                              -0.193591                          1.000000   \n",
      "1040                              -1.603576                          1.000000   \n",
      "1041                              10.183954                          0.958108   \n",
      "1042                               0.199428                          0.962645   \n",
      "1043                               0.804287                          0.877330   \n",
      "1044                               0.995443                          1.010497   \n",
      "1045                              -0.933610                          1.000000   \n",
      "1046                               2.747131                          0.995016   \n",
      "1047                               0.556288                          1.000000   \n",
      "1048                               4.733746                          1.110636   \n",
      "1049                              -2.556058                          1.000000   \n",
      "\n",
      "      Nearest Neighbor Gender   (NOT USED IN CVPR 09)  \\\n",
      "0                                                 1.0   \n",
      "1                                                 1.0   \n",
      "2                                                 1.0   \n",
      "3                                                 1.0   \n",
      "4                                                 1.0   \n",
      "5                                                 1.0   \n",
      "6                                                 1.0   \n",
      "7                                                 1.0   \n",
      "8                                                 1.0   \n",
      "9                                                 1.0   \n",
      "10                                                1.0   \n",
      "11                                                1.0   \n",
      "12                                                1.0   \n",
      "13                                                2.0   \n",
      "14                                                2.0   \n",
      "15                                                1.0   \n",
      "16                                                1.0   \n",
      "17                                                1.0   \n",
      "18                                                1.0   \n",
      "19                                                2.0   \n",
      "20                                                2.0   \n",
      "21                                                1.0   \n",
      "22                                                1.0   \n",
      "23                                                2.0   \n",
      "24                                                1.0   \n",
      "25                                                1.0   \n",
      "26                                                2.0   \n",
      "27                                                1.0   \n",
      "28                                                1.0   \n",
      "29                                                2.0   \n",
      "...                                               ...   \n",
      "1020                                              1.0   \n",
      "1021                                              1.0   \n",
      "1022                                              2.0   \n",
      "1023                                              2.0   \n",
      "1024                                              1.0   \n",
      "1025                                              1.0   \n",
      "1026                                              1.0   \n",
      "1027                                              1.0   \n",
      "1028                                              2.0   \n",
      "1029                                              1.0   \n",
      "1030                                              1.0   \n",
      "1031                                              1.0   \n",
      "1032                                              1.0   \n",
      "1033                                              1.0   \n",
      "1034                                              1.0   \n",
      "1035                                              1.0   \n",
      "1036                                              1.0   \n",
      "1037                                              2.0   \n",
      "1038                                              1.0   \n",
      "1039                                              1.0   \n",
      "1040                                              1.0   \n",
      "1041                                              1.0   \n",
      "1042                                              1.0   \n",
      "1043                                              1.0   \n",
      "1044                                              1.0   \n",
      "1045                                              2.0   \n",
      "1046                                              2.0   \n",
      "1047                                              1.0   \n",
      "1048                                              2.0   \n",
      "1049                                              2.0   \n",
      "\n",
      "      NEarest Neighbor Agebin   (NOT USED IN CVPR 09)  \n",
      "0                                                 5.0  \n",
      "1                                                 5.0  \n",
      "2                                                 5.0  \n",
      "3                                                 5.0  \n",
      "4                                                 5.0  \n",
      "5                                                 5.0  \n",
      "6                                                 5.0  \n",
      "7                                                 5.0  \n",
      "8                                                 5.0  \n",
      "9                                                 4.0  \n",
      "10                                                6.0  \n",
      "11                                                6.0  \n",
      "12                                                5.0  \n",
      "13                                                5.0  \n",
      "14                                                2.0  \n",
      "15                                                5.0  \n",
      "16                                                6.0  \n",
      "17                                                5.0  \n",
      "18                                                5.0  \n",
      "19                                                2.0  \n",
      "20                                                5.0  \n",
      "21                                                5.0  \n",
      "22                                                6.0  \n",
      "23                                                5.0  \n",
      "24                                                5.0  \n",
      "25                                                5.0  \n",
      "26                                                5.0  \n",
      "27                                                5.0  \n",
      "28                                                5.0  \n",
      "29                                                6.0  \n",
      "...                                               ...  \n",
      "1020                                              2.0  \n",
      "1021                                              5.0  \n",
      "1022                                              6.0  \n",
      "1023                                              6.0  \n",
      "1024                                              4.0  \n",
      "1025                                              5.0  \n",
      "1026                                              5.0  \n",
      "1027                                              5.0  \n",
      "1028                                              2.0  \n",
      "1029                                              5.0  \n",
      "1030                                              5.0  \n",
      "1031                                              2.0  \n",
      "1032                                              2.0  \n",
      "1033                                              2.0  \n",
      "1034                                              5.0  \n",
      "1035                                              5.0  \n",
      "1036                                              1.0  \n",
      "1037                                              2.0  \n",
      "1038                                              3.0  \n",
      "1039                                              3.0  \n",
      "1040                                              1.0  \n",
      "1041                                              4.0  \n",
      "1042                                              6.0  \n",
      "1043                                              2.0  \n",
      "1044                                              4.0  \n",
      "1045                                              4.0  \n",
      "1046                                              5.0  \n",
      "1047                                              6.0  \n",
      "1048                                              1.0  \n",
      "1049                                              2.0  \n",
      "\n",
      "[1050 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "genFeat_train = data_train[0]  #it can be used as representation: contextual features\n",
    "genFeat_test = data_test[0]\n",
    "ageClass_train = data_train[1]\n",
    "ageClass_test = data_test[1]\n",
    "faceGist_train = data_train[4]\n",
    "faceGist_test = data_test[4]\n",
    "ffcoefs_test = data_test[3]\n",
    "ffcoefs_train = data_train[3]\n",
    "\n",
    "x_test = pd.DataFrame(genFeat_test, columns=[\"xcenter\", \"ycenter\", \"minSpanningTreeDegree\", \"SizeRelativeToNeighbor\", \"PosX\", \"PosY\", \"neiAngle\", \"myAngle\",\n",
    "                              \"SizeRelAverage\", \"x of [x y] position relative average.\", \"y of [x y] position relative average.\", \"Size Relative to Planar FaceFit.\", \"Nearest Neighbor Gender   (NOT USED IN CVPR 09)\",\n",
    "                             \"NEarest Neighbor Agebin   (NOT USED IN CVPR 09)\"])\n",
    "x_train = pd.DataFrame(genFeat_train, columns=[\"xcenter\", \"ycenter\", \"minSpanningTreeDegree\", \"SizeRelativeToNeighbor\", \"PosX\", \"PosY\", \"neiAngle\", \"myAngle\",\n",
    "                              \"SizeRelAverage\", \"x of [x y] position relative average.\", \"y of [x y] position relative average.\", \"Size Relative to Planar FaceFit.\", \"Nearest Neighbor Gender   (NOT USED IN CVPR 09)\",\n",
    "                             \"NEarest Neighbor Agebin   (NOT USED IN CVPR 09)\"])\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Defina y entrene una modelo de red neuronal *feed forward* para la inferencia de la edad de la persona a través de la representación escogida. Intente llegar a un *mse* menor a 100 en el conjunto de pruebas. Recuerde que **NO** puede seleccionar modelos a través del conjunto de pruebas. Visualice sus resultados si estima conveniente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema como tarea de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1050 samples\n",
      "Epoch 1/50\n",
      "3500/3500 [==============================] - 2s 581us/step - loss: 1.9548 - categorical_accuracy: 0.1457 - val_loss: 1.9591 - val_categorical_accuracy: 0.1429\n",
      "Epoch 2/50\n",
      "3500/3500 [==============================] - 1s 366us/step - loss: 1.9526 - categorical_accuracy: 0.1357 - val_loss: 1.9521 - val_categorical_accuracy: 0.1429\n",
      "Epoch 3/50\n",
      "3500/3500 [==============================] - 1s 349us/step - loss: 1.9526 - categorical_accuracy: 0.1426 - val_loss: 1.9495 - val_categorical_accuracy: 0.1429\n",
      "Epoch 4/50\n",
      "3500/3500 [==============================] - 1s 358us/step - loss: 1.9518 - categorical_accuracy: 0.1366 - val_loss: 1.9490 - val_categorical_accuracy: 0.1429\n",
      "Epoch 5/50\n",
      "3500/3500 [==============================] - 1s 423us/step - loss: 1.9518 - categorical_accuracy: 0.1460 - val_loss: 1.9544 - val_categorical_accuracy: 0.1429\n",
      "Epoch 6/50\n",
      "3500/3500 [==============================] - 1s 338us/step - loss: 1.9504 - categorical_accuracy: 0.1434 - val_loss: 1.9579 - val_categorical_accuracy: 0.1429\n",
      "Epoch 7/50\n",
      "3500/3500 [==============================] - 1s 341us/step - loss: 1.9518 - categorical_accuracy: 0.1451 - val_loss: 1.9506 - val_categorical_accuracy: 0.1429\n",
      "Epoch 8/50\n",
      "3500/3500 [==============================] - 1s 388us/step - loss: 1.9522 - categorical_accuracy: 0.1343 - val_loss: 1.9534 - val_categorical_accuracy: 0.1429\n",
      "Epoch 9/50\n",
      "3500/3500 [==============================] - 1s 369us/step - loss: 1.9516 - categorical_accuracy: 0.1386 - val_loss: 1.9508 - val_categorical_accuracy: 0.1429\n",
      "Epoch 10/50\n",
      "3500/3500 [==============================] - 1s 326us/step - loss: 1.9520 - categorical_accuracy: 0.1337 - val_loss: 1.9469 - val_categorical_accuracy: 0.1429\n",
      "Epoch 11/50\n",
      "3500/3500 [==============================] - 1s 374us/step - loss: 1.9520 - categorical_accuracy: 0.1471 - val_loss: 1.9532 - val_categorical_accuracy: 0.1429\n",
      "Epoch 12/50\n",
      "3500/3500 [==============================] - 1s 370us/step - loss: 1.9527 - categorical_accuracy: 0.1443 - val_loss: 1.9514 - val_categorical_accuracy: 0.1429\n",
      "Epoch 13/50\n",
      "3500/3500 [==============================] - 1s 334us/step - loss: 1.9518 - categorical_accuracy: 0.1383 - val_loss: 1.9573 - val_categorical_accuracy: 0.1429\n",
      "Epoch 14/50\n",
      "3500/3500 [==============================] - 1s 408us/step - loss: 1.9513 - categorical_accuracy: 0.1417 - val_loss: 1.9502 - val_categorical_accuracy: 0.1429\n",
      "Epoch 15/50\n",
      "3500/3500 [==============================] - 1s 336us/step - loss: 1.9524 - categorical_accuracy: 0.1431 - val_loss: 1.9517 - val_categorical_accuracy: 0.1429\n",
      "Epoch 16/50\n",
      "3500/3500 [==============================] - 1s 383us/step - loss: 1.9537 - categorical_accuracy: 0.1383 - val_loss: 1.9478 - val_categorical_accuracy: 0.1429\n",
      "Epoch 17/50\n",
      "3500/3500 [==============================] - 2s 492us/step - loss: 1.9529 - categorical_accuracy: 0.1306 - val_loss: 1.9499 - val_categorical_accuracy: 0.1429\n",
      "Epoch 18/50\n",
      "3500/3500 [==============================] - 2s 495us/step - loss: 1.9522 - categorical_accuracy: 0.1340 - val_loss: 1.9585 - val_categorical_accuracy: 0.1429\n",
      "Epoch 19/50\n",
      "3500/3500 [==============================] - 1s 353us/step - loss: 1.9501 - categorical_accuracy: 0.1423 - val_loss: 1.9551 - val_categorical_accuracy: 0.1429\n",
      "Epoch 20/50\n",
      "3500/3500 [==============================] - 1s 376us/step - loss: 1.9515 - categorical_accuracy: 0.1434 - val_loss: 1.9507 - val_categorical_accuracy: 0.1429\n",
      "Epoch 21/50\n",
      "3500/3500 [==============================] - 1s 368us/step - loss: 1.9512 - categorical_accuracy: 0.1440 - val_loss: 1.9522 - val_categorical_accuracy: 0.1429\n",
      "Epoch 22/50\n",
      "3500/3500 [==============================] - 1s 371us/step - loss: 1.9507 - categorical_accuracy: 0.1437 - val_loss: 1.9496 - val_categorical_accuracy: 0.1429\n",
      "Epoch 23/50\n",
      "3500/3500 [==============================] - 1s 359us/step - loss: 1.9515 - categorical_accuracy: 0.1397 - val_loss: 1.9482 - val_categorical_accuracy: 0.1429\n",
      "Epoch 24/50\n",
      "3500/3500 [==============================] - 1s 396us/step - loss: 1.9522 - categorical_accuracy: 0.1389 - val_loss: 1.9480 - val_categorical_accuracy: 0.1429\n",
      "Epoch 25/50\n",
      "3500/3500 [==============================] - 1s 374us/step - loss: 1.9520 - categorical_accuracy: 0.1329 - val_loss: 1.9525 - val_categorical_accuracy: 0.1429\n",
      "Epoch 26/50\n",
      "3500/3500 [==============================] - 1s 339us/step - loss: 1.9528 - categorical_accuracy: 0.1351 - val_loss: 1.9498 - val_categorical_accuracy: 0.1429\n",
      "Epoch 27/50\n",
      "3500/3500 [==============================] - 1s 416us/step - loss: 1.9525 - categorical_accuracy: 0.1309 - val_loss: 1.9513 - val_categorical_accuracy: 0.1429\n",
      "Epoch 28/50\n",
      "3500/3500 [==============================] - 1s 410us/step - loss: 1.9522 - categorical_accuracy: 0.1429 - val_loss: 1.9491 - val_categorical_accuracy: 0.1429\n",
      "Epoch 29/50\n",
      "3500/3500 [==============================] - 1s 356us/step - loss: 1.9540 - categorical_accuracy: 0.1254 - val_loss: 1.9498 - val_categorical_accuracy: 0.1429\n",
      "Epoch 30/50\n",
      "3500/3500 [==============================] - 1s 413us/step - loss: 1.9517 - categorical_accuracy: 0.1409 - val_loss: 1.9483 - val_categorical_accuracy: 0.1429\n",
      "Epoch 31/50\n",
      "3500/3500 [==============================] - 1s 373us/step - loss: 1.9525 - categorical_accuracy: 0.1377 - val_loss: 1.9488 - val_categorical_accuracy: 0.1429\n",
      "Epoch 32/50\n",
      "3500/3500 [==============================] - 1s 371us/step - loss: 1.9510 - categorical_accuracy: 0.1463 - val_loss: 1.9464 - val_categorical_accuracy: 0.1429\n",
      "Epoch 33/50\n",
      "3500/3500 [==============================] - 1s 405us/step - loss: 1.9499 - categorical_accuracy: 0.1457 - val_loss: 1.9635 - val_categorical_accuracy: 0.1429\n",
      "Epoch 34/50\n",
      "3500/3500 [==============================] - 1s 350us/step - loss: 1.9514 - categorical_accuracy: 0.1411 - val_loss: 1.9498 - val_categorical_accuracy: 0.1429\n",
      "Epoch 35/50\n",
      "3500/3500 [==============================] - 1s 369us/step - loss: 1.9505 - categorical_accuracy: 0.1440 - val_loss: 1.9594 - val_categorical_accuracy: 0.1429\n",
      "Epoch 36/50\n",
      "3500/3500 [==============================] - 1s 410us/step - loss: 1.9507 - categorical_accuracy: 0.1443 - val_loss: 1.9520 - val_categorical_accuracy: 0.1429\n",
      "Epoch 37/50\n",
      "3500/3500 [==============================] - 1s 402us/step - loss: 1.9514 - categorical_accuracy: 0.1457 - val_loss: 1.9529 - val_categorical_accuracy: 0.1429\n",
      "Epoch 38/50\n",
      "3500/3500 [==============================] - 2s 501us/step - loss: 1.9511 - categorical_accuracy: 0.1400 - val_loss: 1.9585 - val_categorical_accuracy: 0.1429\n",
      "Epoch 39/50\n",
      "3500/3500 [==============================] - 2s 501us/step - loss: 1.9527 - categorical_accuracy: 0.1474 - val_loss: 1.9532 - val_categorical_accuracy: 0.1429\n",
      "Epoch 40/50\n",
      "3500/3500 [==============================] - 2s 452us/step - loss: 1.9513 - categorical_accuracy: 0.1406 - val_loss: 1.9519 - val_categorical_accuracy: 0.1429\n",
      "Epoch 41/50\n",
      "3500/3500 [==============================] - 1s 403us/step - loss: 1.9507 - categorical_accuracy: 0.1480 - val_loss: 1.9571 - val_categorical_accuracy: 0.1429\n",
      "Epoch 42/50\n",
      "3500/3500 [==============================] - 2s 475us/step - loss: 1.9516 - categorical_accuracy: 0.1497 - val_loss: 1.9489 - val_categorical_accuracy: 0.1429\n",
      "Epoch 43/50\n",
      "3500/3500 [==============================] - 2s 485us/step - loss: 1.9517 - categorical_accuracy: 0.1394 - val_loss: 1.9477 - val_categorical_accuracy: 0.1429\n",
      "Epoch 44/50\n",
      "3500/3500 [==============================] - 2s 448us/step - loss: 1.9512 - categorical_accuracy: 0.1454 - val_loss: 1.9565 - val_categorical_accuracy: 0.1429\n",
      "Epoch 45/50\n",
      "3500/3500 [==============================] - 2s 482us/step - loss: 1.9520 - categorical_accuracy: 0.1411 - val_loss: 1.9546 - val_categorical_accuracy: 0.1429\n",
      "Epoch 46/50\n",
      "3500/3500 [==============================] - 1s 365us/step - loss: 1.9515 - categorical_accuracy: 0.1369 - val_loss: 1.9545 - val_categorical_accuracy: 0.1429\n",
      "Epoch 47/50\n",
      "3500/3500 [==============================] - 2s 445us/step - loss: 1.9528 - categorical_accuracy: 0.1380 - val_loss: 1.9465 - val_categorical_accuracy: 0.1171\n",
      "Epoch 48/50\n",
      "3500/3500 [==============================] - 2s 470us/step - loss: 1.9526 - categorical_accuracy: 0.1354 - val_loss: 1.9490 - val_categorical_accuracy: 0.1429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "3500/3500 [==============================] - 1s 393us/step - loss: 1.9513 - categorical_accuracy: 0.1374 - val_loss: 1.9526 - val_categorical_accuracy: 0.1429\n",
      "Epoch 50/50\n",
      "3500/3500 [==============================] - 1s 363us/step - loss: 1.9533 - categorical_accuracy: 0.1291 - val_loss: 1.9510 - val_categorical_accuracy: 0.1429\n"
     ]
    }
   ],
   "source": [
    "x_test = pd.DataFrame(ffcoefs_test)\n",
    "x_train = pd.DataFrame(ffcoefs_train)\n",
    "y_train = ageClass_train\n",
    "y_test = ageClass_test\n",
    "\n",
    "y_train_class = np.copy(y_train)\n",
    "y_test_class = np.copy(y_test)\n",
    "#Traducción a clases\n",
    "classes = np.array([1, 5, 10, 16, 28, 51, 75])\n",
    "for i in range(7):\n",
    "    y_train_class[y_train_class == classes[i]] = i\n",
    "    y_test_class[y_test_class == classes[i]] = i\n",
    "\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_class,num_classes=7)#etiquetas categóricas\n",
    "y_test_onehot = keras.utils.to_categorical(y_test_class,num_classes=7)#etiquetas categóricas\n",
    "\n",
    "ffw_network = keras.models.Sequential()\n",
    "ffw_network.add(Dense(units = 256, kernel_initializer='glorot_uniform', input_dim = x_train.shape[1] ,activation = \"sigmoid\"))\n",
    "ffw_network.add(Dense(units = 256 ,kernel_initializer='glorot_uniform', activation = \"sigmoid\"))\n",
    "ffw_network.add(Dense(units = 256 ,kernel_initializer='glorot_uniform', activation = \"sigmoid\"))\n",
    "ffw_network.add(Dense(units = 256 ,kernel_initializer='glorot_uniform', activation = \"sigmoid\"))\n",
    "ffw_network.add(Dense(units = 7, kernel_initializer='glorot_uniform', activation = \"softmax\"))\n",
    "\n",
    "ffw_network.compile(optimizer=SGD(lr=0.01), loss='categorical_crossentropy', metrics = [keras.metrics.categorical_accuracy])\n",
    "\n",
    "history = ffw_network.fit(x_train, y_train_onehot, epochs = 50, batch_size=32, verbose =1, validation_data=(x_test, y_test_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 0s 123us/step\n",
      "[1.951013677687872, 0.14285714285714285]\n",
      "El error cuadratico medio es: 637.7142857142857\n"
     ]
    }
   ],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "print(ffw_network.evaluate(x_test, y_test_onehot))\n",
    "y_aprox_class = ffw_network.predict_classes(x_test)\n",
    "\n",
    "y_aprox = np.copy(y_aprox_class)\n",
    "for i in range(7):\n",
    "    y_aprox[y_aprox == i] = classes[i]\n",
    "#print(y_aprox)\n",
    "#print(y_test)\n",
    "print(\"El error cuadratico medio es:\",mse(y_aprox, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema como tarea de Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1050 samples\n",
      "Epoch 1/50\n",
      "3500/3500 [==============================] - 2s 578us/step - loss: 511.2256 - acc: 0.0200 - val_loss: 906.6819 - val_acc: 0.0333\n",
      "Epoch 2/50\n",
      "3500/3500 [==============================] - 1s 305us/step - loss: 397.8805 - acc: 0.0251 - val_loss: 854.8810 - val_acc: 0.0495\n",
      "Epoch 3/50\n",
      "3500/3500 [==============================] - 1s 355us/step - loss: 388.1403 - acc: 0.0337 - val_loss: 892.0030 - val_acc: 0.0657\n",
      "Epoch 4/50\n",
      "3500/3500 [==============================] - 1s 347us/step - loss: 377.1154 - acc: 0.0343 - val_loss: 887.0383 - val_acc: 0.0724\n",
      "Epoch 5/50\n",
      "3500/3500 [==============================] - 1s 374us/step - loss: 364.4534 - acc: 0.0311 - val_loss: 771.8360 - val_acc: 0.0048\n",
      "Epoch 6/50\n",
      "3500/3500 [==============================] - 1s 375us/step - loss: 349.0865 - acc: 0.0294 - val_loss: 736.7531 - val_acc: 0.0657\n",
      "Epoch 7/50\n",
      "3500/3500 [==============================] - 1s 340us/step - loss: 334.3504 - acc: 0.0646 - val_loss: 749.3473 - val_acc: 0.0324\n",
      "Epoch 8/50\n",
      "3500/3500 [==============================] - 2s 436us/step - loss: 322.4194 - acc: 0.0637 - val_loss: 750.6510 - val_acc: 0.0324\n",
      "Epoch 9/50\n",
      "3500/3500 [==============================] - 1s 419us/step - loss: 305.6148 - acc: 0.0329 - val_loss: 642.9066 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "3500/3500 [==============================] - 1s 328us/step - loss: 287.8195 - acc: 0.0300 - val_loss: 616.6580 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "3500/3500 [==============================] - 1s 399us/step - loss: 276.0853 - acc: 0.0251 - val_loss: 651.3730 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "3500/3500 [==============================] - 1s 398us/step - loss: 261.4648 - acc: 0.0374 - val_loss: 607.8104 - val_acc: 0.0381\n",
      "Epoch 13/50\n",
      "3500/3500 [==============================] - 1s 414us/step - loss: 248.8018 - acc: 0.0391 - val_loss: 605.1063 - val_acc: 0.0410\n",
      "Epoch 14/50\n",
      "3500/3500 [==============================] - 1s 372us/step - loss: 239.8775 - acc: 0.0371 - val_loss: 616.2125 - val_acc: 0.0076\n",
      "Epoch 15/50\n",
      "3500/3500 [==============================] - 1s 351us/step - loss: 229.5081 - acc: 0.0423 - val_loss: 638.9532 - val_acc: 9.5238e-04\n",
      "Epoch 16/50\n",
      "3500/3500 [==============================] - 2s 452us/step - loss: 218.3591 - acc: 0.0394 - val_loss: 609.5293 - val_acc: 0.0390\n",
      "Epoch 17/50\n",
      "3500/3500 [==============================] - 1s 407us/step - loss: 203.3056 - acc: 0.0474 - val_loss: 668.4264 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "3500/3500 [==============================] - 1s 367us/step - loss: 199.0719 - acc: 0.0526 - val_loss: 740.4841 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "3500/3500 [==============================] - 1s 372us/step - loss: 190.9200 - acc: 0.0597 - val_loss: 597.2687 - val_acc: 0.0314\n",
      "Epoch 20/50\n",
      "3500/3500 [==============================] - 1s 334us/step - loss: 181.5774 - acc: 0.0640 - val_loss: 598.0111 - val_acc: 0.0305\n",
      "Epoch 21/50\n",
      "3500/3500 [==============================] - 1s 424us/step - loss: 172.9528 - acc: 0.0529 - val_loss: 668.1188 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "3500/3500 [==============================] - 2s 500us/step - loss: 166.3594 - acc: 0.0571 - val_loss: 632.7649 - val_acc: 0.0019\n",
      "Epoch 23/50\n",
      "3500/3500 [==============================] - 1s 356us/step - loss: 160.0656 - acc: 0.0589 - val_loss: 783.6629 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "3500/3500 [==============================] - 1s 352us/step - loss: 149.2396 - acc: 0.0537 - val_loss: 792.6321 - val_acc: 9.5238e-04\n",
      "Epoch 25/50\n",
      "3500/3500 [==============================] - 1s 365us/step - loss: 143.2770 - acc: 0.0646 - val_loss: 690.8671 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "3500/3500 [==============================] - 1s 390us/step - loss: 137.7309 - acc: 0.0611 - val_loss: 752.4425 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "3500/3500 [==============================] - 2s 435us/step - loss: 129.4764 - acc: 0.0697 - val_loss: 644.2820 - val_acc: 9.5238e-04\n",
      "Epoch 28/50\n",
      "3500/3500 [==============================] - 1s 350us/step - loss: 126.4029 - acc: 0.0546 - val_loss: 721.5784 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "3500/3500 [==============================] - 1s 334us/step - loss: 121.9733 - acc: 0.0651 - val_loss: 677.3587 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "3500/3500 [==============================] - 1s 339us/step - loss: 117.3468 - acc: 0.0626 - val_loss: 697.0454 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "3500/3500 [==============================] - 1s 329us/step - loss: 103.9959 - acc: 0.0594 - val_loss: 635.5787 - val_acc: 0.0019\n",
      "Epoch 32/50\n",
      "3500/3500 [==============================] - 2s 435us/step - loss: 104.0110 - acc: 0.0571 - val_loss: 613.3262 - val_acc: 0.0029\n",
      "Epoch 33/50\n",
      "3500/3500 [==============================] - 1s 365us/step - loss: 103.6606 - acc: 0.0620 - val_loss: 615.2756 - val_acc: 0.0038\n",
      "Epoch 34/50\n",
      "3500/3500 [==============================] - 1s 356us/step - loss: 97.4327 - acc: 0.0697 - val_loss: 726.7634 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "3500/3500 [==============================] - 1s 330us/step - loss: 90.0691 - acc: 0.0674 - val_loss: 659.4637 - val_acc: 9.5238e-04\n",
      "Epoch 36/50\n",
      "3500/3500 [==============================] - 1s 414us/step - loss: 90.3831 - acc: 0.0614 - val_loss: 657.2831 - val_acc: 9.5238e-04\n",
      "Epoch 37/50\n",
      "3500/3500 [==============================] - 2s 526us/step - loss: 87.1645 - acc: 0.0677 - val_loss: 687.8776 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "3500/3500 [==============================] - 2s 475us/step - loss: 83.3751 - acc: 0.0671 - val_loss: 626.1648 - val_acc: 0.0019\n",
      "Epoch 39/50\n",
      "3500/3500 [==============================] - 1s 397us/step - loss: 79.1656 - acc: 0.0666 - val_loss: 723.5967 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "3500/3500 [==============================] - 2s 519us/step - loss: 76.4440 - acc: 0.0703 - val_loss: 653.1502 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "3500/3500 [==============================] - 2s 492us/step - loss: 75.7731 - acc: 0.0671 - val_loss: 630.0338 - val_acc: 0.0019\n",
      "Epoch 42/50\n",
      "3500/3500 [==============================] - 2s 477us/step - loss: 70.7288 - acc: 0.0689 - val_loss: 665.6134 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "3500/3500 [==============================] - 2s 535us/step - loss: 68.1842 - acc: 0.0740 - val_loss: 653.9737 - val_acc: 9.5238e-04\n",
      "Epoch 44/50\n",
      "3500/3500 [==============================] - 2s 464us/step - loss: 67.8575 - acc: 0.0751 - val_loss: 721.1264 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "3500/3500 [==============================] - 2s 429us/step - loss: 63.8114 - acc: 0.0751 - val_loss: 606.8683 - val_acc: 0.0029\n",
      "Epoch 46/50\n",
      "3500/3500 [==============================] - 2s 465us/step - loss: 61.8339 - acc: 0.0766 - val_loss: 594.7808 - val_acc: 0.0029\n",
      "Epoch 47/50\n",
      "3500/3500 [==============================] - 2s 450us/step - loss: 63.4966 - acc: 0.0706 - val_loss: 571.7515 - val_acc: 0.0067\n",
      "Epoch 48/50\n",
      "3500/3500 [==============================] - 1s 413us/step - loss: 54.6206 - acc: 0.0786 - val_loss: 615.6107 - val_acc: 9.5238e-04\n",
      "Epoch 49/50\n",
      "3500/3500 [==============================] - 2s 447us/step - loss: 56.3449 - acc: 0.0783 - val_loss: 617.5767 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "3500/3500 [==============================] - 1s 364us/step - loss: 53.4898 - acc: 0.0783 - val_loss: 752.5549 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "x_test = pd.DataFrame(ffcoefs_test)\n",
    "x_train = pd.DataFrame(ffcoefs_train)\n",
    "y_train = ageClass_train\n",
    "y_test = ageClass_test\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train_scaled = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n",
    "x_test_scaled = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
    "\n",
    "\n",
    "ffw2 = keras.models.Sequential()\n",
    "ffw2.add(Dense(256, kernel_initializer='uniform',input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "ffw2.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "ffw2.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "ffw2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "ffw2.compile(loss='mse', optimizer = \"rmsprop\", metrics = [\"accuracy\"])\n",
    "history = ffw2.fit(x_train_scaled, y_train, epochs = 50, verbose =1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
