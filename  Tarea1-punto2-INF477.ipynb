{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "#print(datos)\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste Training Set\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Ajuste Validation Set\n",
    "scaler2 = StandardScaler().fit(df_val)\n",
    "X_val_scaled = pd.DataFrame(scaler2.transform(df_val),columns=df_val.columns)\n",
    "y_val_scaled = X_val_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Ajuste Validation Set\n",
    "scaler3 = StandardScaler().fit(df_test)\n",
    "X_test_scaled = pd.DataFrame(scaler3.transform(df_test),columns=df_test.columns)\n",
    "y_test_scaled = X_test_scaled.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "yTest = df_test.pop('Eat').values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Para el mismo problema definido anteriormente (sección 1) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (loss) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de gradients (link) en el backend de Keras. Deberá generar un histograma para todos los pesos de cada capa antes y despues del entrenamiento con 250 epochs. Comente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a88fa18abb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlistOfVariableTensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistOfVariableTensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We can now calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Capa 1 entrenada')\n",
    "plt.hist(evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###calculate gradients\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "loss = keras.losses.mean_squared_error(model.layers[0].output,y_train_scaled)\n",
    "loss2 = keras.losses.mean_squared_error(model.layers[1].output,y_train_scaled)\n",
    "listOfVariableTensors = model.layers[0].trainable_weights \n",
    "list1fVariableTensors = model.layers[1].trainable_weights \n",
    "\n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "gradients2 = K.gradients(loss2, list1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss2,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients2 = sess.run(gradients2,feed_dict={model.input:X_train_scaled.values})\n",
    "\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "\n",
    "\n",
    "Loss = keras.losses.mean_squared_error(model2.layers[0].output,y_train_scaled)\n",
    "Loss2 = keras.losses.mean_squared_error(model2.layers[1].output,y_train_scaled)\n",
    "#loss = keras.losses.mean_squared_error(model2.output,y_train_scaled)--original\n",
    "\n",
    "ListOfVariableTensors = model2.layers[0].trainable_weights \n",
    "List1fVariableTensors = model2.layers[1].trainable_weights \n",
    "\n",
    "Gradients = K.gradients(Loss, ListOfVariableTensors) #We can now calculate the gradients.\n",
    "Gradients2 = K.gradients(Loss2,List1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "Evaluated_gradients = sess.run(Gradients,feed_dict={model2.input:X_train_scaled.values})\n",
    "Evaluated_gradients2 = sess.run(Gradients2,feed_dict={model2.input:X_train_scaled.values})\n",
    "\n",
    "Evaluated_gradients = [gradient/len(y_train) for gradient in Evaluated_gradients]\n",
    "Evaluated_gradients2 = [gradient/len(y_train) for gradient in Evaluated_gradients2]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(evaluated_gradients2, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.title('Capa 1 entrenada')\n",
    "plt.hist(Evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.title('Capa 2 entrenada')\n",
    "plt.hist(Evaluated_gradients2, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "#history2 = model2.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "history2=pd.read_csv(\"history2b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###calculate gradients\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "loss = keras.losses.mean_squared_error(model2.layers[0].output,y_train_scaled)\n",
    "loss2 = keras.losses.mean_squared_error(model2.layers[1].output,y_train_scaled)\n",
    "#loss = keras.losses.mean_squared_error(model2.output,y_train_scaled)--original\n",
    "\n",
    "listOfVariableTensors = model2.layers[0].trainable_weights \n",
    "list1fVariableTensors = model2.layers[1].trainable_weights \n",
    "\n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "gradients2 = K.gradients(loss2, list1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model2.input:X_train_scaled.values})\n",
    "evaluated_gradients2 = sess.run(gradients2,feed_dict={model2.input:X_train_scaled.values})\n",
    "\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients, bins = 160)\n",
    "plt.grid(True)\n",
    "plt.subplot(212)\n",
    "plt.title('Layer 2 training')\n",
    "plt.hist(evaluated_gradients2, bins = 160)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weightss, biases = model2.layers[0].get_weights()\n",
    "weightss1, biases1 = model2.layers[1].get_weights()\n",
    "weightse, biases = model.layers[0].get_weights()\n",
    "weights1e, biases1 = model.layers[1].get_weights()\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(221)\n",
    "plt.title('Weights Layer 1')\n",
    "plt.hist(weightss, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.title('Weights Layer 2')\n",
    "plt.hist(weightss1, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.title('Weights Layer 1 training')\n",
    "plt.hist(weightse, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.title('Weights Layer 2 training')\n",
    "plt.hist(weights1e, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               326656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 326,913\n",
      "Trainable params: 326,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()\n",
    "result= pd.DataFrame(history2.history)\n",
    "result.to_csv(\"history2b.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento pero ahora entrenando una red mucho más profunda de 6 capas, 5 capas escondidas y 1 de salida. Utilice el inicializador de pesos *uniform* el cual inicializa mediante una distribución uniforme entre $-1/\\sqrt{N}$ y $1/\\sqrt{N}$ para cada capa, con $N$ el número de neuronas de la capa anterior. Por simplicidad visualice las 3-4 primeras capas de la red. Comente si observa el efecto del *gradiente desvaneciente* antes y/o después de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = Sequential()\n",
    "modelc.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelc.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "#historyc = model2.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "#result= pd.DataFrame(historyc.history)\n",
    "#result.to_csv(\"history2c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc2 = Sequential()\n",
    "modelc2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelc2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyc=pd.read_csv(\"history2c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-45b7825b1109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mEvaluated_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodelc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mEvaluated_gradients2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradients2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodelc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mEvaluated_gradients3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradients3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodelc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1113\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \"\"\"\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[0;32m--> 237\u001b[0;31m                       (fetch, type(fetch)))\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "###calculate gradients\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "loss = keras.losses.mean_squared_error(modelc.layers[0].output,y_train_scaled)\n",
    "loss2 = keras.losses.mean_squared_error(modelc.layers[1].output,y_train_scaled)\n",
    "loss3 = keras.losses.mean_squared_error(modelc.layers[2].output,y_train_scaled)\n",
    "listOfVariableTensors = modelc.layers[0].trainable_weights \n",
    "list1fVariableTensors = modelc.layers[1].trainable_weights \n",
    "list2fVariableTensors = modelc.layers[2].trainable_weights \n",
    "\n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "gradients2 = K.gradients(loss2, list1fVariableTensors) \n",
    "gradients3 = K.gradients(loss3, list1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss2,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelc.input:X_train_scaled.values})\n",
    "evaluated_gradients2 = sess.run(gradients2,feed_dict={modelc.input:X_train_scaled.values})\n",
    "evaluated_gradients3 = sess.run(gradients3,feed_dict={modelc.input:X_train_scaled.values})\n",
    "\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "evaluated_gradients3 = [gradient/len(y_train) for gradient in evaluated_gradients3]\n",
    "\n",
    "#################################################################\n",
    "Loss = keras.losses.mean_squared_error(modelc.layers[0].output,y_train_scaled)\n",
    "Loss2 = keras.losses.mean_squared_error(modelc.layers[1].output,y_train_scaled)\n",
    "Loss3 = keras.losses.mean_squared_error(modelc.layers[2].output,y_train_scaled)\n",
    "ListOfVariableTensors = modelc2.layers[0].trainable_weights \n",
    "List1fVariableTensors = modelc2.layers[1].trainable_weights \n",
    "List2fVariableTensors = modelc2.layers[2].trainable_weights \n",
    "\n",
    "Gradients = K.gradients(Loss, ListOfVariableTensors) #We can now calculate the gradients.\n",
    "Gradients2 = K.gradients(Loss2,List1fVariableTensors) \n",
    "Gradients3 = K.gradients(Loss3, List1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss2,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "Evaluated_gradients = sess.run(Gradients,feed_dict={modelc2.input:X_train_scaled.values})\n",
    "Evaluated_gradients2 = sess.run(Gradients2,feed_dict={modelc2.input:X_train_scaled.values})\n",
    "Evaluated_gradients3 = sess.run(Gradients3,feed_dict={modelc2.input:X_train_scaled.values})\n",
    "\n",
    "Evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "Evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "Evaluated_gradients3 = [gradient/len(y_train) for gradient in evaluated_gradients3]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(323)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(evaluated_gradients2[0][:], bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(325)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients3, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(322)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(Evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(324)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(Evaluated_gradients2, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(326)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(Evaluated_gradients3, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightss, biases = modelc.layers[0].get_weights()\n",
    "weightss1, biases1 = modelc.layers[1].get_weights()\n",
    "weightse, biases = modelc.layers[2].get_weights()\n",
    "weights1e, biases1 = modelc.layers[3].get_weights()\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(221)\n",
    "plt.title('Weights Layer 1')\n",
    "plt.hist(weightss, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.title('Weights Layer 2')\n",
    "plt.hist(weightss1, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.title('Weights Layer 1 training')\n",
    "plt.hist(weightse, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.title('Weights Layer 2 training')\n",
    "plt.hist(weights1e, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">d) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento, pero ahora entrenando la red profunda con el inicializador de Glorot [[1]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/(N_{in}+N_{out})}$  y $\\sqrt{6/(N_{in}+N_{out})}$ . Por simplicidad visualice las 3-4 primeras capas de la red. Comente si el efecto del *gradiente desvaneciente* se amortigua antes y/o después de entrenar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 8s 844us/step - loss: 15.4202 - val_loss: 13.6638\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 8.2270 - val_loss: 2.5299\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 2.3861 - val_loss: 4.4246\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 6s 607us/step - loss: 1.5479 - val_loss: 0.7078\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 6s 636us/step - loss: 1.2048 - val_loss: 0.6749\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.9604 - val_loss: 0.5603\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 603us/step - loss: 0.8813 - val_loss: 1.3545\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.6560 - val_loss: 0.8100\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.5867 - val_loss: 0.3179\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.5460 - val_loss: 1.1003\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.4969 - val_loss: 0.5310\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 6s 615us/step - loss: 0.3868 - val_loss: 0.4820\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 8s 843us/step - loss: 0.3872 - val_loss: 0.3704\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.3615 - val_loss: 0.4261\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.3329 - val_loss: 0.2262\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 7s 752us/step - loss: 0.2926 - val_loss: 0.1764\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 7s 762us/step - loss: 0.2604 - val_loss: 0.1600\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.2729 - val_loss: 0.1587\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.2325 - val_loss: 0.5823\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.2396 - val_loss: 0.4334\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.2294 - val_loss: 0.1309\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.1765 - val_loss: 0.1172\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 6s 630us/step - loss: 0.1727 - val_loss: 0.1181\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.1862 - val_loss: 0.6345\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 5s 531us/step - loss: 0.1774 - val_loss: 0.1178\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.1668 - val_loss: 0.1072\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.1320 - val_loss: 0.1492\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.1437 - val_loss: 0.1086\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.1614 - val_loss: 0.3342\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 6s 587us/step - loss: 0.1245 - val_loss: 0.1143\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.1145 - val_loss: 0.1213\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.1260 - val_loss: 0.0882\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1358 - val_loss: 0.1143\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.1057 - val_loss: 0.0920\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.1066 - val_loss: 0.2801\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.1080 - val_loss: 0.1081\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.1053 - val_loss: 0.0991\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0962 - val_loss: 0.0751\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.1002 - val_loss: 0.0935\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 6s 661us/step - loss: 0.0965 - val_loss: 0.0714\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 6s 600us/step - loss: 0.0908 - val_loss: 0.0885\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0938 - val_loss: 0.0708\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0898 - val_loss: 0.0757\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 5s 465us/step - loss: 0.0833 - val_loss: 0.0888\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 5s 533us/step - loss: 0.0948 - val_loss: 0.1609\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 6s 611us/step - loss: 0.0860 - val_loss: 0.1238\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0729 - val_loss: 0.1080\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.0734 - val_loss: 0.1464\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 6s 588us/step - loss: 0.0844 - val_loss: 0.0630\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0773 - val_loss: 0.0706\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 0.0675 - val_loss: 0.0890\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.0834 - val_loss: 0.0704\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0649 - val_loss: 0.0637\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0625 - val_loss: 0.1061\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 8s 807us/step - loss: 0.0769 - val_loss: 0.0969\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0797 - val_loss: 0.0598\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 7s 736us/step - loss: 0.0739 - val_loss: 0.0835\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 7s 757us/step - loss: 0.0633 - val_loss: 0.0577\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 8s 819us/step - loss: 0.0582 - val_loss: 0.0804\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 7s 741us/step - loss: 0.0637 - val_loss: 0.0637\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0706 - val_loss: 0.0683\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0619 - val_loss: 0.0556\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 6s 594us/step - loss: 0.0672 - val_loss: 0.0525\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0598 - val_loss: 0.1147\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0591 - val_loss: 0.0586\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 8s 804us/step - loss: 0.0555 - val_loss: 0.1076\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0644 - val_loss: 0.0561\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0569 - val_loss: 0.0629\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 5s 552us/step - loss: 0.0564 - val_loss: 0.0706\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0631 - val_loss: 0.0736\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 6s 618us/step - loss: 0.0641 - val_loss: 0.0591\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0564 - val_loss: 0.0967\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 8s 823us/step - loss: 0.0514 - val_loss: 0.0522\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0482 - val_loss: 0.0509\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0534 - val_loss: 0.0605\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0524 - val_loss: 0.0471\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 9s 909us/step - loss: 0.0443 - val_loss: 0.0769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 9s 896us/step - loss: 0.0574 - val_loss: 0.0468\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 9s 873us/step - loss: 0.0497 - val_loss: 0.0690\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 9s 898us/step - loss: 0.0474 - val_loss: 0.0924\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 9s 875us/step - loss: 0.0463 - val_loss: 0.0510\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0530 - val_loss: 0.0567\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0501 - val_loss: 0.0705\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 10s 975us/step - loss: 0.0468 - val_loss: 0.0593\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 10s 978us/step - loss: 0.0511 - val_loss: 0.0763\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 7s 714us/step - loss: 0.0457 - val_loss: 0.0615\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 5s 492us/step - loss: 0.0484 - val_loss: 0.0449\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0494 - val_loss: 0.0534\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0424 - val_loss: 0.0483\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0467 - val_loss: 0.0487\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0436 - val_loss: 0.0541\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 6s 623us/step - loss: 0.0456 - val_loss: 0.0517\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0416 - val_loss: 0.0462\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0441 - val_loss: 0.1405\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0438 - val_loss: 0.0444\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0424 - val_loss: 0.0791\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 5s 536us/step - loss: 0.0489 - val_loss: 0.0586\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 5s 494us/step - loss: 0.0385 - val_loss: 0.0528\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 6s 591us/step - loss: 0.0385 - val_loss: 0.0534\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0383 - val_loss: 0.0434\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 5s 503us/step - loss: 0.0406 - val_loss: 0.0414\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.0410 - val_loss: 0.0638\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 5s 478us/step - loss: 0.0410 - val_loss: 0.0460\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0443 - val_loss: 0.0425\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0391 - val_loss: 0.0432\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 7s 736us/step - loss: 0.0417 - val_loss: 0.0509\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 8s 787us/step - loss: 0.0374 - val_loss: 0.0434\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0366 - val_loss: 0.0529\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0407 - val_loss: 0.0624\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0348 - val_loss: 0.0424\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.0372 - val_loss: 0.0402\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0434 - val_loss: 0.0541\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0400 - val_loss: 0.0513\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.0365 - val_loss: 0.0426\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0414 - val_loss: 0.0465\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 719us/step - loss: 0.0399 - val_loss: 0.0435\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 6s 662us/step - loss: 0.0373 - val_loss: 0.0752\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 6s 641us/step - loss: 0.0338 - val_loss: 0.0469\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 8s 816us/step - loss: 0.0347 - val_loss: 0.0388\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 7s 713us/step - loss: 0.0354 - val_loss: 0.0511\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 8s 785us/step - loss: 0.0380 - val_loss: 0.0422\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 8s 778us/step - loss: 0.0337 - val_loss: 0.0522\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.0386 - val_loss: 0.0428\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 8s 854us/step - loss: 0.0324 - val_loss: 0.0404\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0348 - val_loss: 0.0411\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 10s 1000us/step - loss: 0.0378 - val_loss: 0.0511\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 10s 984us/step - loss: 0.0348 - val_loss: 0.0501\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 9s 894us/step - loss: 0.0352 - val_loss: 0.0588\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.0398 - val_loss: 0.0454\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 9s 972us/step - loss: 0.0356 - val_loss: 0.0446\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 9s 968us/step - loss: 0.0300 - val_loss: 0.0407\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 9s 953us/step - loss: 0.0290 - val_loss: 0.0404\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 10s 998us/step - loss: 0.0312 - val_loss: 0.0560\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0345 - val_loss: 0.0374\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.0356 - val_loss: 0.0383\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 9s 894us/step - loss: 0.0342 - val_loss: 0.0400\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0275 - val_loss: 0.0694\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 9s 970us/step - loss: 0.0318 - val_loss: 0.0373\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0316 - val_loss: 0.0382\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 9s 891us/step - loss: 0.0302 - val_loss: 0.0402\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.0325 - val_loss: 0.0699\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.0360 - val_loss: 0.0455\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.0323 - val_loss: 0.0367\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 9s 873us/step - loss: 0.0284 - val_loss: 0.0372\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0303 - val_loss: 0.0460\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0339 - val_loss: 0.0428\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.0321 - val_loss: 0.0387\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0331 - val_loss: 0.0389\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 10s 981us/step - loss: 0.0302 - val_loss: 0.0407\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0329 - val_loss: 0.0447\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0273 - val_loss: 0.0390\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 9s 963us/step - loss: 0.0324 - val_loss: 0.0412\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 9s 959us/step - loss: 0.0290 - val_loss: 0.0378\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0299 - val_loss: 0.0389\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.0292 - val_loss: 0.0358\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 6s 626us/step - loss: 0.0296 - val_loss: 0.0507\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0293 - val_loss: 0.0660\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 8s 865us/step - loss: 0.0313 - val_loss: 0.0463\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.0276 - val_loss: 0.0363\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 8s 831us/step - loss: 0.0362 - val_loss: 0.0487\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 6s 615us/step - loss: 0.0281 - val_loss: 0.0413\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.0269 - val_loss: 0.0800\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.0281 - val_loss: 0.0441\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.0320 - val_loss: 0.0347\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0287 - val_loss: 0.0459\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0280 - val_loss: 0.0363\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0287 - val_loss: 0.0439\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 6s 576us/step - loss: 0.0268 - val_loss: 0.0524\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 8s 781us/step - loss: 0.0297 - val_loss: 0.0354\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 8s 772us/step - loss: 0.0271 - val_loss: 0.0339\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.0295 - val_loss: 0.0362\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0266 - val_loss: 0.0481\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 6s 652us/step - loss: 0.0286 - val_loss: 0.0420\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0255 - val_loss: 0.0365\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0266 - val_loss: 0.0381\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0270 - val_loss: 0.0422\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0268 - val_loss: 0.0348\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 5s 467us/step - loss: 0.0276 - val_loss: 0.0436\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0293 - val_loss: 0.0338\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0277 - val_loss: 0.0346\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 5s 510us/step - loss: 0.0252 - val_loss: 0.0552\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0273 - val_loss: 0.0689\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 6s 594us/step - loss: 0.0299 - val_loss: 0.0596\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0258 - val_loss: 0.0351\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0254 - val_loss: 0.0351\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0251 - val_loss: 0.0393\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 5s 510us/step - loss: 0.0265 - val_loss: 0.0624\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0242 - val_loss: 0.0414\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 5s 559us/step - loss: 0.0260 - val_loss: 0.0376\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0276 - val_loss: 0.0344\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0249 - val_loss: 0.0346\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 5s 525us/step - loss: 0.0257 - val_loss: 0.0491\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.0253 - val_loss: 0.0760\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0243 - val_loss: 0.0529\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 5s 562us/step - loss: 0.0243 - val_loss: 0.0339\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0273 - val_loss: 0.0479\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0248 - val_loss: 0.0580\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 6s 599us/step - loss: 0.0242 - val_loss: 0.0329\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.0244 - val_loss: 0.0354\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0233 - val_loss: 0.0386\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 6s 593us/step - loss: 0.0258 - val_loss: 0.0356\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0248 - val_loss: 0.0384\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0267 - val_loss: 0.0329\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0261 - val_loss: 0.0336\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0270 - val_loss: 0.0465\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0259 - val_loss: 0.0645\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0245 - val_loss: 0.0358\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 5s 550us/step - loss: 0.0234 - val_loss: 0.0374\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0252 - val_loss: 0.0316\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0253 - val_loss: 0.0505\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 5s 506us/step - loss: 0.0218 - val_loss: 0.0365\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0248 - val_loss: 0.0323\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 0.0225 - val_loss: 0.0332\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.0239 - val_loss: 0.0335\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0212 - val_loss: 0.0456\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0260 - val_loss: 0.0460\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0233 - val_loss: 0.0329\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.0262 - val_loss: 0.0340\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 533us/step - loss: 0.0253 - val_loss: 0.0540\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.0229 - val_loss: 0.0405\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0250 - val_loss: 0.0621\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 7s 686us/step - loss: 0.0254 - val_loss: 0.0333\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0255 - val_loss: 0.0332\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0240 - val_loss: 0.0336\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0220 - val_loss: 0.0369\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0215 - val_loss: 0.0576\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 5s 489us/step - loss: 0.0243 - val_loss: 0.0321\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 514us/step - loss: 0.0223 - val_loss: 0.0432\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.0265 - val_loss: 0.0417\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 6s 568us/step - loss: 0.0218 - val_loss: 0.0341\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 521us/step - loss: 0.0229 - val_loss: 0.0322\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0246 - val_loss: 0.0331\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0210 - val_loss: 0.0331\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0233 - val_loss: 0.0426\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0237 - val_loss: 0.0319\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0237 - val_loss: 0.0329\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0226 - val_loss: 0.0329\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 477us/step - loss: 0.0222 - val_loss: 0.0409\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 6s 616us/step - loss: 0.0237 - val_loss: 0.0338\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0224 - val_loss: 0.0346\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 5s 563us/step - loss: 0.0209 - val_loss: 0.0332\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0217 - val_loss: 0.0458\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 7s 756us/step - loss: 0.0219 - val_loss: 0.0540\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 624us/step - loss: 0.0224 - val_loss: 0.0320\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 547us/step - loss: 0.0226 - val_loss: 0.0444\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.0210 - val_loss: 0.0356\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 7s 737us/step - loss: 0.0206 - val_loss: 0.0335\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0228 - val_loss: 0.0356\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0199 - val_loss: 0.0371\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0220 - val_loss: 0.0313\n"
     ]
    }
   ],
   "source": [
    "modeld = Sequential()\n",
    "modeld.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modeld.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyd = modeld.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resultd= pd.DataFrame(historyd.history)\n",
    "resultd.to_csv(\"history2d.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Vuelva a repetir la experimentación ahora cambiando la función de activación por ReLU, es decir, deberá visualizar los gradientes de los pesos de cada capa antes y después del entrenamiento, con inicialización *uniform* y comparar con la inicialización de He [[2]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/N_{in}}$ y $\\sqrt{6/N_{in}} $. Comente si ocurre el mismo fenómeno anterior (para función sigmoidal) sobre el efecto del *gradiente desvaneciente* para la función ReLU. Explique la importancia de la inicialización de los pesos dependiendo de la arquitectura.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "...\n",
    "or\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='relu')) #he\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 27.4344 - val_loss: 2.8600\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.9867 - val_loss: 1.1283\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.5372 - val_loss: 1.5091\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.4210 - val_loss: 1.1276\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 6s 634us/step - loss: 0.3344 - val_loss: 0.7306\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.2691 - val_loss: 0.8697\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.2534 - val_loss: 0.8071\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.2207 - val_loss: 0.6698\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.1865 - val_loss: 0.6007\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.2037 - val_loss: 0.5931\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.1855 - val_loss: 0.5772\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.1759 - val_loss: 0.5993\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 6s 588us/step - loss: 0.1535 - val_loss: 0.4717\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.1516 - val_loss: 1.5862\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1355 - val_loss: 0.7435\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.1242 - val_loss: 0.5200\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.1102 - val_loss: 0.5257\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.1243 - val_loss: 0.4474\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1129 - val_loss: 0.4343\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 6s 630us/step - loss: 0.1053 - val_loss: 0.4304\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.1081 - val_loss: 0.4470\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.1094 - val_loss: 0.4190\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0940 - val_loss: 0.4378\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 6s 604us/step - loss: 0.1187 - val_loss: 0.4998\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0962 - val_loss: 0.4856\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 0.0947 - val_loss: 0.5427\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0903 - val_loss: 0.4063\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0858 - val_loss: 0.4867\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.0885 - val_loss: 0.5411\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0777 - val_loss: 0.4092\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0929 - val_loss: 0.3641\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0882 - val_loss: 0.4405\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0752 - val_loss: 0.4195\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.0726 - val_loss: 0.5243\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.0658 - val_loss: 0.5708\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0750 - val_loss: 0.5328\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0667 - val_loss: 0.8558\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.0766 - val_loss: 0.3785\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 7s 677us/step - loss: 0.0634 - val_loss: 0.3520\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0649 - val_loss: 0.3878\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0668 - val_loss: 0.3400\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0639 - val_loss: 0.3678\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0636 - val_loss: 0.3416\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0623 - val_loss: 0.3334\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 9s 896us/step - loss: 0.0600 - val_loss: 0.3361\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 0.0554 - val_loss: 0.3354\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.0555 - val_loss: 0.3388\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0562 - val_loss: 0.3784\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.0508 - val_loss: 0.3581\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0562 - val_loss: 0.7160\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0604 - val_loss: 0.4663\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0562 - val_loss: 0.3887\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 8s 857us/step - loss: 0.0516 - val_loss: 0.4434\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0507 - val_loss: 0.3382\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0509 - val_loss: 0.3520\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 0.0490 - val_loss: 0.3652\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0466 - val_loss: 0.3178\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 6s 576us/step - loss: 0.0527 - val_loss: 0.3269\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0479 - val_loss: 0.3419\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 6s 593us/step - loss: 0.0516 - val_loss: 0.4133\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0444 - val_loss: 0.3155\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 6s 600us/step - loss: 0.0462 - val_loss: 0.3955\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 7s 692us/step - loss: 0.0461 - val_loss: 0.3159\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0430 - val_loss: 0.3517\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0457 - val_loss: 0.3141\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0456 - val_loss: 0.3167\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0432 - val_loss: 0.3203\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 7s 705us/step - loss: 0.0873 - val_loss: 0.4329\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0456 - val_loss: 0.3465\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0425 - val_loss: 0.3079\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0417 - val_loss: 0.3443\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.0405 - val_loss: 0.3154\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 0.0461 - val_loss: 0.3097\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0411 - val_loss: 0.3205\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0374 - val_loss: 0.3316\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0381 - val_loss: 0.2913\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 6s 624us/step - loss: 0.0374 - val_loss: 0.3275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0413 - val_loss: 0.2902\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0421 - val_loss: 0.2978\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 6s 654us/step - loss: 0.0385 - val_loss: 0.2980\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 5s 508us/step - loss: 0.0478 - val_loss: 0.3138\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0373 - val_loss: 0.3032\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 7s 734us/step - loss: 0.0355 - val_loss: 0.3118\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0383 - val_loss: 0.3205\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.0370 - val_loss: 0.3064\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 9s 951us/step - loss: 0.0382 - val_loss: 0.2913\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 9s 965us/step - loss: 0.0437 - val_loss: 0.3126\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 8s 783us/step - loss: 0.0419 - val_loss: 0.2976\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 769us/step - loss: 0.0363 - val_loss: 0.2939\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.0385 - val_loss: 0.2963\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 8s 795us/step - loss: 0.0372 - val_loss: 0.3050\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 5s 539us/step - loss: 0.0313 - val_loss: 0.3327\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0356 - val_loss: 0.2937\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 7s 730us/step - loss: 0.0317 - val_loss: 0.3130\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0374 - val_loss: 0.2817\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0329 - val_loss: 0.3970\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0346 - val_loss: 0.3024\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0316 - val_loss: 0.2884\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0320 - val_loss: 0.3166\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.0366 - val_loss: 0.2922\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0336 - val_loss: 0.2925\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 8s 847us/step - loss: 0.0337 - val_loss: 0.2882\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0358 - val_loss: 0.2943\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0342 - val_loss: 0.2870\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0346 - val_loss: 0.2862\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.0341 - val_loss: 0.3093\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.0341 - val_loss: 0.2774\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0307 - val_loss: 0.3988\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 9s 886us/step - loss: 0.0306 - val_loss: 0.2922\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 9s 960us/step - loss: 0.0312 - val_loss: 0.3106\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 9s 928us/step - loss: 0.0331 - val_loss: 0.3761\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0316 - val_loss: 0.2781\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0305 - val_loss: 0.2788\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0327 - val_loss: 0.2860\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 8s 790us/step - loss: 0.0283 - val_loss: 0.2877\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.0275 - val_loss: 0.2787\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 8s 813us/step - loss: 0.0284 - val_loss: 0.3140\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.0270 - val_loss: 0.2882\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0288 - val_loss: 0.2799\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.0308 - val_loss: 0.2899\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0284 - val_loss: 0.2917\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 9s 974us/step - loss: 0.0297 - val_loss: 0.2869\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0293 - val_loss: 0.3062\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 9s 974us/step - loss: 0.0306 - val_loss: 0.2817\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0287 - val_loss: 0.3759\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0297 - val_loss: 0.3010\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 10s 996us/step - loss: 0.0290 - val_loss: 0.2729\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.0261 - val_loss: 0.2851\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 8s 839us/step - loss: 0.0253 - val_loss: 0.2880\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0283 - val_loss: 0.3004\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 9s 895us/step - loss: 0.0346 - val_loss: 0.2711\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0301 - val_loss: 0.2846\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 8s 824us/step - loss: 0.0321 - val_loss: 0.2990\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 7s 729us/step - loss: 0.0288 - val_loss: 0.3015\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 7s 686us/step - loss: 0.0277 - val_loss: 0.2872\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 6s 640us/step - loss: 0.0297 - val_loss: 0.2836\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0283 - val_loss: 0.2909\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0614 - val_loss: 0.3006\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 7s 676us/step - loss: 0.0261 - val_loss: 0.2880\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 8s 864us/step - loss: 0.0287 - val_loss: 0.4402\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 7s 715us/step - loss: 0.0265 - val_loss: 0.2881\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.0270 - val_loss: 0.2956\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 7s 719us/step - loss: 0.0294 - val_loss: 0.3241\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 7s 696us/step - loss: 0.0249 - val_loss: 0.2890\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0249 - val_loss: 0.2772\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0258 - val_loss: 0.2895\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0250 - val_loss: 0.2931\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0237 - val_loss: 0.2888\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.0227 - val_loss: 0.2916\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0290 - val_loss: 0.3055\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0272 - val_loss: 0.2774\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0250 - val_loss: 0.2697\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 6s 599us/step - loss: 0.0252 - val_loss: 0.3183\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0243 - val_loss: 0.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0241 - val_loss: 0.2928\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 9s 890us/step - loss: 0.0217 - val_loss: 0.2834\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 9s 926us/step - loss: 0.0248 - val_loss: 0.2714\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 9s 918us/step - loss: 0.0246 - val_loss: 0.2818\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 9s 971us/step - loss: 0.0245 - val_loss: 0.2760\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 9s 971us/step - loss: 0.0236 - val_loss: 0.2881\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0238 - val_loss: 0.2845\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.0237 - val_loss: 0.2957\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0267 - val_loss: 0.2867\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0242 - val_loss: 0.2822\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 9s 908us/step - loss: 0.0219 - val_loss: 0.2745\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 10s 976us/step - loss: 0.0316 - val_loss: 0.2893\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 9s 940us/step - loss: 0.0247 - val_loss: 0.2940\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 9s 964us/step - loss: 0.0237 - val_loss: 0.2911\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0214 - val_loss: 0.2954\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0245 - val_loss: 0.3320\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0214 - val_loss: 0.2856\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0237 - val_loss: 0.2882\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0254 - val_loss: 0.2994\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 10s 995us/step - loss: 0.0226 - val_loss: 0.3000\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0242 - val_loss: 0.2865\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0237 - val_loss: 0.2910\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 9s 956us/step - loss: 0.0220 - val_loss: 0.2819\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 0.0253 - val_loss: 0.3027\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0242 - val_loss: 0.2898\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0223 - val_loss: 0.2771\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0206 - val_loss: 0.2642\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0209 - val_loss: 0.2813\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0234 - val_loss: 0.2976\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0222 - val_loss: 0.2716\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0225 - val_loss: 0.2995\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 10s 982us/step - loss: 0.0205 - val_loss: 0.2872\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0208 - val_loss: 0.2936\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0242 - val_loss: 0.2910\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 7s 683us/step - loss: 0.0220 - val_loss: 0.2747\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 652us/step - loss: 0.0205 - val_loss: 0.2776\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0216 - val_loss: 0.3495\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 7s 748us/step - loss: 0.0211 - val_loss: 0.2954\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0196 - val_loss: 0.2783\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 8s 775us/step - loss: 0.0212 - val_loss: 0.2921\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.0204 - val_loss: 0.2715\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0202 - val_loss: 0.2888\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0209 - val_loss: 0.2803\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.0256 - val_loss: 0.2785\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0217 - val_loss: 0.3037\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0200 - val_loss: 0.2802\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 8s 847us/step - loss: 0.0222 - val_loss: 0.2830\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0202 - val_loss: 0.2890\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0214 - val_loss: 0.2792\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0208 - val_loss: 0.2750\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0236 - val_loss: 0.2807\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.0214 - val_loss: 0.2822\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0206 - val_loss: 0.3176\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.0211 - val_loss: 0.2836\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0188 - val_loss: 0.2816\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 7s 768us/step - loss: 0.0223 - val_loss: 0.2772\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0200 - val_loss: 0.2939\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.0200 - val_loss: 0.2999\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0204 - val_loss: 0.2856\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0193 - val_loss: 0.2777\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.0196 - val_loss: 0.2708\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 6s 568us/step - loss: 0.0215 - val_loss: 0.3170\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0191 - val_loss: 0.3173\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 6s 570us/step - loss: 0.0192 - val_loss: 0.2836\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0192 - val_loss: 0.2770\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0195 - val_loss: 0.2764\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 9s 891us/step - loss: 0.0206 - val_loss: 0.3056\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0195 - val_loss: 0.3421\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0212 - val_loss: 0.2895\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.0184 - val_loss: 0.2782\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.0196 - val_loss: 0.2962\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0176 - val_loss: 0.2808\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0192 - val_loss: 0.2891\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0229 - val_loss: 0.2814\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0201 - val_loss: 0.2831\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 8s 825us/step - loss: 0.0198 - val_loss: 0.2883\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0179 - val_loss: 0.2940\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0193 - val_loss: 0.2943\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.0178 - val_loss: 0.2822\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 7s 669us/step - loss: 0.0198 - val_loss: 0.3247\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0184 - val_loss: 0.2937\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 8s 812us/step - loss: 0.0173 - val_loss: 0.2851\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0170 - val_loss: 0.2815\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 8s 782us/step - loss: 0.0183 - val_loss: 0.2805\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.0177 - val_loss: 0.2922\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.0187 - val_loss: 0.2871\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 7s 698us/step - loss: 0.0185 - val_loss: 0.2867\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 8s 787us/step - loss: 0.0177 - val_loss: 0.2803\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0184 - val_loss: 0.3042\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0183 - val_loss: 0.2983\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0185 - val_loss: 0.2808\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.0184 - val_loss: 0.2899\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.0179 - val_loss: 0.3051\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 9s 937us/step - loss: 0.0206 - val_loss: 0.2841\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 8s 836us/step - loss: 0.0178 - val_loss: 0.2882\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.0175 - val_loss: 0.2826\n"
     ]
    }
   ],
   "source": [
    "modele = Sequential()\n",
    "modele.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "modele.add(Dense(256,  kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.001)\n",
    "modele.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historye = modele.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resulte= pd.DataFrame(historyd.history)\n",
    "resulte.to_csv(\"history2e(uniform).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 15.9934 - val_loss: 4.1458\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 1.8212 - val_loss: 3.2474\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 1.0384 - val_loss: 2.2443\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.6994 - val_loss: 2.0361\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.5443 - val_loss: 1.9875\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.4534 - val_loss: 1.7690\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.3952 - val_loss: 1.6390\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.3391 - val_loss: 1.6707\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.3057 - val_loss: 1.5853\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2787 - val_loss: 1.4733\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2561 - val_loss: 1.4612\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2421 - val_loss: 1.4548\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2179 - val_loss: 1.3559\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2071 - val_loss: 1.5436\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1952 - val_loss: 1.2635\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1873 - val_loss: 1.3319\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1782 - val_loss: 1.2781\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1749 - val_loss: 1.2374\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1631 - val_loss: 1.2542\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1561 - val_loss: 1.1899\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 24s 2ms/step - loss: 0.1504 - val_loss: 1.1432\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1455 - val_loss: 1.1136\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1367 - val_loss: 1.0669\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1348 - val_loss: 1.1734\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1304 - val_loss: 1.0922\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 24s 2ms/step - loss: 0.1295 - val_loss: 1.1963\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1275 - val_loss: 1.1409\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1226 - val_loss: 1.0659\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1212 - val_loss: 1.0489\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1144 - val_loss: 1.0916\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1095 - val_loss: 1.0790\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1056 - val_loss: 1.0463\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.1034 - val_loss: 0.9995\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.1060 - val_loss: 0.9344\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0998 - val_loss: 1.0236\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0981 - val_loss: 0.9802\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0958 - val_loss: 1.0135\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0973 - val_loss: 0.9425\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0916 - val_loss: 0.9414\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0914 - val_loss: 0.9710\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0882 - val_loss: 0.9522\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0864 - val_loss: 0.9235\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0842 - val_loss: 0.9116\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0847 - val_loss: 0.8926\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0829 - val_loss: 0.8964\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0806 - val_loss: 0.8928\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0784 - val_loss: 0.8807\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0798 - val_loss: 0.8784\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0765 - val_loss: 0.8906\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0737 - val_loss: 0.8457\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0733 - val_loss: 0.8123\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0725 - val_loss: 0.8206\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0719 - val_loss: 0.8238\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0710 - val_loss: 0.8400\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0694 - val_loss: 0.8602\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0681 - val_loss: 0.8122\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0670 - val_loss: 0.8055\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0675 - val_loss: 0.7823\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0675 - val_loss: 0.8352\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0657 - val_loss: 0.7625\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0643 - val_loss: 0.7820\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0636 - val_loss: 0.7937\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0617 - val_loss: 0.7754\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0644 - val_loss: 0.7449\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0631 - val_loss: 0.7995\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0605 - val_loss: 0.7619\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0608 - val_loss: 0.7472\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0608 - val_loss: 0.7568\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0603 - val_loss: 0.8454\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0593 - val_loss: 0.6987\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0581 - val_loss: 0.7549\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0593 - val_loss: 0.7252\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0585 - val_loss: 0.6976\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0569 - val_loss: 0.7052\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0570 - val_loss: 0.7130\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0558 - val_loss: 0.7169\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0548 - val_loss: 0.6749\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0547 - val_loss: 0.6810\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0533 - val_loss: 0.6896\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0535 - val_loss: 0.7163\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0526 - val_loss: 0.6997\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0524 - val_loss: 0.6791\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0519 - val_loss: 0.6661\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0505 - val_loss: 0.6793\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0512 - val_loss: 0.6490\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0503 - val_loss: 0.6646\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0493 - val_loss: 0.6943\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0499 - val_loss: 0.6650\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0493 - val_loss: 0.6501\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0485 - val_loss: 0.6601\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0483 - val_loss: 0.6610\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0477 - val_loss: 0.6312\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0469 - val_loss: 0.6719\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0465 - val_loss: 0.6427\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0494 - val_loss: 0.6144\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0477 - val_loss: 0.6825\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0457 - val_loss: 0.6240\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0451 - val_loss: 0.6385\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0450 - val_loss: 0.6267\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0450 - val_loss: 0.5997\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0464 - val_loss: 0.6270\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0440 - val_loss: 0.6455\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0467 - val_loss: 0.6051\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0474 - val_loss: 0.6112\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0452 - val_loss: 0.6054\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0440 - val_loss: 0.6068\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0433 - val_loss: 0.6293\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0431 - val_loss: 0.5939\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0439 - val_loss: 0.6871\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0440 - val_loss: 0.5755\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0425 - val_loss: 0.6233\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0413 - val_loss: 0.5993\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0416 - val_loss: 0.5721\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0415 - val_loss: 0.5777\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0402 - val_loss: 0.5764\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0403 - val_loss: 0.5741\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0406 - val_loss: 0.6377\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0406 - val_loss: 0.5497\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0397 - val_loss: 0.6457\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0396 - val_loss: 0.5437\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0386 - val_loss: 0.5684\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0385 - val_loss: 0.5682\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0393 - val_loss: 0.5656\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0380 - val_loss: 0.5718\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0381 - val_loss: 0.5480\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0375 - val_loss: 0.5609\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0375 - val_loss: 0.5362\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0373 - val_loss: 0.5693\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0380 - val_loss: 0.5691\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0387 - val_loss: 0.5724\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0378 - val_loss: 0.5668\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0363 - val_loss: 0.5469\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0364 - val_loss: 0.5526\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0362 - val_loss: 0.5695\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0360 - val_loss: 0.5489\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0357 - val_loss: 0.5572\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0364 - val_loss: 0.5773\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0355 - val_loss: 0.5574\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0350 - val_loss: 0.5441\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0352 - val_loss: 0.5488\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0351 - val_loss: 0.5282\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0353 - val_loss: 0.5613\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0345 - val_loss: 0.5392\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0339 - val_loss: 0.5421\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0340 - val_loss: 0.5275\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.0339 - val_loss: 0.5290\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.0337 - val_loss: 0.5373\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0335 - val_loss: 0.5447\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0346 - val_loss: 0.5403\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 0.0352 - val_loss: 0.5206\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0332 - val_loss: 0.5323\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0342 - val_loss: 0.5301\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 5s 493us/step - loss: 0.0325 - val_loss: 0.5115\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0329 - val_loss: 0.5496\n",
      "Epoch 155/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 5s 509us/step - loss: 0.0326 - val_loss: 0.5384\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 5s 530us/step - loss: 0.0330 - val_loss: 0.5520\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0336 - val_loss: 0.5173\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 5s 523us/step - loss: 0.0334 - val_loss: 0.5173\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0322 - val_loss: 0.5260\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0320 - val_loss: 0.5238\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0321 - val_loss: 0.5078\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0321 - val_loss: 0.5589\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0319 - val_loss: 0.5220\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0325 - val_loss: 0.5606\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0328 - val_loss: 0.5297\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 566us/step - loss: 0.0304 - val_loss: 0.5216\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 5s 538us/step - loss: 0.0317 - val_loss: 0.5157\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0314 - val_loss: 0.5157\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0311 - val_loss: 0.5167\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0308 - val_loss: 0.5002\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0300 - val_loss: 0.5208\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0310 - val_loss: 0.4975\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0305 - val_loss: 0.4883\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0303 - val_loss: 0.5096\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 5s 513us/step - loss: 0.0297 - val_loss: 0.4828\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0298 - val_loss: 0.4953\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0297 - val_loss: 0.4967\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0293 - val_loss: 0.5181\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0295 - val_loss: 0.5005\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0298 - val_loss: 0.5079\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0290 - val_loss: 0.6056\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0293 - val_loss: 0.4865\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 5s 535us/step - loss: 0.0290 - val_loss: 0.5081\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0287 - val_loss: 0.4913\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0290 - val_loss: 0.5040\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0291 - val_loss: 0.5007\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.0285 - val_loss: 0.4864\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0286 - val_loss: 0.4960\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0290 - val_loss: 0.4865\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0285 - val_loss: 0.5164\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0281 - val_loss: 0.4719\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0284 - val_loss: 0.4926\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 6s 616us/step - loss: 0.0278 - val_loss: 0.4852\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0278 - val_loss: 0.4876\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0279 - val_loss: 0.4947\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.0275 - val_loss: 0.4809\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 5s 478us/step - loss: 0.0274 - val_loss: 0.5100\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0273 - val_loss: 0.4931\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0277 - val_loss: 0.5000\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 6s 636us/step - loss: 0.0273 - val_loss: 0.4846\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0272 - val_loss: 0.4767\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0275 - val_loss: 0.4913\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0271 - val_loss: 0.4694\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0266 - val_loss: 0.4813\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0264 - val_loss: 0.4871\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 6s 603us/step - loss: 0.0264 - val_loss: 0.4916\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0266 - val_loss: 0.4758\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0265 - val_loss: 0.5456\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 5s 495us/step - loss: 0.0264 - val_loss: 0.4869\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 5s 544us/step - loss: 0.0266 - val_loss: 0.4643\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0262 - val_loss: 0.4852\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0259 - val_loss: 0.4695\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0263 - val_loss: 0.4769\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.0258 - val_loss: 0.4810\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0260 - val_loss: 0.4938\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0260 - val_loss: 0.4871\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.0256 - val_loss: 0.4815\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0257 - val_loss: 0.4920\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0262 - val_loss: 0.4931\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 5s 479us/step - loss: 0.0261 - val_loss: 0.4642\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 5s 547us/step - loss: 0.0265 - val_loss: 0.4812\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0255 - val_loss: 0.4896\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0252 - val_loss: 0.4832\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0253 - val_loss: 0.4887\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.0251 - val_loss: 0.4791\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0250 - val_loss: 0.4699\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 6s 625us/step - loss: 0.0250 - val_loss: 0.4743\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0254 - val_loss: 0.4727\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0249 - val_loss: 0.4696\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0251 - val_loss: 0.4649\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0253 - val_loss: 0.4572\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0255 - val_loss: 0.4665\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0249 - val_loss: 0.4703\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0245 - val_loss: 0.4646\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0245 - val_loss: 0.4686\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0246 - val_loss: 0.4643\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.0244 - val_loss: 0.4799\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0240 - val_loss: 0.4682\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 5s 499us/step - loss: 0.0237 - val_loss: 0.4637\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0241 - val_loss: 0.4733\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0235 - val_loss: 0.4721\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0239 - val_loss: 0.4719\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0243 - val_loss: 0.4746\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0241 - val_loss: 0.4573\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0241 - val_loss: 0.4723\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0236 - val_loss: 0.4768\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0236 - val_loss: 0.4541\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0237 - val_loss: 0.4553\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 635us/step - loss: 0.0237 - val_loss: 0.4593\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0237 - val_loss: 0.4644\n"
     ]
    }
   ],
   "source": [
    "modelhe = Sequential()\n",
    "modelhe.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu')) #uniform\n",
    "modelhe.add(Dense(256,  kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.00008)\n",
    "modelhe.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historye = modelhe.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resulte= pd.DataFrame(historyd.history)\n",
    "resulte.to_csv(\"history2he(he-uniform).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>f) ¿Qué es lo que sucede con la red más profunda? ¿El modelo logra convergencia en su entrenamiento? Modifique aspectos estructurales (funciones de activación, inicializadores, regularización, momentum, variación de tasa de aprendizaje, entre otros) de la red profunda de 6 capas definida anteriormente (no modifique la profundidad ni el número de neuronas) para lograr un error cuadrático medio (mse) similar o menor al de una red no profunda, como la definida en b) en esta sección, sobre el conjunto de pruebas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 10.1803 - val_loss: 2.0508\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 2.5345 - val_loss: 1.5179\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 1.4274 - val_loss: 5.7192\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 1.0272 - val_loss: 0.7788\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.8274 - val_loss: 0.9646\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.6861 - val_loss: 0.8756\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.5210 - val_loss: 1.1036\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.4704 - val_loss: 2.7827\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.4209 - val_loss: 1.7655\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.3845 - val_loss: 0.3054\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.3298 - val_loss: 0.7431\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.3261 - val_loss: 0.9655\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.2710 - val_loss: 0.3975\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.2776 - val_loss: 0.7837\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2016 - val_loss: 0.4784\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.2439 - val_loss: 0.5800\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.2116 - val_loss: 0.2056\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.2128 - val_loss: 0.9804\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 15s 1ms/step - loss: 0.1795 - val_loss: 0.6510\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1773 - val_loss: 0.4807\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1635 - val_loss: 0.4915\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.1540 - val_loss: 0.5670\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.1307 - val_loss: 0.8168\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1341 - val_loss: 1.1925\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1333 - val_loss: 0.7628\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.1309 - val_loss: 0.6497\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1254 - val_loss: 0.7087\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.1121 - val_loss: 0.6492\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1206 - val_loss: 0.5186\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 15s 1ms/step - loss: 0.1110 - val_loss: 0.5373\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1057 - val_loss: 0.1475\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1037 - val_loss: 0.8316\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0994 - val_loss: 0.7372\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0880 - val_loss: 0.2902\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0974 - val_loss: 0.5261\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0941 - val_loss: 0.4900\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0836 - val_loss: 0.6475\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0858 - val_loss: 0.7319\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0888 - val_loss: 0.1894\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0819 - val_loss: 0.7557\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0664 - val_loss: 0.4618\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0862 - val_loss: 0.7268\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0690 - val_loss: 0.8346\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0750 - val_loss: 0.4152\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0779 - val_loss: 0.4452\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0676 - val_loss: 0.4688\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0621 - val_loss: 0.6631\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0732 - val_loss: 0.3829\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0698 - val_loss: 0.3996\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0703 - val_loss: 0.5463\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0683 - val_loss: 0.8058\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0635 - val_loss: 0.4675\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0713 - val_loss: 0.8036\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0668 - val_loss: 0.5758\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0591 - val_loss: 0.5412\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0607 - val_loss: 0.5384\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0609 - val_loss: 0.2590\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0714 - val_loss: 0.2616\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0532 - val_loss: 0.5044\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0515 - val_loss: 0.5384\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0542 - val_loss: 0.8664\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0543 - val_loss: 0.5379\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0625 - val_loss: 0.5150\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0546 - val_loss: 0.7476\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0521 - val_loss: 0.3991\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0562 - val_loss: 0.8850\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0532 - val_loss: 0.5996\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0537 - val_loss: 0.5653\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0504 - val_loss: 0.8867\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0524 - val_loss: 0.3466\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0478 - val_loss: 0.3240\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0476 - val_loss: 0.1086\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0559 - val_loss: 0.4893\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0511 - val_loss: 0.3574\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0439 - val_loss: 0.7614\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0497 - val_loss: 0.5327\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0501 - val_loss: 0.5979\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0491 - val_loss: 0.3957\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0448 - val_loss: 0.4514\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0483 - val_loss: 0.2472\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0440 - val_loss: 0.5099\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0477 - val_loss: 0.8173\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0409 - val_loss: 0.6475\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0410 - val_loss: 0.5962\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0470 - val_loss: 0.5045\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0414 - val_loss: 0.4835\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0516 - val_loss: 0.9354\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0430 - val_loss: 0.5069\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0433 - val_loss: 0.4786\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0488 - val_loss: 0.6950\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0439 - val_loss: 0.3816\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0399 - val_loss: 0.5785\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0395 - val_loss: 0.5672\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0442 - val_loss: 0.3988\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0367 - val_loss: 0.4771\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0366 - val_loss: 0.5513\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0375 - val_loss: 0.4593\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0463 - val_loss: 0.8423\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0341 - val_loss: 0.6499\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0386 - val_loss: 0.5801\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0425 - val_loss: 0.5465\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0345 - val_loss: 0.4040\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0386 - val_loss: 0.5942\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0375 - val_loss: 0.4459\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0483 - val_loss: 0.5689\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0346 - val_loss: 0.4514\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0350 - val_loss: 0.6457\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0353 - val_loss: 0.6742\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0357 - val_loss: 0.4250\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0344 - val_loss: 0.7291\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0353 - val_loss: 1.0639\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0383 - val_loss: 0.7327\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0324 - val_loss: 0.7149\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0345 - val_loss: 0.7784\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0413 - val_loss: 0.7392\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0324 - val_loss: 0.6451\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0336 - val_loss: 0.6645\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0366 - val_loss: 0.4140\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0354 - val_loss: 0.6697\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0332 - val_loss: 0.5518\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0353 - val_loss: 0.5353\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0319 - val_loss: 0.3206\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0393 - val_loss: 0.4406\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0316 - val_loss: 0.6906\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0309 - val_loss: 0.5223\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0309 - val_loss: 0.7054\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0330 - val_loss: 0.5982\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0313 - val_loss: 0.6726\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0334 - val_loss: 0.6391\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0307 - val_loss: 0.6448\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0322 - val_loss: 0.5527\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0304 - val_loss: 0.5374\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0327 - val_loss: 0.5756\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0354 - val_loss: 0.4827\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0282 - val_loss: 0.4870\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0313 - val_loss: 0.6080\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0281 - val_loss: 0.6509\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0312 - val_loss: 0.4670\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0319 - val_loss: 0.4182\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0311 - val_loss: 0.5536\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0317 - val_loss: 0.6266\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0286 - val_loss: 0.5183\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0274 - val_loss: 0.5954\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0277 - val_loss: 0.5095\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0300 - val_loss: 0.6362\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0267 - val_loss: 0.4002\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0279 - val_loss: 0.5055\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.0284 - val_loss: 0.5721\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0289 - val_loss: 0.4187\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0275 - val_loss: 0.3373\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0288 - val_loss: 0.6692\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0317 - val_loss: 0.5724\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 7s 683us/step - loss: 0.0322 - val_loss: 0.5939\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.0257 - val_loss: 0.3264\n",
      "Epoch 155/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0317 - val_loss: 0.3936\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 0.0253 - val_loss: 0.6136\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0295 - val_loss: 0.5685\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 7s 722us/step - loss: 0.0270 - val_loss: 0.4987\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 7s 669us/step - loss: 0.0291 - val_loss: 0.6013\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.0288 - val_loss: 0.4700\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.0283 - val_loss: 0.4105\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0267 - val_loss: 0.7365\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 601us/step - loss: 0.0274 - val_loss: 0.6919\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0263 - val_loss: 0.7663\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0251 - val_loss: 0.4014\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 574us/step - loss: 0.0259 - val_loss: 0.5188\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.0254 - val_loss: 0.4836\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.0253 - val_loss: 0.6273\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0264 - val_loss: 0.5341\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0248 - val_loss: 0.3570\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0271 - val_loss: 0.9269\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 6s 570us/step - loss: 0.0246 - val_loss: 0.5703\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0257 - val_loss: 0.5088\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 6s 625us/step - loss: 0.0253 - val_loss: 0.4276\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0248 - val_loss: 0.4393\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0305 - val_loss: 0.7010\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0248 - val_loss: 0.4718\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.0256 - val_loss: 0.5917\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0249 - val_loss: 0.5153\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.0261 - val_loss: 0.5824\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 6s 644us/step - loss: 0.0252 - val_loss: 0.4423\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.0233 - val_loss: 0.6933\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 6s 666us/step - loss: 0.0255 - val_loss: 0.4356\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0243 - val_loss: 0.3799\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0310 - val_loss: 0.2811\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.0247 - val_loss: 0.4056\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0256 - val_loss: 0.6129\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0269 - val_loss: 0.6142\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 6s 623us/step - loss: 0.0249 - val_loss: 0.6858\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.0228 - val_loss: 0.6141\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 7s 673us/step - loss: 0.0251 - val_loss: 0.5651\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0262 - val_loss: 0.5114\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0233 - val_loss: 0.4038\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.0256 - val_loss: 0.5376\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0251 - val_loss: 0.5300\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.0238 - val_loss: 0.5711\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0239 - val_loss: 0.3656\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 6s 641us/step - loss: 0.0245 - val_loss: 0.4905\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0242 - val_loss: 0.5241\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0226 - val_loss: 0.1698\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 7s 695us/step - loss: 0.0232 - val_loss: 0.6873\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 6s 640us/step - loss: 0.0271 - val_loss: 0.4610\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.0246 - val_loss: 0.6259\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0250 - val_loss: 0.3159\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0226 - val_loss: 0.4755\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0228 - val_loss: 0.7126\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0226 - val_loss: 0.4511\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0295 - val_loss: 0.5844\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0218 - val_loss: 0.5403\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 7s 685us/step - loss: 0.0211 - val_loss: 0.4205\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0237 - val_loss: 0.5048\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 6s 611us/step - loss: 0.0211 - val_loss: 0.4836\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0252 - val_loss: 0.5539\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0213 - val_loss: 0.5858\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0226 - val_loss: 0.5799\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 6s 663us/step - loss: 0.0233 - val_loss: 0.4317\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0214 - val_loss: 0.6179\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0236 - val_loss: 0.4790 - loss: 0.02\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0231 - val_loss: 0.4393\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.0226 - val_loss: 0.4301\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0226 - val_loss: 0.5710\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0224 - val_loss: 0.5829\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0226 - val_loss: 0.5241\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 562us/step - loss: 0.0221 - val_loss: 0.4389\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0218 - val_loss: 0.2945\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0217 - val_loss: 0.5286\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 9s 901us/step - loss: 0.0224 - val_loss: 0.4560\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0215 - val_loss: 0.4920\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0216 - val_loss: 0.4717\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 6s 618us/step - loss: 0.0211 - val_loss: 0.6225\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0217 - val_loss: 0.5869\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0213 - val_loss: 0.6016\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0220 - val_loss: 0.3898\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 6s 585us/step - loss: 0.0226 - val_loss: 0.4159\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 546us/step - loss: 0.0211 - val_loss: 0.6024\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 6s 574us/step - loss: 0.0202 - val_loss: 0.5354\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 5s 536us/step - loss: 0.0223 - val_loss: 0.5792\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0205 - val_loss: 0.5291\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0225 - val_loss: 0.4929\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 563us/step - loss: 0.0212 - val_loss: 0.4722\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0211 - val_loss: 0.4082\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0203 - val_loss: 0.6159\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0199 - val_loss: 0.6871\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0216 - val_loss: 0.5185\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0207 - val_loss: 0.4774\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0213 - val_loss: 0.5681\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.0199 - val_loss: 0.5442\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0212 - val_loss: 0.5004\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0212 - val_loss: 0.5763\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0194 - val_loss: 0.5174\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'historyd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-a055dd2332e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhistoryf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresultf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistoryd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mresultf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"history2f.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnumEpochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'historyd' is not defined"
     ]
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256,  kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyf = modelf.fit(X_train_scaled,\n",
    "                      y_train, epochs=250,\n",
    "                      verbose=1, \n",
    "                      validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "resultf= pd.DataFrame(historyf.history)\n",
    "resultf.to_csv(\"history2f.csv\")\n",
    "numEpochs = 250\n",
    "test_loss[-1] = modelf.evaluate(X_test_scaled, yTest, verbose=0)\n",
    "train_loss=historyf.history['loss']\n",
    "xc = range(numEpochs)\n",
    "plt.figure(1, figsize=(10, 6))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,test_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.title('Training Loss vs Testing Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **g) Experimente con la utilización de una función activación auxiliar (debido a que aproxima) a '**ReLU**'**y que es continua derivable (**softplus**)**¿Cuál es el beneficio de ésta con respecto ReLU? Comente.**\n",
    "\n",
    "Las dos funciones de activación son muy similares, excepto que Softplus es diferenciable es cero. Por otra parte, RElu hace más fáciles los cálculos y su derivada, por lo que los algoritmos forward pass y backward pass son más rápidos. Con la función de activación Relu se favorecen las representaciones distribuidas, por su arquitectura no es derivable en cero, la derivada no es continua. La dura saturación de Relu en el umbral podría bloquear el gradiente en la capa de salida, al tener ceros reales, ya que no pueden recuperarse de esto. Sin embargo, no sucede tan frecuentemente que la suma ponderada sea cero, para esto se puede normalizar la data entre 0 y 1. En casos donde no se tengan estas garantías es mejor usar la versión diferenciable Softplus, que además hace el entrenamiento es más sencillo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 16.0616 - val_loss: 8.6825\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 5.7890 - val_loss: 7.1030\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 4.6571 - val_loss: 6.0007\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 8s 781us/step - loss: 3.9643 - val_loss: 5.2332\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 3.4598 - val_loss: 4.6992\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 8s 789us/step - loss: 3.0641 - val_loss: 4.2025\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 646us/step - loss: 2.7297 - val_loss: 3.8254\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 7s 724us/step - loss: 2.4487 - val_loss: 3.4220\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 7s 761us/step - loss: 2.2064 - val_loss: 3.1768\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 1.9945 - val_loss: 2.9908\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 8s 810us/step - loss: 1.8121 - val_loss: 2.6893\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 7s 744us/step - loss: 1.6537 - val_loss: 2.6236\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 1.5124 - val_loss: 2.3904\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 6s 636us/step - loss: 1.3898 - val_loss: 2.3243\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 1.2834 - val_loss: 2.1637\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 1.1902 - val_loss: 2.1314\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 1.1084 - val_loss: 1.9413\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 7s 730us/step - loss: 1.0362 - val_loss: 1.9118\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 7s 728us/step - loss: 0.9736 - val_loss: 1.8896\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 7s 732us/step - loss: 0.9192 - val_loss: 1.7262\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 7s 705us/step - loss: 0.8703 - val_loss: 1.6959\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.8282 - val_loss: 1.7328\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.7903 - val_loss: 1.5817\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.7572 - val_loss: 1.6300\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.7270 - val_loss: 1.6028\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.6999 - val_loss: 1.5681\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.6760 - val_loss: 1.5445\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 8s 862us/step - loss: 0.6547 - val_loss: 1.5001\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.6350 - val_loss: 1.4926\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 9s 916us/step - loss: 0.6165 - val_loss: 1.4803\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 9s 887us/step - loss: 0.6005 - val_loss: 1.4622\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 8s 843us/step - loss: 0.5850 - val_loss: 1.4372\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.5715 - val_loss: 1.4672\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 8s 821us/step - loss: 0.5584 - val_loss: 1.4759\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.5462 - val_loss: 1.4127\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 9s 953us/step - loss: 0.5346 - val_loss: 1.3903\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 9s 972us/step - loss: 0.5243 - val_loss: 1.4541\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.5146 - val_loss: 1.4152\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.5055 - val_loss: 1.4024\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.4962 - val_loss: 1.4086\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.4880 - val_loss: 1.4022\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.4800 - val_loss: 1.3589\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.4727 - val_loss: 1.4149\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 7s 734us/step - loss: 0.4655 - val_loss: 1.3710\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.4588 - val_loss: 1.3170\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 9s 942us/step - loss: 0.4525 - val_loss: 1.3557\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.4462 - val_loss: 1.3647\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 9s 879us/step - loss: 0.4400 - val_loss: 1.3095\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 8s 865us/step - loss: 0.4346 - val_loss: 1.3446\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 8s 827us/step - loss: 0.4289 - val_loss: 1.3450\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 8s 845us/step - loss: 0.4238 - val_loss: 1.3204\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 9s 897us/step - loss: 0.4185 - val_loss: 1.3268\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 9s 887us/step - loss: 0.4138 - val_loss: 1.3224\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 9s 956us/step - loss: 0.4088 - val_loss: 1.3359\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.4044 - val_loss: 1.3198\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 9s 901us/step - loss: 0.3997 - val_loss: 1.3507\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 8s 851us/step - loss: 0.3957 - val_loss: 1.3585\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 8s 858us/step - loss: 0.3913 - val_loss: 1.3169\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.3874 - val_loss: 1.3502\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 8s 822us/step - loss: 0.3834 - val_loss: 1.3031\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.3794 - val_loss: 1.3546\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 9s 879us/step - loss: 0.3757 - val_loss: 1.3012\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 8s 851us/step - loss: 0.3718 - val_loss: 1.3641\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 8s 795us/step - loss: 0.3689 - val_loss: 1.3153\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 8s 844us/step - loss: 0.3654 - val_loss: 1.3198\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 8s 871us/step - loss: 0.3623 - val_loss: 1.2944\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.3588 - val_loss: 1.3779\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.3558 - val_loss: 1.2701\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.3526 - val_loss: 1.3379\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 8s 771us/step - loss: 0.3494 - val_loss: 1.2512\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 8s 841us/step - loss: 0.3466 - val_loss: 1.2616\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.3437 - val_loss: 1.2538\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.3411 - val_loss: 1.2935\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 9s 912us/step - loss: 0.3379 - val_loss: 1.2960\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.3355 - val_loss: 1.2926\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 9s 898us/step - loss: 0.3330 - val_loss: 1.3189\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 9s 937us/step - loss: 0.3302 - val_loss: 1.3223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.3276 - val_loss: 1.2370\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.3254 - val_loss: 1.2381\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 7s 758us/step - loss: 0.3231 - val_loss: 1.2668\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.3206 - val_loss: 1.2770\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.3184 - val_loss: 1.2878\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 7s 744us/step - loss: 0.3161 - val_loss: 1.2150\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.3141 - val_loss: 1.2763\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.3114 - val_loss: 1.2756\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 8s 778us/step - loss: 0.3096 - val_loss: 1.3462\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 8s 795us/step - loss: 0.3075 - val_loss: 1.2978\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 7s 732us/step - loss: 0.3055 - val_loss: 1.2457\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.3035 - val_loss: 1.2445\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.3014 - val_loss: 1.2712\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.2993 - val_loss: 1.2321\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 7s 728us/step - loss: 0.2975 - val_loss: 1.2364\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.2955 - val_loss: 1.2843\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.2941 - val_loss: 1.2602\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 7s 707us/step - loss: 0.2919 - val_loss: 1.2532\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 8s 803us/step - loss: 0.2901 - val_loss: 1.2573\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 7s 736us/step - loss: 0.2886 - val_loss: 1.2428\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.2868 - val_loss: 1.3282\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.2851 - val_loss: 1.2450\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 7s 735us/step - loss: 0.2833 - val_loss: 1.2016\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 7s 695us/step - loss: 0.2818 - val_loss: 1.2499\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.2801 - val_loss: 1.2360\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.2786 - val_loss: 1.2492\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.2770 - val_loss: 1.2174\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 9s 942us/step - loss: 0.2753 - val_loss: 1.2176\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 8s 770us/step - loss: 0.2740 - val_loss: 1.2126\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.2724 - val_loss: 1.2239\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 0.2709 - val_loss: 1.2253\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 8s 771us/step - loss: 0.2694 - val_loss: 1.2294\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.2682 - val_loss: 1.2175\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 8s 770us/step - loss: 0.2668 - val_loss: 1.1999\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.2652 - val_loss: 1.2615\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 7s 756us/step - loss: 0.2636 - val_loss: 1.2008\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.2624 - val_loss: 1.1913\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 0.2612 - val_loss: 1.1795\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.2598 - val_loss: 1.2103\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.2585 - val_loss: 1.2666\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 8s 830us/step - loss: 0.2573 - val_loss: 1.2634\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.2561 - val_loss: 1.1945\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.2543 - val_loss: 1.2529\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 9s 931us/step - loss: 0.2534 - val_loss: 1.2379\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.2523 - val_loss: 1.2744\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 8s 848us/step - loss: 0.2512 - val_loss: 1.2389\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 8s 782us/step - loss: 0.2499 - val_loss: 1.2059\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.2488 - val_loss: 1.2068\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.2475 - val_loss: 1.1776\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 7s 768us/step - loss: 0.2463 - val_loss: 1.2476\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 8s 807us/step - loss: 0.2452 - val_loss: 1.1881\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.2441 - val_loss: 1.2449\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 8s 820us/step - loss: 0.2429 - val_loss: 1.2202\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 8s 789us/step - loss: 0.2420 - val_loss: 1.1801\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.2409 - val_loss: 1.2306\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.2399 - val_loss: 1.2049\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 8s 856us/step - loss: 0.2387 - val_loss: 1.2201\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2376 - val_loss: 1.1748\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.2366 - val_loss: 1.1689\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 8s 801us/step - loss: 0.2357 - val_loss: 1.2248\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2345 - val_loss: 1.2073\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.2336 - val_loss: 1.2095\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2326 - val_loss: 1.2119\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 8s 775us/step - loss: 0.2317 - val_loss: 1.1777\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.2306 - val_loss: 1.2077\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.2296 - val_loss: 1.2232\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.2286 - val_loss: 1.2176\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2277 - val_loss: 1.2403\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2270 - val_loss: 1.1732\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 7s 759us/step - loss: 0.2259 - val_loss: 1.1881\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 6s 649us/step - loss: 0.2251 - val_loss: 1.2096\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 9s 949us/step - loss: 0.2240 - val_loss: 1.2282\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.2231 - val_loss: 1.1839\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 9s 914us/step - loss: 0.2222 - val_loss: 1.2166\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 9s 893us/step - loss: 0.2214 - val_loss: 1.2495\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 9s 874us/step - loss: 0.2206 - val_loss: 1.2417\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 10s 987us/step - loss: 0.2197 - val_loss: 1.1803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 9s 882us/step - loss: 0.2187 - val_loss: 1.1667\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.2179 - val_loss: 1.1592\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 8s 785us/step - loss: 0.2172 - val_loss: 1.1677\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 8s 862us/step - loss: 0.2162 - val_loss: 1.1438\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 8s 863us/step - loss: 0.2155 - val_loss: 1.1664\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.2148 - val_loss: 1.1375\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 8s 845us/step - loss: 0.2138 - val_loss: 1.1279\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 7s 713us/step - loss: 0.2130 - val_loss: 1.1662\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 9s 905us/step - loss: 0.2122 - val_loss: 1.2174\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.2113 - val_loss: 1.1960\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 9s 912us/step - loss: 0.2105 - val_loss: 1.1382\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.2100 - val_loss: 1.1482\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.2093 - val_loss: 1.2078\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.2083 - val_loss: 1.1783\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.2077 - val_loss: 1.1923\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.2067 - val_loss: 1.1674\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.2059 - val_loss: 1.1363\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 8s 803us/step - loss: 0.2055 - val_loss: 1.1509\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.2046 - val_loss: 1.1573\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 8s 859us/step - loss: 0.2040 - val_loss: 1.1446\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 9s 893us/step - loss: 0.2034 - val_loss: 1.2053\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.2027 - val_loss: 1.1623\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 8s 819us/step - loss: 0.2018 - val_loss: 1.1731\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.2011 - val_loss: 1.1902\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 8s 804us/step - loss: 0.2006 - val_loss: 1.1492\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 8s 863us/step - loss: 0.1998 - val_loss: 1.1422\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 9s 895us/step - loss: 0.1991 - val_loss: 1.1674\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 9s 959us/step - loss: 0.1982 - val_loss: 1.2042\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 8s 822us/step - loss: 0.1977 - val_loss: 1.1823\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.1969 - val_loss: 1.1292\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.1965 - val_loss: 1.1643\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 9s 957us/step - loss: 0.1956 - val_loss: 1.1521\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 10s 990us/step - loss: 0.1951 - val_loss: 1.1094\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.1944 - val_loss: 1.1797\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.1938 - val_loss: 1.1898\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 9s 889us/step - loss: 0.1931 - val_loss: 1.1982\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 9s 940us/step - loss: 0.1925 - val_loss: 1.1858\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 9s 908us/step - loss: 0.1918 - val_loss: 1.2076\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 9s 925us/step - loss: 0.1913 - val_loss: 1.1350\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 9s 965us/step - loss: 0.1908 - val_loss: 1.1771\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1897 - val_loss: 1.1932\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 9s 889us/step - loss: 0.1895 - val_loss: 1.1770\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.1888 - val_loss: 1.1467\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.1884 - val_loss: 1.1341\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.1876 - val_loss: 1.1634\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 9s 919us/step - loss: 0.1869 - val_loss: 1.1508\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 9s 925us/step - loss: 0.1864 - val_loss: 1.1317\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 9s 924us/step - loss: 0.1859 - val_loss: 1.1409\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 9s 927us/step - loss: 0.1852 - val_loss: 1.1718\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.1846 - val_loss: 1.1449\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.1842 - val_loss: 1.1418\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.1834 - val_loss: 1.1048\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 9s 921us/step - loss: 0.1831 - val_loss: 1.1465\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 9s 962us/step - loss: 0.1822 - val_loss: 1.1583\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.1820 - val_loss: 1.1269\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1813 - val_loss: 1.1563\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 862us/step - loss: 0.1808 - val_loss: 1.1348\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 9s 950us/step - loss: 0.1800 - val_loss: 1.1543\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 8s 865us/step - loss: 0.1797 - val_loss: 1.1399\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.1790 - val_loss: 1.1127\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.1786 - val_loss: 1.1433\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 9s 895us/step - loss: 0.1780 - val_loss: 1.1229\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.1777 - val_loss: 1.1385\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1769 - val_loss: 1.0992\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 9s 882us/step - loss: 0.1764 - val_loss: 1.1950\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.1759 - val_loss: 1.1574\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 9s 874us/step - loss: 0.1756 - val_loss: 1.1476\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.1750 - val_loss: 1.1417\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 9s 915us/step - loss: 0.1743 - val_loss: 1.1361\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1737 - val_loss: 1.1777\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 9s 917us/step - loss: 0.1734 - val_loss: 1.1293\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 8s 771us/step - loss: 0.1727 - val_loss: 1.1489\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 8s 770us/step - loss: 0.1723 - val_loss: 1.0660\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 9s 889us/step - loss: 0.1720 - val_loss: 1.1558\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 8s 869us/step - loss: 0.1714 - val_loss: 1.1271\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.1709 - val_loss: 1.1546\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.1705 - val_loss: 1.1171\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 8s 845us/step - loss: 0.1700 - val_loss: 1.1248\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.1695 - val_loss: 1.1168\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.1690 - val_loss: 1.1544\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 8s 866us/step - loss: 0.1685 - val_loss: 1.1372\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 7s 698us/step - loss: 0.1680 - val_loss: 1.1262\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.1676 - val_loss: 1.1012\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.1672 - val_loss: 1.1393\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.1666 - val_loss: 1.1494\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.1662 - val_loss: 1.1278\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 6s 663us/step - loss: 0.1657 - val_loss: 1.1453\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 8s 796us/step - loss: 0.1654 - val_loss: 1.1279\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 7s 760us/step - loss: 0.1648 - val_loss: 1.1490\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.1644 - val_loss: 1.1218\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 7s 707us/step - loss: 0.1641 - val_loss: 1.1418\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.1635 - val_loss: 1.1603\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.1631 - val_loss: 1.0971\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 9s 914us/step - loss: 0.1626 - val_loss: 1.1078\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 9s 924us/step - loss: 0.1622 - val_loss: 1.1274\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 8s 832us/step - loss: 0.1618 - val_loss: 1.1346\n"
     ]
    }
   ],
   "source": [
    "modelg = Sequential()\n",
    "modelg.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(256, kernel_initializer='he_uniform',activation='softplus')) #uniform\n",
    "modelg.add(Dense(256,  kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.00001)\n",
    "modelg.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyg = modelg.fit(X_train_scaled, y_train, epochs=250, \n",
    "                      verbose=1, \n",
    "                      validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Pruebe con utilizar una red shallow (poco profunda), es decir, sitúe todas las neuronas en una única capa ¿Qué sucede con la convergencia del algoritmo? ¿Por qué sucede este fenómeno?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 250\n",
    "test_loss = np.zeros(numEpochs)\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss = self.model.evaluate(x, y, verbose=0)\n",
    "        test_loss[epoch-1] = loss\n",
    "        print('\\nTesting loss: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.1408 - val_loss: 0.0911\n",
      "\n",
      "Testing loss: 116.24844291477258\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 8s 798us/step - loss: 0.0811 - val_loss: 0.0687\n",
      "\n",
      "Testing loss: 116.33328773019545\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 8s 824us/step - loss: 0.0670 - val_loss: 0.0613\n",
      "\n",
      "Testing loss: 116.67242225643842\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 8s 782us/step - loss: 0.0599 - val_loss: 0.0567\n",
      "\n",
      "Testing loss: 117.05968622795422\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 8s 775us/step - loss: 0.0556 - val_loss: 0.0540\n",
      "\n",
      "Testing loss: 115.233867709442\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 7s 760us/step - loss: 0.0524 - val_loss: 0.0514\n",
      "\n",
      "Testing loss: 115.40361512582909\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 9s 923us/step - loss: 0.0499 - val_loss: 0.0477\n",
      "\n",
      "Testing loss: 115.91294894024736\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 9s 917us/step - loss: 0.0482 - val_loss: 0.0483\n",
      "\n",
      "Testing loss: 115.38868648887512\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.0465 - val_loss: 0.0444\n",
      "\n",
      "Testing loss: 116.02029112141594\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.0451 - val_loss: 0.0430\n",
      "\n",
      "Testing loss: 116.45981380320202\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.0438 - val_loss: 0.0432\n",
      "\n",
      "Testing loss: 115.97395814966852\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.0426 - val_loss: 0.0424\n",
      "\n",
      "Testing loss: 116.29403270146962\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0415 - val_loss: 0.0434\n",
      "\n",
      "Testing loss: 116.95172450989546\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 7s 748us/step - loss: 0.0406 - val_loss: 0.0435\n",
      "\n",
      "Testing loss: 117.56533320086092\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 9s 875us/step - loss: 0.0397 - val_loss: 0.0416\n",
      "\n",
      "Testing loss: 117.05388733085472\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 8s 797us/step - loss: 0.0389 - val_loss: 0.0400\n",
      "\n",
      "Testing loss: 115.54462192740458\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0383 - val_loss: 0.0376\n",
      "\n",
      "Testing loss: 116.49097893127517\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 8s 811us/step - loss: 0.0375 - val_loss: 0.0366\n",
      "\n",
      "Testing loss: 116.47231897879293\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.0368 - val_loss: 0.0371\n",
      "\n",
      "Testing loss: 116.55027538701849\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0362 - val_loss: 0.0371\n",
      "\n",
      "Testing loss: 116.4049040583445\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 9s 952us/step - loss: 0.0356 - val_loss: 0.0363\n",
      "\n",
      "Testing loss: 116.50069532993146\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0349 - val_loss: 0.0392\n",
      "\n",
      "Testing loss: 115.15759620463011\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 8s 820us/step - loss: 0.0345 - val_loss: 0.0383\n",
      "\n",
      "Testing loss: 117.14508320867502\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 8s 860us/step - loss: 0.0339 - val_loss: 0.0351\n",
      "\n",
      "Testing loss: 115.94681332118206\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0334 - val_loss: 0.0343\n",
      "\n",
      "Testing loss: 116.22289859363178\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 10s 999us/step - loss: 0.0328 - val_loss: 0.0338\n",
      "\n",
      "Testing loss: 116.33217050758208\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 9s 940us/step - loss: 0.0327 - val_loss: 0.0331\n",
      "\n",
      "Testing loss: 116.27008188754063\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 10s 991us/step - loss: 0.0321 - val_loss: 0.0345\n",
      "\n",
      "Testing loss: 116.3285497791414\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0317 - val_loss: 0.0344\n",
      "\n",
      "Testing loss: 115.50499567870092\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 9s 956us/step - loss: 0.0312 - val_loss: 0.0323\n",
      "\n",
      "Testing loss: 116.0846105926977\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 9s 884us/step - loss: 0.0308 - val_loss: 0.0326\n",
      "\n",
      "Testing loss: 116.88738540317566\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 8s 849us/step - loss: 0.0304 - val_loss: 0.0326\n",
      "\n",
      "Testing loss: 115.89157410669777\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.0302 - val_loss: 0.0310\n",
      "\n",
      "Testing loss: 116.03897104744644\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 9s 901us/step - loss: 0.0298 - val_loss: 0.0304\n",
      "\n",
      "Testing loss: 116.407331301533\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 9s 928us/step - loss: 0.0294 - val_loss: 0.0304\n",
      "\n",
      "Testing loss: 116.4176579766162\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0289 - val_loss: 0.0322\n",
      "\n",
      "Testing loss: 115.76916440667117\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 9s 932us/step - loss: 0.0287 - val_loss: 0.0320\n",
      "\n",
      "Testing loss: 115.89365957680421\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 7s 757us/step - loss: 0.0284 - val_loss: 0.0313\n",
      "\n",
      "Testing loss: 115.6206387788681\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 7s 761us/step - loss: 0.0281 - val_loss: 0.0316\n",
      "\n",
      "Testing loss: 115.92156455220382\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0279 - val_loss: 0.0305\n",
      "\n",
      "Testing loss: 116.23291200959491\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 8s 773us/step - loss: 0.0275 - val_loss: 0.0309\n",
      "\n",
      "Testing loss: 116.95711741801604\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 7s 740us/step - loss: 0.0272 - val_loss: 0.0288\n",
      "\n",
      "Testing loss: 115.97338368161859\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 7s 750us/step - loss: 0.0269 - val_loss: 0.0299\n",
      "\n",
      "Testing loss: 116.03194083431778\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.0266 - val_loss: 0.0291\n",
      "\n",
      "Testing loss: 116.81639888044944\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 7s 745us/step - loss: 0.0263 - val_loss: 0.0292\n",
      "\n",
      "Testing loss: 116.36597506179152\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 7s 740us/step - loss: 0.0262 - val_loss: 0.0292\n",
      "\n",
      "Testing loss: 115.93176638321843\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 7s 724us/step - loss: 0.0258 - val_loss: 0.0296\n",
      "\n",
      "Testing loss: 116.19080317406856\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 8s 775us/step - loss: 0.0256 - val_loss: 0.0288\n",
      "\n",
      "Testing loss: 115.7449190736942\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.0253 - val_loss: 0.0279\n",
      "\n",
      "Testing loss: 116.90160244960104\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 7s 762us/step - loss: 0.0252 - val_loss: 0.0299\n",
      "\n",
      "Testing loss: 115.48779579885777\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 7s 750us/step - loss: 0.0248 - val_loss: 0.0281\n",
      "\n",
      "Testing loss: 116.67607491124983\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 7s 762us/step - loss: 0.0247 - val_loss: 0.0283\n",
      "\n",
      "Testing loss: 116.2386871200142\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 8s 789us/step - loss: 0.0244 - val_loss: 0.0274\n",
      "\n",
      "Testing loss: 115.9562101555888\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0242 - val_loss: 0.0265\n",
      "\n",
      "Testing loss: 116.93027666332196\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.0240 - val_loss: 0.0266\n",
      "\n",
      "Testing loss: 116.12097581214576\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 7s 769us/step - loss: 0.0238 - val_loss: 0.0266\n",
      "\n",
      "Testing loss: 116.58658136830351\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 7s 727us/step - loss: 0.0235 - val_loss: 0.0273\n",
      "\n",
      "Testing loss: 116.13455193934013\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 8s 797us/step - loss: 0.0233 - val_loss: 0.0299\n",
      "\n",
      "Testing loss: 115.19175641948588\n",
      "Epoch 59/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.0231 - val_loss: 0.0262\n",
      "\n",
      "Testing loss: 116.52591812703568\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.0229 - val_loss: 0.0260\n",
      "\n",
      "Testing loss: 116.0101478147761\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 7s 695us/step - loss: 0.0227 - val_loss: 0.0259\n",
      "\n",
      "Testing loss: 116.1656620681506\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 7s 698us/step - loss: 0.0225 - val_loss: 0.0254\n",
      "\n",
      "Testing loss: 116.38291816570546\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 7s 677us/step - loss: 0.0223 - val_loss: 0.0249\n",
      "\n",
      "Testing loss: 116.28486696539915\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 9s 886us/step - loss: 0.0221 - val_loss: 0.0257\n",
      "\n",
      "Testing loss: 115.9195922926698\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 10s 979us/step - loss: 0.0220 - val_loss: 0.0256\n",
      "\n",
      "Testing loss: 115.97455222811129\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0218 - val_loss: 0.0259\n",
      "\n",
      "Testing loss: 116.21677354539473\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 8s 866us/step - loss: 0.0216 - val_loss: 0.0247\n",
      "\n",
      "Testing loss: 116.47187873423955\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.0214 - val_loss: 0.0249\n",
      "\n",
      "Testing loss: 116.76270844870822\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 9s 875us/step - loss: 0.0212 - val_loss: 0.0247\n",
      "\n",
      "Testing loss: 116.34850913929459\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 8s 847us/step - loss: 0.0210 - val_loss: 0.0236\n",
      "\n",
      "Testing loss: 116.53516214814498\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0209 - val_loss: 0.0234\n",
      "\n",
      "Testing loss: 116.71123368455389\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 9s 880us/step - loss: 0.0207 - val_loss: 0.0263\n",
      "\n",
      "Testing loss: 115.42266317249371\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 9s 873us/step - loss: 0.0205 - val_loss: 0.0246\n",
      "\n",
      "Testing loss: 116.20970818758695\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 9s 907us/step - loss: 0.0204 - val_loss: 0.0249\n",
      "\n",
      "Testing loss: 116.18954778333936\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 8s 872us/step - loss: 0.0201 - val_loss: 0.0234\n",
      "\n",
      "Testing loss: 116.74902149023558\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 9s 885us/step - loss: 0.0200 - val_loss: 0.0249\n",
      "\n",
      "Testing loss: 116.75231120250828\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.0199 - val_loss: 0.0242\n",
      "\n",
      "Testing loss: 116.55515743563984\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 9s 886us/step - loss: 0.0197 - val_loss: 0.0254\n",
      "\n",
      "Testing loss: 116.15137714922894\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 9s 874us/step - loss: 0.0195 - val_loss: 0.0248\n",
      "\n",
      "Testing loss: 115.57047853090258\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0194 - val_loss: 0.0243\n",
      "\n",
      "Testing loss: 115.73303331602877\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 9s 905us/step - loss: 0.0192 - val_loss: 0.0277\n",
      "\n",
      "Testing loss: 115.1055798505142\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 8s 867us/step - loss: 0.0191 - val_loss: 0.0222\n",
      "\n",
      "Testing loss: 116.3413307775373\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.0189 - val_loss: 0.0240\n",
      "\n",
      "Testing loss: 116.2135264851003\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 9s 889us/step - loss: 0.0188 - val_loss: 0.0245\n",
      "\n",
      "Testing loss: 115.78530878780022\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0187 - val_loss: 0.0264\n",
      "\n",
      "Testing loss: 115.1324953839189\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.0185 - val_loss: 0.0246\n",
      "\n",
      "Testing loss: 116.01189079175096\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0184 - val_loss: 0.0228\n",
      "\n",
      "Testing loss: 116.6146050016069\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 9s 953us/step - loss: 0.0182 - val_loss: 0.0244\n",
      "\n",
      "Testing loss: 115.76079027428703\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 9s 912us/step - loss: 0.0181 - val_loss: 0.0230\n",
      "\n",
      "Testing loss: 116.60231324870125\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 9s 918us/step - loss: 0.0179 - val_loss: 0.0251\n",
      "\n",
      "Testing loss: 115.62648258827532\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 9s 931us/step - loss: 0.0178 - val_loss: 0.0234\n",
      "\n",
      "Testing loss: 116.24884631157313\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0177 - val_loss: 0.0227\n",
      "\n",
      "Testing loss: 116.09393171546006\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.0175 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 116.56494489379432\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.0173 - val_loss: 0.0228\n",
      "\n",
      "Testing loss: 116.2825326586924\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 9s 874us/step - loss: 0.0173 - val_loss: 0.0225\n",
      "\n",
      "Testing loss: 116.01365620296511\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 9s 884us/step - loss: 0.0171 - val_loss: 0.0219\n",
      "\n",
      "Testing loss: 116.14373387339471\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 9s 898us/step - loss: 0.0170 - val_loss: 0.0221\n",
      "\n",
      "Testing loss: 116.4259255604515\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 9s 937us/step - loss: 0.0169 - val_loss: 0.0220\n",
      "\n",
      "Testing loss: 116.26278126137052\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 9s 925us/step - loss: 0.0168 - val_loss: 0.0219\n",
      "\n",
      "Testing loss: 116.01062612803588\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 9s 921us/step - loss: 0.0167 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.17631560815961\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 8s 860us/step - loss: 0.0165 - val_loss: 0.0213\n",
      "\n",
      "Testing loss: 116.0518054484734\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.0164 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 116.03153805738597\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 9s 918us/step - loss: 0.0163 - val_loss: 0.0234\n",
      "\n",
      "Testing loss: 116.95223803044735\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 9s 937us/step - loss: 0.0162 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.51275217763019\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 9s 896us/step - loss: 0.0160 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.08977786859724\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 8s 869us/step - loss: 0.0159 - val_loss: 0.0233\n",
      "\n",
      "Testing loss: 115.63621322512871\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 9s 931us/step - loss: 0.0158 - val_loss: 0.0220\n",
      "\n",
      "Testing loss: 116.27487918946115\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 8s 851us/step - loss: 0.0157 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.03559300700906\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 9s 927us/step - loss: 0.0155 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 116.36418610182658\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 9s 915us/step - loss: 0.0155 - val_loss: 0.0229\n",
      "\n",
      "Testing loss: 115.79686418047292\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 9s 899us/step - loss: 0.0154 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 116.04628010758817\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 9s 925us/step - loss: 0.0153 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 116.15855715770823\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 10s 977us/step - loss: 0.0152 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.77944638233083\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 9s 942us/step - loss: 0.0150 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 116.26404653243173\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 9s 928us/step - loss: 0.0149 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.43391116036223\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 10s 983us/step - loss: 0.0148 - val_loss: 0.0199\n",
      "\n",
      "Testing loss: 116.31543876012427\n",
      "Epoch 117/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 8s 858us/step - loss: 0.0147 - val_loss: 0.0226\n",
      "\n",
      "Testing loss: 116.04826707491681\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0147 - val_loss: 0.0207\n",
      "\n",
      "Testing loss: 116.49979208725908\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.0145 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.17439780941643\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.0144 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.58986686045976\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.0143 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.5863784019899\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 8s 819us/step - loss: 0.0142 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 115.8490910520307\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0141 - val_loss: 0.0199\n",
      "\n",
      "Testing loss: 116.39392188772297\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0140 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 115.89136616265036\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.0140 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.136021241136\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 9s 926us/step - loss: 0.0139 - val_loss: 0.0205\n",
      "\n",
      "Testing loss: 116.19190200724267\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 9s 912us/step - loss: 0.0138 - val_loss: 0.0225\n",
      "\n",
      "Testing loss: 116.29701165914243\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0137 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 116.48113961703082\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 9s 935us/step - loss: 0.0136 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 116.12497096008877\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 8s 848us/step - loss: 0.0135 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 116.58885173461063\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0134 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.17305983839634\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 7s 761us/step - loss: 0.0133 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.36360603617409\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0132 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.35887435894303\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 7s 741us/step - loss: 0.0131 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.53287431907106\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 7s 715us/step - loss: 0.0130 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.4478743548857\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.0130 - val_loss: 0.0206\n",
      "\n",
      "Testing loss: 116.48462664792417\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 7s 737us/step - loss: 0.0129 - val_loss: 0.0215\n",
      "\n",
      "Testing loss: 115.95589965770222\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 7s 752us/step - loss: 0.0128 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.34318543106599\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.0127 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.43487175654622\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 7s 745us/step - loss: 0.0127 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.131591859488\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.0126 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.50766579773175\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.0125 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.04872846388162\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.0124 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.41873435050579\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 8s 824us/step - loss: 0.0123 - val_loss: 0.0203\n",
      "\n",
      "Testing loss: 115.83621191401241\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.0123 - val_loss: 0.0197\n",
      "\n",
      "Testing loss: 116.72672236352169\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0122 - val_loss: 0.0195\n",
      "\n",
      "Testing loss: 116.45255841492727\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0121 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.9098583185433\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 9s 885us/step - loss: 0.0120 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.7509876888308\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 9s 965us/step - loss: 0.0120 - val_loss: 0.0195\n",
      "\n",
      "Testing loss: 116.24090648176829\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.0118 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 115.90242051980414\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 0.0118 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.94071404445744\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 8s 855us/step - loss: 0.0118 - val_loss: 0.0186\n",
      "\n",
      "Testing loss: 116.40260557253731\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 8s 836us/step - loss: 0.0117 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.36024105054803\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 8s 812us/step - loss: 0.0116 - val_loss: 0.0184\n",
      "\n",
      "Testing loss: 116.21607474044156\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 8s 851us/step - loss: 0.0115 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.30408160648338\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 8s 843us/step - loss: 0.0115 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.11896179599943\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0114 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.3354587547105\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 8s 811us/step - loss: 0.0113 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.54542832654374\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 8s 858us/step - loss: 0.0112 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.25902959661597\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 8s 823us/step - loss: 0.0112 - val_loss: 0.0196\n",
      "\n",
      "Testing loss: 116.53192998974549\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.0111 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.68652216645602\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 9s 952us/step - loss: 0.0111 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 115.99689483329513\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 9s 901us/step - loss: 0.0110 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.03273040934278\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.0109 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.47745692676142\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 8s 798us/step - loss: 0.0109 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.9660470717551\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.0108 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.11697658183489\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 8s 836us/step - loss: 0.0108 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.35278546971574\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0107 - val_loss: 0.0189\n",
      "\n",
      "Testing loss: 116.42637405113749\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 8s 812us/step - loss: 0.0106 - val_loss: 0.0189\n",
      "\n",
      "Testing loss: 116.34408376487886\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0105 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.43948675444442\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.0105 - val_loss: 0.0195\n",
      "\n",
      "Testing loss: 116.2157030633888\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 8s 830us/step - loss: 0.0105 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.50261055528017\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 8s 813us/step - loss: 0.0104 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.52225079573614\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0103 - val_loss: 0.0228\n",
      "\n",
      "Testing loss: 115.6300554615971\n",
      "Epoch 175/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 8s 811us/step - loss: 0.0103 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.50480864739683\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.0102 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.30132164405319\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 8s 816us/step - loss: 0.0102 - val_loss: 0.0197\n",
      "\n",
      "Testing loss: 116.24499783459187\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 8s 773us/step - loss: 0.0101 - val_loss: 0.0183\n",
      "\n",
      "Testing loss: 116.65436378325602\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.0101 - val_loss: 0.0186\n",
      "\n",
      "Testing loss: 116.54328024969465\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0100 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.27855779736268\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 8s 819us/step - loss: 0.0100 - val_loss: 0.0197\n",
      "\n",
      "Testing loss: 116.38407238008158\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 8s 822us/step - loss: 0.0100 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.32997543341222\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0099 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.33157272244948\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0098 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.5192109343944\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.0098 - val_loss: 0.0189\n",
      "\n",
      "Testing loss: 116.32550697498846\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 9s 961us/step - loss: 0.0097 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.75890859072454\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 8s 823us/step - loss: 0.0097 - val_loss: 0.0179\n",
      "\n",
      "Testing loss: 116.44960946140594\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0096 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.01103482815786\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 8s 799us/step - loss: 0.0096 - val_loss: 0.0194\n",
      "\n",
      "Testing loss: 116.12992949675575\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.0095 - val_loss: 0.0197\n",
      "\n",
      "Testing loss: 116.40760105087662\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0095 - val_loss: 0.0181\n",
      "\n",
      "Testing loss: 116.54330342902871\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 8s 823us/step - loss: 0.0094 - val_loss: 0.0195\n",
      "\n",
      "Testing loss: 116.28641306204437\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.0094 - val_loss: 0.0215\n",
      "\n",
      "Testing loss: 115.99230523123133\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 8s 825us/step - loss: 0.0094 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.49922300392748\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.0093 - val_loss: 0.0179\n",
      "\n",
      "Testing loss: 116.25346514138718\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 8s 833us/step - loss: 0.0093 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.04496898502417\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 8s 782us/step - loss: 0.0092 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 115.95786340186768\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 9s 963us/step - loss: 0.0091 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.99972033275655\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.0091 - val_loss: 0.0185\n",
      "\n",
      "Testing loss: 116.40606264936959\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 8s 827us/step - loss: 0.0091 - val_loss: 0.0195\n",
      "\n",
      "Testing loss: 116.2839544747861\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 8s 801us/step - loss: 0.0090 - val_loss: 0.0203\n",
      "\n",
      "Testing loss: 116.0806806123692\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.0090 - val_loss: 0.0194\n",
      "\n",
      "Testing loss: 116.45104393158773\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.0089 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.42355639708546\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 8s 791us/step - loss: 0.0089 - val_loss: 0.0183\n",
      "\n",
      "Testing loss: 116.48838032255615\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 8s 832us/step - loss: 0.0088 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.55882739668554\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0088 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.00134852131126\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.0088 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.43505657761128\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.0087 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.55529574776557\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0087 - val_loss: 0.0188\n",
      "\n",
      "Testing loss: 116.65483764473122\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.0086 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 115.87293341397555\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 783us/step - loss: 0.0086 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.2520562486175\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 8s 816us/step - loss: 0.0086 - val_loss: 0.0187\n",
      "\n",
      "Testing loss: 116.6509058423643\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0086 - val_loss: 0.0187\n",
      "\n",
      "Testing loss: 116.48306100976042\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.0085 - val_loss: 0.0180\n",
      "\n",
      "Testing loss: 116.6601673199791\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0085 - val_loss: 0.0188\n",
      "\n",
      "Testing loss: 116.63120464814898\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 9s 893us/step - loss: 0.0084 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.08423216596313\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0084 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.27132271448511\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0083 - val_loss: 0.0194\n",
      "\n",
      "Testing loss: 116.57454955328376\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0083 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.32521310085443\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 6s 650us/step - loss: 0.0083 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.62129390498973\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.0082 - val_loss: 0.0174\n",
      "\n",
      "Testing loss: 116.58419002741147\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 6s 641us/step - loss: 0.0082 - val_loss: 0.0181\n",
      "\n",
      "Testing loss: 116.93888699074088\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0082 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.27539974145337\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0081 - val_loss: 0.0185\n",
      "\n",
      "Testing loss: 116.744819124556\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 6s 649us/step - loss: 0.0081 - val_loss: 0.0185\n",
      "\n",
      "Testing loss: 116.31477707215983\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 6s 635us/step - loss: 0.0081 - val_loss: 0.0197\n",
      "\n",
      "Testing loss: 116.82773134578287\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0080 - val_loss: 0.0169\n",
      "\n",
      "Testing loss: 116.76233677791679\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 7s 679us/step - loss: 0.0080 - val_loss: 0.0194\n",
      "\n",
      "Testing loss: 116.93382641992268\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 6s 640us/step - loss: 0.0079 - val_loss: 0.0183\n",
      "\n",
      "Testing loss: 116.56165595792285\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 7s 695us/step - loss: 0.0079 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.3712983630356\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 7s 707us/step - loss: 0.0079 - val_loss: 0.0187\n",
      "\n",
      "Testing loss: 116.60446884537212\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0079 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.46252347486744\n",
      "Epoch 233/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0078 - val_loss: 0.0188\n",
      "\n",
      "Testing loss: 116.47633119504081\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 7s 735us/step - loss: 0.0078 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.64891400374395\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0078 - val_loss: 0.0183\n",
      "\n",
      "Testing loss: 116.19084568829824\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 7s 679us/step - loss: 0.0077 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.39587607088274\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0077 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.4377283057963\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 7s 688us/step - loss: 0.0077 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.3367792567025\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.0076 - val_loss: 0.0188\n",
      "\n",
      "Testing loss: 116.4642144705264\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 7s 679us/step - loss: 0.0076 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 115.97617146300644\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.0076 - val_loss: 0.0203\n",
      "\n",
      "Testing loss: 115.99364676493134\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.0076 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.83460285999172\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 7s 688us/step - loss: 0.0076 - val_loss: 0.0194\n",
      "\n",
      "Testing loss: 116.23246112083088\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 662us/step - loss: 0.0075 - val_loss: 0.0189\n",
      "\n",
      "Testing loss: 116.24777187868665\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0075 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.22554723014096\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0075 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.40265668353244\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.0074 - val_loss: 0.0195\n",
      "\n",
      "Testing loss: 116.45965568032184\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.0074 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.6113882021773\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 626us/step - loss: 0.0074 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.35648071389244\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0073 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 115.88831913211965\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.001)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyh = model.fit(X_train_scaled.values,\n",
    "                     y_train_scaled, epochs=250, \n",
    "                     verbose=1, \n",
    "                     validation_data=(X_val_scaled.values, y_val_scaled),\n",
    "                     callbacks=[TestCallback((X_test_scaled.values, yTest))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss= model.evaluate(X_test_scaled, yTest, verbose=0)\n",
    "train_loss=historyh.history['loss']\n",
    "xc = range(numEpochs)\n",
    "plt.figure(1, figsize=(10, 6))\n",
    "plt.plot(xc,train_loss)\n",
    "#plt.plot(xc,test_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.title('Sallow neural networks \\n Training Loss vs Testing Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
