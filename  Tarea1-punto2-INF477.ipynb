{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n",
      "       Unnamed: 0           0          1          2          3          4  \\\n",
      "0               0   73.516695  17.817765  12.469551  12.458130  12.454607   \n",
      "1               1   73.516695  20.649126  18.527789  17.891535  17.887995   \n",
      "2               2   73.516695  17.830377  12.512263  12.404775  12.394493   \n",
      "3               3   73.516695  17.875810  17.871259  17.862402  17.850920   \n",
      "4               4   73.516695  17.883818  17.868256  17.864221  17.818540   \n",
      "5               5   53.358707  17.038820  16.981436  16.167446  16.137631   \n",
      "6               6   53.358707  17.040919  16.975955  16.168874  16.131888   \n",
      "7               7   53.358707  15.190748  15.134397  15.078282  13.721467   \n",
      "8               8   73.516695  20.648642  18.559611  17.674347  16.152675   \n",
      "9               9   73.516695  17.563342  17.562598  12.653657  12.540799   \n",
      "10             10   73.516695  18.593826  17.902116  17.791648  17.709349   \n",
      "11             11   73.516695  17.893322  17.892435  17.891329  17.835056   \n",
      "12             12   73.516695  17.892036  17.835452  17.796401  12.544441   \n",
      "13             13  388.023441  66.102901  35.415029  35.414463  21.068417   \n",
      "14             14   73.516695  19.145558  17.855414  17.779134  12.927104   \n",
      "15             15   73.516695  20.686211  18.654076  18.029737  16.104124   \n",
      "16             16   73.516695  23.621854  23.620385  20.741845  18.712919   \n",
      "17             17   73.516695  20.882498  20.704776  18.830990  18.239768   \n",
      "18             18   53.358707  17.152774  16.979009  16.165980  16.131888   \n",
      "19             19   73.516695  20.792001  20.781995  18.699353  15.970705   \n",
      "20             20   73.516695  20.752137  19.148052  18.575154  17.872719   \n",
      "21             21  388.023441  29.794358  29.450794  19.838451  19.640707   \n",
      "22             22   73.516695  20.707815  17.725040  17.690824  12.899468   \n",
      "23             23   53.358707  17.139133  16.062083  15.810563  15.258986   \n",
      "24             24   53.358707  15.192917  15.192453  15.191957  13.696268   \n",
      "25             25   73.516695  20.722774  18.596453  17.866674  17.743940   \n",
      "26             26   73.516695  20.765497  19.143140  18.645819  17.736610   \n",
      "27             27   73.516695  20.800040  12.574005  12.558918  12.517330   \n",
      "28             28  388.023441  46.500552  46.493093  34.676868  29.085904   \n",
      "29             29   53.358707  18.092880  15.249478  15.177545  13.653715   \n",
      "...           ...         ...        ...        ...        ...        ...   \n",
      "16212       16243  388.023441  29.658517  29.655842  23.552305  23.550885   \n",
      "16213       16244   73.516695  18.635498  17.804251  15.228614  13.653764   \n",
      "16214       16245   53.358707  19.148633  13.653248  13.652988  13.652850   \n",
      "16215       16246   73.516695  20.698171  15.924907  15.855703  13.654009   \n",
      "16216       16247   73.516695  18.643484  18.640762  17.802607  17.802277   \n",
      "16217       16248   73.516695  17.870020  15.238941  15.176526  15.176523   \n",
      "16218       16249   53.358707  19.150693  16.405490  15.022080  14.895449   \n",
      "16219       16250   36.858105  14.107138  12.674530  12.544437  12.505901   \n",
      "16220       16251   73.516695  20.698855  20.698855  15.893020  15.892046   \n",
      "16221       16252   53.358707  15.754498  15.753712  13.653954  13.653736   \n",
      "16222       16253   73.516695  17.677578  12.519029  12.479941  12.477739   \n",
      "16223       16254   73.516695  20.743557  20.741361  18.598127  18.595527   \n",
      "16224       16255   73.516695  20.801457  20.801457  18.634187  18.634183   \n",
      "16225       16256   73.516695  20.581963  15.924696  15.751649  15.452521   \n",
      "16226       16257   53.358707  16.359301  16.358704  15.148839  13.825358   \n",
      "16227       16258   53.358707  16.435036  16.433452  15.192854  15.192072   \n",
      "16228       16259   36.858105  14.159026  13.653760  13.653143  13.653143   \n",
      "16229       16260   53.358707  16.525299  16.435115  14.202458  13.827072   \n",
      "16230       16261   73.516695  20.731531  20.731285  16.422832  16.422798   \n",
      "16231       16262   73.516695  20.221994  18.690270  18.632741  18.405014   \n",
      "16232       16263   73.516695  17.860880  17.780884  17.779987  12.511356   \n",
      "16233       16264  388.023441  46.723458  46.722364  28.505545  28.472771   \n",
      "16234       16265   73.516695  23.607539  23.606041  21.378625  17.851697   \n",
      "16235       16266   73.516695  20.643181  20.588968  18.703556  18.673772   \n",
      "16236       16267   73.516695  20.610314  20.554562  18.683667  18.653061   \n",
      "16237       16268   73.516695  20.753166  18.624076  17.872009  17.851690   \n",
      "16238       16269   73.516695  20.724740  18.579933  17.741621  14.716676   \n",
      "16239       16270   53.358707  20.820797  19.150234  19.148721  15.135514   \n",
      "16240       16271   53.358707  15.707759  15.707644  13.653838  13.653570   \n",
      "16241       16272   53.358707  15.708752  15.708094  13.653893  13.653176   \n",
      "\n",
      "               5          6          7          8    ...      1267  1268  \\\n",
      "0      12.447345  12.433065  12.426926  12.387474    ...       0.0   0.0   \n",
      "1      17.871731  17.852586  17.729842  15.864270    ...       0.0   0.0   \n",
      "2      12.391564  12.324461  12.238106  10.423249    ...       0.0   0.0   \n",
      "3      17.850440  12.558105  12.557645  12.517583    ...       0.0   0.0   \n",
      "4      12.508657  12.490519  12.450098  10.597068    ...       0.0   0.0   \n",
      "5      16.053239  15.713944  15.432893  15.421116    ...       0.0   0.0   \n",
      "6      16.073074  15.843838  15.638061  15.160532    ...       0.0   0.0   \n",
      "7      13.720334  13.671396  13.655370  13.654554    ...       0.0   0.0   \n",
      "8      14.266867  13.666125  13.657868  13.642132    ...       0.0   0.0   \n",
      "9      12.539160  12.536825  12.508203  12.489843    ...       0.0   0.0   \n",
      "10     17.659515  17.569676  17.188025  17.152301    ...       0.0   0.0   \n",
      "11     17.797049  17.796887  17.796494  12.519782    ...       0.0   0.0   \n",
      "12     12.520178  12.520042  12.501077  12.205003    ...       0.0   0.0   \n",
      "13     20.972084  18.855496  18.854617  18.833136    ...       0.0   0.0   \n",
      "14     12.337087  12.336829  12.276143  12.205558    ...       0.0   0.0   \n",
      "15     15.732605  15.382404  14.561514  13.653314    ...       0.0   0.0   \n",
      "16     15.755895  15.645016  15.533408  15.392356    ...       0.0   0.0   \n",
      "17     18.026817  14.817389  14.804453  14.229886    ...       0.0   0.0   \n",
      "18     16.122930  16.090475  15.157053  12.520047    ...       0.0   0.0   \n",
      "19     15.420394  14.874331  12.647200  12.647035    ...       0.0   0.0   \n",
      "20     17.850143  17.739271  14.677496  12.976874    ...       0.0   0.0   \n",
      "21     18.080196  15.228163  15.179768  15.179574    ...       0.0   0.0   \n",
      "22     12.809439  12.740222  12.667372  12.576495    ...       0.0   0.0   \n",
      "23     15.258296  13.676434  13.675309  13.654199    ...       0.0   0.0   \n",
      "24     13.695720  13.655091  13.653141  13.653066    ...       0.0   0.0   \n",
      "25     14.701212  13.740966  13.738704  13.654970    ...       0.0   0.0   \n",
      "26     17.699663  17.687518  14.701643  14.216043    ...       0.0   0.0   \n",
      "27     12.384754  12.355934  12.344451  12.316290    ...       0.0   0.0   \n",
      "28     19.000431  18.995270  18.631019  18.630794    ...       0.0   0.0   \n",
      "29     13.653147  13.652852  13.652687  13.652476    ...       0.0   0.0   \n",
      "...          ...        ...        ...        ...    ...       ...   ...   \n",
      "16212  23.446952  23.446603  23.424316  22.152848    ...       0.0   0.0   \n",
      "16213  13.653754  13.653413  13.652802  13.652570    ...       0.0   0.0   \n",
      "16214  13.652754  13.652495  13.652167  12.965136    ...       0.0   0.0   \n",
      "16215  13.653321  13.653014  13.652836  13.652436    ...       0.0   0.0   \n",
      "16216  13.653867  13.653335  13.652740  13.652676    ...       0.0   0.0   \n",
      "16217  12.525824  12.525722  12.507239  12.484568    ...       0.0   0.0   \n",
      "16218  12.521816  12.521367  12.488777  12.488520    ...       0.0   0.0   \n",
      "16219  12.480945  12.469779  12.345929  12.286274    ...       0.0   0.0   \n",
      "16220  15.875667  15.875528  13.653570  13.652995    ...       0.0   0.0   \n",
      "16221  13.653736  13.653482  13.653142  13.653000    ...       0.0   0.0   \n",
      "16222  12.452035  12.426147  12.421406  12.308018    ...       0.0   0.0   \n",
      "16223  14.582338  14.576939  12.993091  12.611977    ...       0.0   0.0   \n",
      "16224  14.566950  14.566947  14.254289  14.252941    ...       0.0   0.0   \n",
      "16225  14.852202  13.654823  13.653847  13.653692    ...       0.0   0.0   \n",
      "16226  13.825283  13.654598  13.653577  13.653275    ...       0.0   0.0   \n",
      "16227  15.191944  13.836318  13.787761  13.744369    ...       0.0   0.0   \n",
      "16228  13.652803  13.652756  13.652713  13.092874    ...       0.0   0.0   \n",
      "16229  13.743711  13.695585  13.694933  12.759605    ...       0.0   0.0   \n",
      "16230  16.136916  16.135040  15.893446  15.853311    ...       0.0   0.0   \n",
      "16231  17.735798  14.707738  13.842602  13.689474    ...       0.0   0.0   \n",
      "16232  12.510997  12.481791  12.475366  10.571260    ...       0.0   0.0   \n",
      "16233  19.796532  19.731292  14.209230  14.144322    ...       0.0   0.0   \n",
      "16234  15.649065  15.422739  13.654045  13.653121    ...       0.0   0.0   \n",
      "16235  14.895437  14.894662  13.684300  13.683890    ...       0.0   0.0   \n",
      "16236  14.895197  14.894752  13.684066  13.683760    ...       0.0   0.0   \n",
      "16237  17.851254  17.742176  14.655754  12.706683    ...       0.0   0.0   \n",
      "16238  13.697829  13.697558  13.653512  13.652942    ...       0.0   0.0   \n",
      "16239  15.123685  12.942704  12.938162  12.488633    ...       0.0   0.0   \n",
      "16240  13.653314  13.652591  13.652585  13.652550    ...       0.0   0.0   \n",
      "16241  13.653120  13.652930  13.652528  13.652322    ...       0.0   0.0   \n",
      "\n",
      "       1269  1270  1271  1272  1273  1274  pubchem_id        Eat  \n",
      "0       0.5   0.0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
      "1       0.0   0.0   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
      "2       0.0   0.0   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
      "3       0.0   0.0   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
      "4       0.0   0.0   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
      "5       0.0   0.0   0.0   0.0   0.0   0.0       25017 -16.169604  \n",
      "6       0.0   0.0   0.0   0.0   0.0   0.0       25019 -17.378477  \n",
      "7       0.0   0.0   0.0   0.0   0.0   0.0       25023 -15.673737  \n",
      "8       0.0   0.0   0.0   0.0   0.0   0.0       25032 -10.427851  \n",
      "9       0.0   0.0   0.0   0.0   0.0   0.0       25042  -8.744178  \n",
      "10      0.0   0.0   0.0   0.0   0.0   0.0       25051 -12.244325  \n",
      "11      0.0   0.0   0.0   0.0   0.0   0.0       25054 -14.149376  \n",
      "12      0.0   0.0   0.0   0.0   0.0   0.0       25055  -8.572029  \n",
      "13      0.0   0.0   0.0   0.0   0.0   0.0       25057 -13.696046  \n",
      "14      0.0   0.0   0.0   0.0   0.0   0.0       25066  -8.293733  \n",
      "15      0.0   0.0   0.0   0.0   0.0   0.0       25072 -11.248055  \n",
      "16      0.0   0.0   0.0   0.0   0.0   0.0       25089 -10.170327  \n",
      "17      0.0   0.0   0.0   0.0   0.0   0.0       25090 -11.116150  \n",
      "18      0.0   0.0   0.0   0.0   0.0   0.0       25093 -11.642478  \n",
      "19      0.0   0.0   0.0   0.0   0.0   0.0       25098  -8.329572  \n",
      "20      0.0   0.0   0.0   0.0   0.0   0.0       25106  -8.170857  \n",
      "21      0.0   0.0   0.0   0.0   0.0   0.0       25118 -12.796349  \n",
      "22      0.0   0.0   0.0   0.0   0.0   0.0       25121 -10.500986  \n",
      "23      0.0   0.0   0.0   0.0   0.0   0.0       25123 -11.520089  \n",
      "24      0.0   0.0   0.0   0.0   0.0   0.0       25125 -11.197458  \n",
      "25      0.0   0.0   0.0   0.0   0.0   0.0       25129 -13.505988  \n",
      "26      0.0   0.0   0.0   0.0   0.0   0.0       25131 -14.299903  \n",
      "27      0.0   0.0   0.0   0.0   0.0   0.0       25132 -11.331558  \n",
      "28      0.0   0.0   0.0   0.0   0.0   0.0       25141  -9.290619  \n",
      "29      0.0   0.0   0.0   0.0   0.0   0.0       25142  -9.586835  \n",
      "...     ...   ...   ...   ...   ...   ...         ...        ...  \n",
      "16212   0.0   0.0   0.0   0.0   0.0   0.0       74895  -6.849553  \n",
      "16213   0.0   0.0   0.0   0.0   0.0   0.0       74896 -10.408032  \n",
      "16214   0.0   0.0   0.0   0.0   0.0   0.0       74897 -10.100940  \n",
      "16215   0.0   0.0   0.0   0.0   0.0   0.0       74903 -11.170072  \n",
      "16216   0.0   0.0   0.0   0.0   0.0   0.0       74904 -11.163202  \n",
      "16217   0.0   0.0   0.0   0.0   0.0   0.0       74905 -15.003575  \n",
      "16218   0.0   0.0   0.0   0.0   0.0   0.0       74906 -11.932200  \n",
      "16219   0.0   0.0   0.0   0.0   0.0   0.0       74910 -11.987681  \n",
      "16220   0.0   0.0   0.0   0.0   0.0   0.0       74911 -11.908604  \n",
      "16221   0.0   0.0   0.0   0.0   0.0   0.0       74912 -12.330795  \n",
      "16222   0.0   0.0   0.0   0.0   0.0   0.0       74917 -13.007444  \n",
      "16223   0.0   0.0   0.0   0.0   0.0   0.0       74918 -15.191880  \n",
      "16224   0.0   0.0   0.0   0.0   0.0   0.0       74919  -6.746408  \n",
      "16225   0.0   0.0   0.0   0.0   0.0   0.0       74921 -11.988657  \n",
      "16226   0.0   0.0   0.0   0.0   0.0   0.0       74922 -10.752393  \n",
      "16227   0.0   0.0   0.0   0.0   0.0   0.0       74927 -12.044004  \n",
      "16228   0.0   0.0   0.0   0.0   0.0   0.0       74928 -10.988833  \n",
      "16229   0.0   0.0   0.0   0.0   0.0   0.0       74935 -10.635767  \n",
      "16230   0.0   0.0   0.0   0.0   0.0   0.0       74947  -8.633728  \n",
      "16231   0.0   0.0   0.0   0.0   0.0   0.0       74953 -11.206702  \n",
      "16232   0.0   0.0   0.0   0.0   0.0   0.0       74956  -6.823134  \n",
      "16233   0.0   0.0   0.0   0.0   0.0   0.0       74963 -11.964498  \n",
      "16234   0.0   0.0   0.0   0.0   0.0   0.0       74968  -8.918597  \n",
      "16235   0.0   0.0   0.0   0.0   0.0   0.0       74971 -11.794917  \n",
      "16236   0.0   0.0   0.0   0.0   0.0   0.0       74974 -11.809176  \n",
      "16237   0.0   0.0   0.0   0.0   0.0   0.0       74976  -8.876123  \n",
      "16238   0.0   0.0   0.0   0.0   0.0   0.0       74977 -13.105268  \n",
      "16239   0.0   0.0   0.0   0.0   0.0   0.0       74978 -16.801464  \n",
      "16240   0.0   0.0   0.0   0.0   0.0   0.0       74979 -13.335088  \n",
      "16241   0.0   0.0   0.0   0.0   0.0   0.0       74980 -13.336696  \n",
      "\n",
      "[16242 rows x 1278 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "print(datos)\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_train)\n",
    "#Transform training\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Transform val\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "y_val_scaled = X_val_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Transform test\n",
    "X_Test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "y_Test_scaled = X_Test_scaled.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Para el mismo problema definido anteriormente (sección 1) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (loss) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de gradients (link) en el backend de Keras. Deberá generar un histograma para todos los pesos de cada capa antes y despues del entrenamiento con 250 epochs. Comente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a88fa18abb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlistOfVariableTensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistOfVariableTensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We can now calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Capa 1 entrenada')\n",
    "plt.hist(evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###calculate gradients\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "loss = keras.losses.mean_squared_error(model.layers[0].output,y_train_scaled)\n",
    "loss2 = keras.losses.mean_squared_error(model.layers[1].output,y_train_scaled)\n",
    "listOfVariableTensors = model.layers[0].trainable_weights \n",
    "list1fVariableTensors = model.layers[1].trainable_weights \n",
    "\n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "gradients2 = K.gradients(loss2, list1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss2,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients2 = sess.run(gradients2,feed_dict={model.input:X_train_scaled.values})\n",
    "\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "\n",
    "\n",
    "Loss = keras.losses.mean_squared_error(model2.layers[0].output,y_train_scaled)\n",
    "Loss2 = keras.losses.mean_squared_error(model2.layers[1].output,y_train_scaled)\n",
    "#loss = keras.losses.mean_squared_error(model2.output,y_train_scaled)--original\n",
    "\n",
    "ListOfVariableTensors = model2.layers[0].trainable_weights \n",
    "List1fVariableTensors = model2.layers[1].trainable_weights \n",
    "\n",
    "Gradients = K.gradients(Loss, ListOfVariableTensors) #We can now calculate the gradients.\n",
    "Gradients2 = K.gradients(Loss2,List1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "Evaluated_gradients = sess.run(Gradients,feed_dict={model2.input:X_train_scaled.values})\n",
    "Evaluated_gradients2 = sess.run(Gradients2,feed_dict={model2.input:X_train_scaled.values})\n",
    "\n",
    "Evaluated_gradients = [gradient/len(y_train) for gradient in Evaluated_gradients]\n",
    "Evaluated_gradients2 = [gradient/len(y_train) for gradient in Evaluated_gradients2]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(evaluated_gradients2, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.title('Capa 1 entrenada')\n",
    "plt.hist(Evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.title('Capa 2 entrenada')\n",
    "plt.hist(Evaluated_gradients2, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "#history2 = model2.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "history2=pd.read_csv(\"history2b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###calculate gradients\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "loss = keras.losses.mean_squared_error(model2.layers[0].output,y_train_scaled)\n",
    "loss2 = keras.losses.mean_squared_error(model2.layers[1].output,y_train_scaled)\n",
    "#loss = keras.losses.mean_squared_error(model2.output,y_train_scaled)--original\n",
    "\n",
    "listOfVariableTensors = model2.layers[0].trainable_weights \n",
    "list1fVariableTensors = model2.layers[1].trainable_weights \n",
    "\n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "gradients2 = K.gradients(loss2, list1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model2.input:X_train_scaled.values})\n",
    "evaluated_gradients2 = sess.run(gradients2,feed_dict={model2.input:X_train_scaled.values})\n",
    "\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients, bins = 160)\n",
    "plt.grid(True)\n",
    "plt.subplot(212)\n",
    "plt.title('Layer 2 training')\n",
    "plt.hist(evaluated_gradients2, bins = 160)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weightss, biases = model2.layers[0].get_weights()\n",
    "weightss1, biases1 = model2.layers[1].get_weights()\n",
    "weightse, biases = model.layers[0].get_weights()\n",
    "weights1e, biases1 = model.layers[1].get_weights()\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(221)\n",
    "plt.title('Weights Layer 1')\n",
    "plt.hist(weightss, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.title('Weights Layer 2')\n",
    "plt.hist(weightss1, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.title('Weights Layer 1 training')\n",
    "plt.hist(weightse, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.title('Weights Layer 2 training')\n",
    "plt.hist(weights1e, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               326656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 326,913\n",
      "Trainable params: 326,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()\n",
    "result= pd.DataFrame(history2.history)\n",
    "result.to_csv(\"history2b.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento pero ahora entrenando una red mucho más profunda de 6 capas, 5 capas escondidas y 1 de salida. Utilice el inicializador de pesos *uniform* el cual inicializa mediante una distribución uniforme entre $-1/\\sqrt{N}$ y $1/\\sqrt{N}$ para cada capa, con $N$ el número de neuronas de la capa anterior. Por simplicidad visualice las 3-4 primeras capas de la red. Comente si observa el efecto del *gradiente desvaneciente* antes y/o después de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = Sequential()\n",
    "modelc.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelc.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "#historyc = model2.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "#result= pd.DataFrame(historyc.history)\n",
    "#result.to_csv(\"history2c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc2 = Sequential()\n",
    "modelc2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelc2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyc=pd.read_csv(\"history2c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-45b7825b1109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mEvaluated_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodelc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mEvaluated_gradients2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradients2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodelc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mEvaluated_gradients3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradients3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodelc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1113\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \"\"\"\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[0;32m--> 237\u001b[0;31m                       (fetch, type(fetch)))\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "###calculate gradients\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "loss = keras.losses.mean_squared_error(modelc.layers[0].output,y_train_scaled)\n",
    "loss2 = keras.losses.mean_squared_error(modelc.layers[1].output,y_train_scaled)\n",
    "loss3 = keras.losses.mean_squared_error(modelc.layers[2].output,y_train_scaled)\n",
    "listOfVariableTensors = modelc.layers[0].trainable_weights \n",
    "list1fVariableTensors = modelc.layers[1].trainable_weights \n",
    "list2fVariableTensors = modelc.layers[2].trainable_weights \n",
    "\n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "gradients2 = K.gradients(loss2, list1fVariableTensors) \n",
    "gradients3 = K.gradients(loss3, list1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss2,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelc.input:X_train_scaled.values})\n",
    "evaluated_gradients2 = sess.run(gradients2,feed_dict={modelc.input:X_train_scaled.values})\n",
    "evaluated_gradients3 = sess.run(gradients3,feed_dict={modelc.input:X_train_scaled.values})\n",
    "\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "evaluated_gradients3 = [gradient/len(y_train) for gradient in evaluated_gradients3]\n",
    "\n",
    "#################################################################\n",
    "Loss = keras.losses.mean_squared_error(modelc.layers[0].output,y_train_scaled)\n",
    "Loss2 = keras.losses.mean_squared_error(modelc.layers[1].output,y_train_scaled)\n",
    "Loss3 = keras.losses.mean_squared_error(modelc.layers[2].output,y_train_scaled)\n",
    "ListOfVariableTensors = modelc2.layers[0].trainable_weights \n",
    "List1fVariableTensors = modelc2.layers[1].trainable_weights \n",
    "List2fVariableTensors = modelc2.layers[2].trainable_weights \n",
    "\n",
    "Gradients = K.gradients(Loss, ListOfVariableTensors) #We can now calculate the gradients.\n",
    "Gradients2 = K.gradients(Loss2,List1fVariableTensors) \n",
    "Gradients3 = K.gradients(Loss3, List1fVariableTensors) \n",
    "#gradients = tf.keras.backend.gradients(loss,listOfVariableTensors)\n",
    "#gradients2 = tf.keras.backend.gradients(loss2,list1fVariableTensors)\n",
    "\n",
    "sess = K.get_session()\n",
    "Evaluated_gradients = sess.run(Gradients,feed_dict={modelc2.input:X_train_scaled.values})\n",
    "Evaluated_gradients2 = sess.run(Gradients2,feed_dict={modelc2.input:X_train_scaled.values})\n",
    "Evaluated_gradients3 = sess.run(Gradients3,feed_dict={modelc2.input:X_train_scaled.values})\n",
    "\n",
    "Evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "Evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "Evaluated_gradients3 = [gradient/len(y_train) for gradient in evaluated_gradients3]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(323)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(evaluated_gradients2[0][:], bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(325)\n",
    "plt.title('Layer 1')\n",
    "plt.hist(evaluated_gradients3, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(322)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(Evaluated_gradients, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(324)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(Evaluated_gradients2, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(326)\n",
    "plt.title('Layer 2')\n",
    "plt.hist(Evaluated_gradients3, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.35309107e-04, -1.46957856e-04, -1.72065906e-04, ...,\n",
      "        -1.40854041e-04, -1.50902808e-04, -1.53867077e-04],\n",
      "       [-1.33886351e-04, -1.42309145e-04, -1.85017314e-04, ...,\n",
      "        -1.46147038e-04, -1.47369399e-04, -1.76563160e-04],\n",
      "       [-3.41052801e-05, -7.56938171e-05, -1.04875340e-04, ...,\n",
      "        -7.04733029e-05, -6.40679427e-05, -1.51165747e-04],\n",
      "       ...,\n",
      "       [-5.20299618e-05, -2.62901776e-05, -5.86863280e-05, ...,\n",
      "         1.08592874e-04,  1.44283811e-04,  1.61558957e-04],\n",
      "       [-3.85868407e-05, -2.18436835e-05, -4.34979702e-05, ...,\n",
      "         1.64642042e-04,  1.65174584e-04,  8.25671159e-05],\n",
      "       [-3.86196407e-05, -2.18620698e-05, -4.35345246e-05, ...,\n",
      "         1.64619749e-04,  1.65380392e-04,  8.43845119e-05]], dtype=float32), array([0.00081257, 0.00045968, 0.0009156 , 0.00068322, 0.00078898,\n",
      "       0.00084695, 0.00089989, 0.00074167, 0.0007683 , 0.00084658,\n",
      "       0.00054385, 0.00067213, 0.00073001, 0.00086861, 0.00090241,\n",
      "       0.0008125 , 0.00075266, 0.00076128, 0.00080196, 0.00074092,\n",
      "       0.00061141, 0.00081649, 0.00073696, 0.00077746, 0.00082658,\n",
      "       0.00070236, 0.00037121, 0.00080439, 0.00082424, 0.00078577,\n",
      "       0.00061505, 0.0004356 , 0.00065577, 0.00074195, 0.00067245,\n",
      "       0.00065069, 0.00074514, 0.0007819 , 0.00083494, 0.00080809,\n",
      "       0.00082776, 0.0009092 , 0.0008816 , 0.00080446, 0.00087819,\n",
      "       0.00057406, 0.00085006, 0.00087618, 0.00065511, 0.00078164,\n",
      "       0.00081291, 0.00084235, 0.0007037 , 0.00075839, 0.00076472,\n",
      "       0.00090145, 0.00074592, 0.0007958 , 0.00087042, 0.00073711,\n",
      "       0.00047525, 0.00077897, 0.00067858, 0.00086616, 0.00086787,\n",
      "       0.00080817, 0.00079443, 0.0008272 , 0.00077644, 0.00089598,\n",
      "       0.00069399, 0.00085162, 0.00079002, 0.000512  , 0.00065684,\n",
      "       0.00078651, 0.00031345, 0.00092442, 0.00088833, 0.00072674,\n",
      "       0.00086175, 0.00077018, 0.00078874, 0.00085473, 0.00078051,\n",
      "       0.00079624, 0.00081845, 0.00076991, 0.00072241, 0.00083587,\n",
      "       0.00073418, 0.00071222, 0.00079616, 0.00045041, 0.00077021,\n",
      "       0.00087809, 0.0008218 , 0.00038505, 0.00086702, 0.00075097,\n",
      "       0.00080306, 0.00072075, 0.00081701, 0.00084301, 0.00083042,\n",
      "       0.00081475, 0.00069619, 0.00079946, 0.00073246, 0.00085289,\n",
      "       0.00085051, 0.00079996, 0.00079021, 0.00082249, 0.00089423,\n",
      "       0.00078598, 0.00079146, 0.00078571, 0.00083724, 0.00074367,\n",
      "       0.00077138, 0.0008836 , 0.00090275, 0.0008787 , 0.00089638,\n",
      "       0.00085851, 0.00083237, 0.00068094, 0.00075842, 0.00079546,\n",
      "       0.00066907, 0.00089699, 0.00061107, 0.00083699, 0.00087152,\n",
      "       0.00067158, 0.00086427, 0.00076848, 0.00089659, 0.00055679,\n",
      "       0.00091751, 0.00070285, 0.0005697 , 0.00059938, 0.00088621,\n",
      "       0.00073545, 0.00079539, 0.00088404, 0.00089659, 0.00056359,\n",
      "       0.00069227, 0.00074305, 0.00073163, 0.00066485, 0.00088836,\n",
      "       0.00087488, 0.00086108, 0.00085235, 0.00084046, 0.00069612,\n",
      "       0.00081738, 0.00077178, 0.00085994, 0.00090295, 0.0008778 ,\n",
      "       0.00055458, 0.00081882, 0.00082142, 0.00092624, 0.00089382,\n",
      "       0.00083734, 0.00076609, 0.00089723, 0.00082653, 0.00077031,\n",
      "       0.0008515 , 0.00088281, 0.00036084, 0.00045481, 0.00076921,\n",
      "       0.00080819, 0.00087632, 0.00084636, 0.00077757, 0.00071978,\n",
      "       0.00083859, 0.00073135, 0.00068478, 0.00076625, 0.0008807 ,\n",
      "       0.0006959 , 0.00084261, 0.00083321, 0.0007469 , 0.00066654,\n",
      "       0.00080687, 0.00085839, 0.00087208, 0.00070986, 0.00071226,\n",
      "       0.00072822, 0.00072839, 0.00074959, 0.00074123, 0.00086619,\n",
      "       0.00073307, 0.0005328 , 0.0008024 , 0.00078529, 0.00080572,\n",
      "       0.00059461, 0.00074674, 0.00086966, 0.00048009, 0.00069435,\n",
      "       0.00082881, 0.00088568, 0.00073478, 0.00087173, 0.00066293,\n",
      "       0.00085405, 0.00067176, 0.00073764, 0.00064755, 0.00085589,\n",
      "       0.00062767, 0.00089068, 0.00068441, 0.00089335, 0.00088418,\n",
      "       0.00087253, 0.00067372, 0.00075315, 0.00076795, 0.00085997,\n",
      "       0.00070408, 0.00077537, 0.00089441, 0.0006897 , 0.0003206 ,\n",
      "       0.00075738, 0.00086683, 0.00070942, 0.00055144, 0.00076418,\n",
      "       0.0009316 , 0.00085511, 0.00081817, 0.000851  , 0.0006587 ,\n",
      "       0.00090006, 0.00074479, 0.00087144, 0.00053194, 0.00070061,\n",
      "       0.00070296], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(evaluated_gradients3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightss, biases = modelc.layers[0].get_weights()\n",
    "weightss1, biases1 = modelc.layers[1].get_weights()\n",
    "weightse, biases = modelc.layers[2].get_weights()\n",
    "weights1e, biases1 = modelc.layers[3].get_weights()\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"GridSpec w/ different subplotpars\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(221)\n",
    "plt.title('Weights Layer 1')\n",
    "plt.hist(weightss, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.title('Weights Layer 2')\n",
    "plt.hist(weightss1, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.title('Weights Layer 1 training')\n",
    "plt.hist(weightse, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.title('Weights Layer 2 training')\n",
    "plt.hist(weights1e, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento, pero ahora entrenando la red profunda con el inicializador de Glorot [[1]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/(N_{in}+N_{out})}$  y $\\sqrt{6/(N_{in}+N_{out})}$ . Por simplicidad visualice las 3-4 primeras capas de la red. Comente si el efecto del *gradiente desvaneciente* se amortigua antes y/o después de entrenar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 8s 844us/step - loss: 15.4202 - val_loss: 13.6638\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 8.2270 - val_loss: 2.5299\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 2.3861 - val_loss: 4.4246\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 6s 607us/step - loss: 1.5479 - val_loss: 0.7078\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 6s 636us/step - loss: 1.2048 - val_loss: 0.6749\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.9604 - val_loss: 0.5603\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 603us/step - loss: 0.8813 - val_loss: 1.3545\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.6560 - val_loss: 0.8100\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.5867 - val_loss: 0.3179\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.5460 - val_loss: 1.1003\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.4969 - val_loss: 0.5310\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 6s 615us/step - loss: 0.3868 - val_loss: 0.4820\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 8s 843us/step - loss: 0.3872 - val_loss: 0.3704\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.3615 - val_loss: 0.4261\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.3329 - val_loss: 0.2262\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 7s 752us/step - loss: 0.2926 - val_loss: 0.1764\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 7s 762us/step - loss: 0.2604 - val_loss: 0.1600\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.2729 - val_loss: 0.1587\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.2325 - val_loss: 0.5823\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.2396 - val_loss: 0.4334\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.2294 - val_loss: 0.1309\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.1765 - val_loss: 0.1172\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 6s 630us/step - loss: 0.1727 - val_loss: 0.1181\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.1862 - val_loss: 0.6345\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 5s 531us/step - loss: 0.1774 - val_loss: 0.1178\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.1668 - val_loss: 0.1072\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.1320 - val_loss: 0.1492\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.1437 - val_loss: 0.1086\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.1614 - val_loss: 0.3342\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 6s 587us/step - loss: 0.1245 - val_loss: 0.1143\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.1145 - val_loss: 0.1213\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.1260 - val_loss: 0.0882\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1358 - val_loss: 0.1143\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.1057 - val_loss: 0.0920\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.1066 - val_loss: 0.2801\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.1080 - val_loss: 0.1081\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.1053 - val_loss: 0.0991\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0962 - val_loss: 0.0751\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.1002 - val_loss: 0.0935\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 6s 661us/step - loss: 0.0965 - val_loss: 0.0714\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 6s 600us/step - loss: 0.0908 - val_loss: 0.0885\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0938 - val_loss: 0.0708\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0898 - val_loss: 0.0757\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 5s 465us/step - loss: 0.0833 - val_loss: 0.0888\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 5s 533us/step - loss: 0.0948 - val_loss: 0.1609\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 6s 611us/step - loss: 0.0860 - val_loss: 0.1238\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0729 - val_loss: 0.1080\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.0734 - val_loss: 0.1464\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 6s 588us/step - loss: 0.0844 - val_loss: 0.0630\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0773 - val_loss: 0.0706\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 0.0675 - val_loss: 0.0890\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.0834 - val_loss: 0.0704\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0649 - val_loss: 0.0637\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0625 - val_loss: 0.1061\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 8s 807us/step - loss: 0.0769 - val_loss: 0.0969\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0797 - val_loss: 0.0598\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 7s 736us/step - loss: 0.0739 - val_loss: 0.0835\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 7s 757us/step - loss: 0.0633 - val_loss: 0.0577\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 8s 819us/step - loss: 0.0582 - val_loss: 0.0804\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 7s 741us/step - loss: 0.0637 - val_loss: 0.0637\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0706 - val_loss: 0.0683\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0619 - val_loss: 0.0556\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 6s 594us/step - loss: 0.0672 - val_loss: 0.0525\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0598 - val_loss: 0.1147\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0591 - val_loss: 0.0586\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 8s 804us/step - loss: 0.0555 - val_loss: 0.1076\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0644 - val_loss: 0.0561\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0569 - val_loss: 0.0629\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 5s 552us/step - loss: 0.0564 - val_loss: 0.0706\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0631 - val_loss: 0.0736\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 6s 618us/step - loss: 0.0641 - val_loss: 0.0591\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0564 - val_loss: 0.0967\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 8s 823us/step - loss: 0.0514 - val_loss: 0.0522\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0482 - val_loss: 0.0509\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0534 - val_loss: 0.0605\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0524 - val_loss: 0.0471\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 9s 909us/step - loss: 0.0443 - val_loss: 0.0769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 9s 896us/step - loss: 0.0574 - val_loss: 0.0468\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 9s 873us/step - loss: 0.0497 - val_loss: 0.0690\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 9s 898us/step - loss: 0.0474 - val_loss: 0.0924\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 9s 875us/step - loss: 0.0463 - val_loss: 0.0510\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0530 - val_loss: 0.0567\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0501 - val_loss: 0.0705\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 10s 975us/step - loss: 0.0468 - val_loss: 0.0593\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 10s 978us/step - loss: 0.0511 - val_loss: 0.0763\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 7s 714us/step - loss: 0.0457 - val_loss: 0.0615\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 5s 492us/step - loss: 0.0484 - val_loss: 0.0449\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0494 - val_loss: 0.0534\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0424 - val_loss: 0.0483\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0467 - val_loss: 0.0487\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0436 - val_loss: 0.0541\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 6s 623us/step - loss: 0.0456 - val_loss: 0.0517\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0416 - val_loss: 0.0462\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0441 - val_loss: 0.1405\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0438 - val_loss: 0.0444\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0424 - val_loss: 0.0791\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 5s 536us/step - loss: 0.0489 - val_loss: 0.0586\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 5s 494us/step - loss: 0.0385 - val_loss: 0.0528\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 6s 591us/step - loss: 0.0385 - val_loss: 0.0534\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0383 - val_loss: 0.0434\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 5s 503us/step - loss: 0.0406 - val_loss: 0.0414\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.0410 - val_loss: 0.0638\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 5s 478us/step - loss: 0.0410 - val_loss: 0.0460\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0443 - val_loss: 0.0425\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0391 - val_loss: 0.0432\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 7s 736us/step - loss: 0.0417 - val_loss: 0.0509\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 8s 787us/step - loss: 0.0374 - val_loss: 0.0434\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0366 - val_loss: 0.0529\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0407 - val_loss: 0.0624\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0348 - val_loss: 0.0424\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.0372 - val_loss: 0.0402\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0434 - val_loss: 0.0541\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0400 - val_loss: 0.0513\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.0365 - val_loss: 0.0426\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0414 - val_loss: 0.0465\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 719us/step - loss: 0.0399 - val_loss: 0.0435\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 6s 662us/step - loss: 0.0373 - val_loss: 0.0752\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 6s 641us/step - loss: 0.0338 - val_loss: 0.0469\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 8s 816us/step - loss: 0.0347 - val_loss: 0.0388\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 7s 713us/step - loss: 0.0354 - val_loss: 0.0511\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 8s 785us/step - loss: 0.0380 - val_loss: 0.0422\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 8s 778us/step - loss: 0.0337 - val_loss: 0.0522\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.0386 - val_loss: 0.0428\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 8s 854us/step - loss: 0.0324 - val_loss: 0.0404\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0348 - val_loss: 0.0411\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 10s 1000us/step - loss: 0.0378 - val_loss: 0.0511\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 10s 984us/step - loss: 0.0348 - val_loss: 0.0501\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 9s 894us/step - loss: 0.0352 - val_loss: 0.0588\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.0398 - val_loss: 0.0454\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 9s 972us/step - loss: 0.0356 - val_loss: 0.0446\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 9s 968us/step - loss: 0.0300 - val_loss: 0.0407\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 9s 953us/step - loss: 0.0290 - val_loss: 0.0404\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 10s 998us/step - loss: 0.0312 - val_loss: 0.0560\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0345 - val_loss: 0.0374\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.0356 - val_loss: 0.0383\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 9s 894us/step - loss: 0.0342 - val_loss: 0.0400\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0275 - val_loss: 0.0694\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 9s 970us/step - loss: 0.0318 - val_loss: 0.0373\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0316 - val_loss: 0.0382\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 9s 891us/step - loss: 0.0302 - val_loss: 0.0402\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.0325 - val_loss: 0.0699\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.0360 - val_loss: 0.0455\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.0323 - val_loss: 0.0367\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 9s 873us/step - loss: 0.0284 - val_loss: 0.0372\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0303 - val_loss: 0.0460\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0339 - val_loss: 0.0428\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.0321 - val_loss: 0.0387\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0331 - val_loss: 0.0389\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 10s 981us/step - loss: 0.0302 - val_loss: 0.0407\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0329 - val_loss: 0.0447\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0273 - val_loss: 0.0390\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 9s 963us/step - loss: 0.0324 - val_loss: 0.0412\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 9s 959us/step - loss: 0.0290 - val_loss: 0.0378\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0299 - val_loss: 0.0389\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.0292 - val_loss: 0.0358\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 6s 626us/step - loss: 0.0296 - val_loss: 0.0507\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0293 - val_loss: 0.0660\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 8s 865us/step - loss: 0.0313 - val_loss: 0.0463\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.0276 - val_loss: 0.0363\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 8s 831us/step - loss: 0.0362 - val_loss: 0.0487\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 6s 615us/step - loss: 0.0281 - val_loss: 0.0413\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.0269 - val_loss: 0.0800\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.0281 - val_loss: 0.0441\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.0320 - val_loss: 0.0347\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0287 - val_loss: 0.0459\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0280 - val_loss: 0.0363\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0287 - val_loss: 0.0439\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 6s 576us/step - loss: 0.0268 - val_loss: 0.0524\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 8s 781us/step - loss: 0.0297 - val_loss: 0.0354\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 8s 772us/step - loss: 0.0271 - val_loss: 0.0339\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.0295 - val_loss: 0.0362\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0266 - val_loss: 0.0481\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 6s 652us/step - loss: 0.0286 - val_loss: 0.0420\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0255 - val_loss: 0.0365\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0266 - val_loss: 0.0381\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0270 - val_loss: 0.0422\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0268 - val_loss: 0.0348\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 5s 467us/step - loss: 0.0276 - val_loss: 0.0436\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0293 - val_loss: 0.0338\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0277 - val_loss: 0.0346\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 5s 510us/step - loss: 0.0252 - val_loss: 0.0552\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0273 - val_loss: 0.0689\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 6s 594us/step - loss: 0.0299 - val_loss: 0.0596\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0258 - val_loss: 0.0351\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0254 - val_loss: 0.0351\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0251 - val_loss: 0.0393\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 5s 510us/step - loss: 0.0265 - val_loss: 0.0624\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0242 - val_loss: 0.0414\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 5s 559us/step - loss: 0.0260 - val_loss: 0.0376\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0276 - val_loss: 0.0344\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0249 - val_loss: 0.0346\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 5s 525us/step - loss: 0.0257 - val_loss: 0.0491\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.0253 - val_loss: 0.0760\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0243 - val_loss: 0.0529\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 5s 562us/step - loss: 0.0243 - val_loss: 0.0339\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0273 - val_loss: 0.0479\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0248 - val_loss: 0.0580\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 6s 599us/step - loss: 0.0242 - val_loss: 0.0329\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.0244 - val_loss: 0.0354\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0233 - val_loss: 0.0386\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 6s 593us/step - loss: 0.0258 - val_loss: 0.0356\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0248 - val_loss: 0.0384\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0267 - val_loss: 0.0329\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0261 - val_loss: 0.0336\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0270 - val_loss: 0.0465\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0259 - val_loss: 0.0645\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0245 - val_loss: 0.0358\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 5s 550us/step - loss: 0.0234 - val_loss: 0.0374\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0252 - val_loss: 0.0316\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0253 - val_loss: 0.0505\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 5s 506us/step - loss: 0.0218 - val_loss: 0.0365\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0248 - val_loss: 0.0323\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 0.0225 - val_loss: 0.0332\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.0239 - val_loss: 0.0335\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0212 - val_loss: 0.0456\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0260 - val_loss: 0.0460\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0233 - val_loss: 0.0329\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.0262 - val_loss: 0.0340\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 533us/step - loss: 0.0253 - val_loss: 0.0540\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.0229 - val_loss: 0.0405\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0250 - val_loss: 0.0621\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 7s 686us/step - loss: 0.0254 - val_loss: 0.0333\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0255 - val_loss: 0.0332\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0240 - val_loss: 0.0336\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0220 - val_loss: 0.0369\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0215 - val_loss: 0.0576\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 5s 489us/step - loss: 0.0243 - val_loss: 0.0321\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 514us/step - loss: 0.0223 - val_loss: 0.0432\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.0265 - val_loss: 0.0417\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 6s 568us/step - loss: 0.0218 - val_loss: 0.0341\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 521us/step - loss: 0.0229 - val_loss: 0.0322\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0246 - val_loss: 0.0331\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0210 - val_loss: 0.0331\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0233 - val_loss: 0.0426\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0237 - val_loss: 0.0319\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0237 - val_loss: 0.0329\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0226 - val_loss: 0.0329\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 477us/step - loss: 0.0222 - val_loss: 0.0409\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 6s 616us/step - loss: 0.0237 - val_loss: 0.0338\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0224 - val_loss: 0.0346\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 5s 563us/step - loss: 0.0209 - val_loss: 0.0332\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0217 - val_loss: 0.0458\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 7s 756us/step - loss: 0.0219 - val_loss: 0.0540\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 624us/step - loss: 0.0224 - val_loss: 0.0320\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 547us/step - loss: 0.0226 - val_loss: 0.0444\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.0210 - val_loss: 0.0356\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 7s 737us/step - loss: 0.0206 - val_loss: 0.0335\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0228 - val_loss: 0.0356\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0199 - val_loss: 0.0371\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0220 - val_loss: 0.0313\n"
     ]
    }
   ],
   "source": [
    "modeld = Sequential()\n",
    "modeld.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modeld.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyd = modeld.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resultd= pd.DataFrame(historyd.history)\n",
    "resultd.to_csv(\"history2d.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Vuelva a repetir la experimentación ahora cambiando la función de activación por ReLU, es decir, deberá visualizar los gradientes de los pesos de cada capa antes y después del entrenamiento, con inicialización *uniform* y comparar con la inicialización de He [[2]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/N_{in}}$ y $\\sqrt{6/N_{in}} $. Comente si ocurre el mismo fenómeno anterior (para función sigmoidal) sobre el efecto del *gradiente desvaneciente* para la función ReLU. Explique la importancia de la inicialización de los pesos dependiendo de la arquitectura.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "...\n",
    "or\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='relu')) #he\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 27.4344 - val_loss: 2.8600\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.9867 - val_loss: 1.1283\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.5372 - val_loss: 1.5091\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.4210 - val_loss: 1.1276\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 6s 634us/step - loss: 0.3344 - val_loss: 0.7306\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.2691 - val_loss: 0.8697\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.2534 - val_loss: 0.8071\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.2207 - val_loss: 0.6698\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.1865 - val_loss: 0.6007\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.2037 - val_loss: 0.5931\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.1855 - val_loss: 0.5772\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.1759 - val_loss: 0.5993\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 6s 588us/step - loss: 0.1535 - val_loss: 0.4717\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.1516 - val_loss: 1.5862\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1355 - val_loss: 0.7435\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.1242 - val_loss: 0.5200\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.1102 - val_loss: 0.5257\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.1243 - val_loss: 0.4474\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1129 - val_loss: 0.4343\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 6s 630us/step - loss: 0.1053 - val_loss: 0.4304\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.1081 - val_loss: 0.4470\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.1094 - val_loss: 0.4190\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0940 - val_loss: 0.4378\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 6s 604us/step - loss: 0.1187 - val_loss: 0.4998\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0962 - val_loss: 0.4856\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 0.0947 - val_loss: 0.5427\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0903 - val_loss: 0.4063\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0858 - val_loss: 0.4867\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.0885 - val_loss: 0.5411\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0777 - val_loss: 0.4092\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0929 - val_loss: 0.3641\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0882 - val_loss: 0.4405\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0752 - val_loss: 0.4195\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.0726 - val_loss: 0.5243\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.0658 - val_loss: 0.5708\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0750 - val_loss: 0.5328\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0667 - val_loss: 0.8558\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.0766 - val_loss: 0.3785\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 7s 677us/step - loss: 0.0634 - val_loss: 0.3520\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0649 - val_loss: 0.3878\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0668 - val_loss: 0.3400\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0639 - val_loss: 0.3678\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0636 - val_loss: 0.3416\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0623 - val_loss: 0.3334\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 9s 896us/step - loss: 0.0600 - val_loss: 0.3361\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 0.0554 - val_loss: 0.3354\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.0555 - val_loss: 0.3388\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0562 - val_loss: 0.3784\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.0508 - val_loss: 0.3581\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0562 - val_loss: 0.7160\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0604 - val_loss: 0.4663\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0562 - val_loss: 0.3887\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 8s 857us/step - loss: 0.0516 - val_loss: 0.4434\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0507 - val_loss: 0.3382\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0509 - val_loss: 0.3520\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 0.0490 - val_loss: 0.3652\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0466 - val_loss: 0.3178\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 6s 576us/step - loss: 0.0527 - val_loss: 0.3269\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0479 - val_loss: 0.3419\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 6s 593us/step - loss: 0.0516 - val_loss: 0.4133\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0444 - val_loss: 0.3155\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 6s 600us/step - loss: 0.0462 - val_loss: 0.3955\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 7s 692us/step - loss: 0.0461 - val_loss: 0.3159\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0430 - val_loss: 0.3517\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0457 - val_loss: 0.3141\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0456 - val_loss: 0.3167\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0432 - val_loss: 0.3203\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 7s 705us/step - loss: 0.0873 - val_loss: 0.4329\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0456 - val_loss: 0.3465\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0425 - val_loss: 0.3079\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0417 - val_loss: 0.3443\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.0405 - val_loss: 0.3154\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 0.0461 - val_loss: 0.3097\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0411 - val_loss: 0.3205\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0374 - val_loss: 0.3316\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0381 - val_loss: 0.2913\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 6s 624us/step - loss: 0.0374 - val_loss: 0.3275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0413 - val_loss: 0.2902\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0421 - val_loss: 0.2978\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 6s 654us/step - loss: 0.0385 - val_loss: 0.2980\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 5s 508us/step - loss: 0.0478 - val_loss: 0.3138\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0373 - val_loss: 0.3032\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 7s 734us/step - loss: 0.0355 - val_loss: 0.3118\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0383 - val_loss: 0.3205\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.0370 - val_loss: 0.3064\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 9s 951us/step - loss: 0.0382 - val_loss: 0.2913\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 9s 965us/step - loss: 0.0437 - val_loss: 0.3126\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 8s 783us/step - loss: 0.0419 - val_loss: 0.2976\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 769us/step - loss: 0.0363 - val_loss: 0.2939\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.0385 - val_loss: 0.2963\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 8s 795us/step - loss: 0.0372 - val_loss: 0.3050\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 5s 539us/step - loss: 0.0313 - val_loss: 0.3327\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0356 - val_loss: 0.2937\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 7s 730us/step - loss: 0.0317 - val_loss: 0.3130\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0374 - val_loss: 0.2817\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0329 - val_loss: 0.3970\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0346 - val_loss: 0.3024\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0316 - val_loss: 0.2884\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0320 - val_loss: 0.3166\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.0366 - val_loss: 0.2922\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0336 - val_loss: 0.2925\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 8s 847us/step - loss: 0.0337 - val_loss: 0.2882\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0358 - val_loss: 0.2943\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0342 - val_loss: 0.2870\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0346 - val_loss: 0.2862\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.0341 - val_loss: 0.3093\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.0341 - val_loss: 0.2774\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0307 - val_loss: 0.3988\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 9s 886us/step - loss: 0.0306 - val_loss: 0.2922\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 9s 960us/step - loss: 0.0312 - val_loss: 0.3106\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 9s 928us/step - loss: 0.0331 - val_loss: 0.3761\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0316 - val_loss: 0.2781\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0305 - val_loss: 0.2788\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0327 - val_loss: 0.2860\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 8s 790us/step - loss: 0.0283 - val_loss: 0.2877\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.0275 - val_loss: 0.2787\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 8s 813us/step - loss: 0.0284 - val_loss: 0.3140\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.0270 - val_loss: 0.2882\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0288 - val_loss: 0.2799\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.0308 - val_loss: 0.2899\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0284 - val_loss: 0.2917\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 9s 974us/step - loss: 0.0297 - val_loss: 0.2869\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0293 - val_loss: 0.3062\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 9s 974us/step - loss: 0.0306 - val_loss: 0.2817\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0287 - val_loss: 0.3759\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0297 - val_loss: 0.3010\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 10s 996us/step - loss: 0.0290 - val_loss: 0.2729\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.0261 - val_loss: 0.2851\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 8s 839us/step - loss: 0.0253 - val_loss: 0.2880\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0283 - val_loss: 0.3004\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 9s 895us/step - loss: 0.0346 - val_loss: 0.2711\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0301 - val_loss: 0.2846\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 8s 824us/step - loss: 0.0321 - val_loss: 0.2990\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 7s 729us/step - loss: 0.0288 - val_loss: 0.3015\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 7s 686us/step - loss: 0.0277 - val_loss: 0.2872\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 6s 640us/step - loss: 0.0297 - val_loss: 0.2836\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0283 - val_loss: 0.2909\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0614 - val_loss: 0.3006\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 7s 676us/step - loss: 0.0261 - val_loss: 0.2880\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 8s 864us/step - loss: 0.0287 - val_loss: 0.4402\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 7s 715us/step - loss: 0.0265 - val_loss: 0.2881\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.0270 - val_loss: 0.2956\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 7s 719us/step - loss: 0.0294 - val_loss: 0.3241\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 7s 696us/step - loss: 0.0249 - val_loss: 0.2890\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0249 - val_loss: 0.2772\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0258 - val_loss: 0.2895\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0250 - val_loss: 0.2931\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0237 - val_loss: 0.2888\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.0227 - val_loss: 0.2916\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0290 - val_loss: 0.3055\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0272 - val_loss: 0.2774\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0250 - val_loss: 0.2697\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 6s 599us/step - loss: 0.0252 - val_loss: 0.3183\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0243 - val_loss: 0.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0241 - val_loss: 0.2928\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 9s 890us/step - loss: 0.0217 - val_loss: 0.2834\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 9s 926us/step - loss: 0.0248 - val_loss: 0.2714\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 9s 918us/step - loss: 0.0246 - val_loss: 0.2818\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 9s 971us/step - loss: 0.0245 - val_loss: 0.2760\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 9s 971us/step - loss: 0.0236 - val_loss: 0.2881\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0238 - val_loss: 0.2845\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.0237 - val_loss: 0.2957\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0267 - val_loss: 0.2867\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0242 - val_loss: 0.2822\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 9s 908us/step - loss: 0.0219 - val_loss: 0.2745\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 10s 976us/step - loss: 0.0316 - val_loss: 0.2893\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 9s 940us/step - loss: 0.0247 - val_loss: 0.2940\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 9s 964us/step - loss: 0.0237 - val_loss: 0.2911\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0214 - val_loss: 0.2954\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0245 - val_loss: 0.3320\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0214 - val_loss: 0.2856\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0237 - val_loss: 0.2882\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0254 - val_loss: 0.2994\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 10s 995us/step - loss: 0.0226 - val_loss: 0.3000\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0242 - val_loss: 0.2865\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0237 - val_loss: 0.2910\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 9s 956us/step - loss: 0.0220 - val_loss: 0.2819\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 0.0253 - val_loss: 0.3027\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0242 - val_loss: 0.2898\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0223 - val_loss: 0.2771\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0206 - val_loss: 0.2642\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0209 - val_loss: 0.2813\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0234 - val_loss: 0.2976\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0222 - val_loss: 0.2716\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0225 - val_loss: 0.2995\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 10s 982us/step - loss: 0.0205 - val_loss: 0.2872\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0208 - val_loss: 0.2936\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0242 - val_loss: 0.2910\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 7s 683us/step - loss: 0.0220 - val_loss: 0.2747\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 652us/step - loss: 0.0205 - val_loss: 0.2776\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0216 - val_loss: 0.3495\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 7s 748us/step - loss: 0.0211 - val_loss: 0.2954\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0196 - val_loss: 0.2783\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 8s 775us/step - loss: 0.0212 - val_loss: 0.2921\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.0204 - val_loss: 0.2715\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0202 - val_loss: 0.2888\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0209 - val_loss: 0.2803\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.0256 - val_loss: 0.2785\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0217 - val_loss: 0.3037\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0200 - val_loss: 0.2802\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 8s 847us/step - loss: 0.0222 - val_loss: 0.2830\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0202 - val_loss: 0.2890\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0214 - val_loss: 0.2792\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0208 - val_loss: 0.2750\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0236 - val_loss: 0.2807\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.0214 - val_loss: 0.2822\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0206 - val_loss: 0.3176\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.0211 - val_loss: 0.2836\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0188 - val_loss: 0.2816\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 7s 768us/step - loss: 0.0223 - val_loss: 0.2772\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0200 - val_loss: 0.2939\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.0200 - val_loss: 0.2999\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0204 - val_loss: 0.2856\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0193 - val_loss: 0.2777\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.0196 - val_loss: 0.2708\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 6s 568us/step - loss: 0.0215 - val_loss: 0.3170\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0191 - val_loss: 0.3173\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 6s 570us/step - loss: 0.0192 - val_loss: 0.2836\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0192 - val_loss: 0.2770\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0195 - val_loss: 0.2764\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 9s 891us/step - loss: 0.0206 - val_loss: 0.3056\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0195 - val_loss: 0.3421\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0212 - val_loss: 0.2895\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.0184 - val_loss: 0.2782\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.0196 - val_loss: 0.2962\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0176 - val_loss: 0.2808\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0192 - val_loss: 0.2891\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0229 - val_loss: 0.2814\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0201 - val_loss: 0.2831\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 8s 825us/step - loss: 0.0198 - val_loss: 0.2883\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0179 - val_loss: 0.2940\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0193 - val_loss: 0.2943\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.0178 - val_loss: 0.2822\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 7s 669us/step - loss: 0.0198 - val_loss: 0.3247\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0184 - val_loss: 0.2937\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 8s 812us/step - loss: 0.0173 - val_loss: 0.2851\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0170 - val_loss: 0.2815\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 8s 782us/step - loss: 0.0183 - val_loss: 0.2805\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.0177 - val_loss: 0.2922\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.0187 - val_loss: 0.2871\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 7s 698us/step - loss: 0.0185 - val_loss: 0.2867\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 8s 787us/step - loss: 0.0177 - val_loss: 0.2803\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0184 - val_loss: 0.3042\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0183 - val_loss: 0.2983\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0185 - val_loss: 0.2808\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.0184 - val_loss: 0.2899\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.0179 - val_loss: 0.3051\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 9s 937us/step - loss: 0.0206 - val_loss: 0.2841\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 8s 836us/step - loss: 0.0178 - val_loss: 0.2882\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.0175 - val_loss: 0.2826\n"
     ]
    }
   ],
   "source": [
    "modele = Sequential()\n",
    "modele.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "modele.add(Dense(256,  kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.001)\n",
    "modele.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historye = modele.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resulte= pd.DataFrame(historyd.history)\n",
    "resulte.to_csv(\"history2e(uniform).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 15.9934 - val_loss: 4.1458\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 1.8212 - val_loss: 3.2474\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 1.0384 - val_loss: 2.2443\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.6994 - val_loss: 2.0361\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.5443 - val_loss: 1.9875\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.4534 - val_loss: 1.7690\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.3952 - val_loss: 1.6390\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.3391 - val_loss: 1.6707\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.3057 - val_loss: 1.5853\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2787 - val_loss: 1.4733\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2561 - val_loss: 1.4612\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2421 - val_loss: 1.4548\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2179 - val_loss: 1.3559\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2071 - val_loss: 1.5436\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1952 - val_loss: 1.2635\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1873 - val_loss: 1.3319\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1782 - val_loss: 1.2781\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1749 - val_loss: 1.2374\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1631 - val_loss: 1.2542\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1561 - val_loss: 1.1899\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 24s 2ms/step - loss: 0.1504 - val_loss: 1.1432\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1455 - val_loss: 1.1136\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1367 - val_loss: 1.0669\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1348 - val_loss: 1.1734\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1304 - val_loss: 1.0922\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 24s 2ms/step - loss: 0.1295 - val_loss: 1.1963\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1275 - val_loss: 1.1409\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1226 - val_loss: 1.0659\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1212 - val_loss: 1.0489\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1144 - val_loss: 1.0916\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1095 - val_loss: 1.0790\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1056 - val_loss: 1.0463\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.1034 - val_loss: 0.9995\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.1060 - val_loss: 0.9344\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0998 - val_loss: 1.0236\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0981 - val_loss: 0.9802\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0958 - val_loss: 1.0135\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0973 - val_loss: 0.9425\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0916 - val_loss: 0.9414\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0914 - val_loss: 0.9710\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0882 - val_loss: 0.9522\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0864 - val_loss: 0.9235\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0842 - val_loss: 0.9116\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0847 - val_loss: 0.8926\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0829 - val_loss: 0.8964\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0806 - val_loss: 0.8928\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0784 - val_loss: 0.8807\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0798 - val_loss: 0.8784\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0765 - val_loss: 0.8906\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0737 - val_loss: 0.8457\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0733 - val_loss: 0.8123\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0725 - val_loss: 0.8206\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0719 - val_loss: 0.8238\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0710 - val_loss: 0.8400\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0694 - val_loss: 0.8602\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0681 - val_loss: 0.8122\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0670 - val_loss: 0.8055\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0675 - val_loss: 0.7823\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0675 - val_loss: 0.8352\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0657 - val_loss: 0.7625\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0643 - val_loss: 0.7820\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0636 - val_loss: 0.7937\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0617 - val_loss: 0.7754\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0644 - val_loss: 0.7449\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0631 - val_loss: 0.7995\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0605 - val_loss: 0.7619\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0608 - val_loss: 0.7472\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0608 - val_loss: 0.7568\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0603 - val_loss: 0.8454\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0593 - val_loss: 0.6987\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0581 - val_loss: 0.7549\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0593 - val_loss: 0.7252\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0585 - val_loss: 0.6976\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0569 - val_loss: 0.7052\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0570 - val_loss: 0.7130\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0558 - val_loss: 0.7169\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0548 - val_loss: 0.6749\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0547 - val_loss: 0.6810\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0533 - val_loss: 0.6896\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0535 - val_loss: 0.7163\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0526 - val_loss: 0.6997\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0524 - val_loss: 0.6791\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0519 - val_loss: 0.6661\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0505 - val_loss: 0.6793\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0512 - val_loss: 0.6490\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0503 - val_loss: 0.6646\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0493 - val_loss: 0.6943\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0499 - val_loss: 0.6650\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0493 - val_loss: 0.6501\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0485 - val_loss: 0.6601\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0483 - val_loss: 0.6610\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0477 - val_loss: 0.6312\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0469 - val_loss: 0.6719\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0465 - val_loss: 0.6427\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0494 - val_loss: 0.6144\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0477 - val_loss: 0.6825\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0457 - val_loss: 0.6240\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0451 - val_loss: 0.6385\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0450 - val_loss: 0.6267\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0450 - val_loss: 0.5997\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0464 - val_loss: 0.6270\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0440 - val_loss: 0.6455\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0467 - val_loss: 0.6051\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0474 - val_loss: 0.6112\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0452 - val_loss: 0.6054\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0440 - val_loss: 0.6068\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0433 - val_loss: 0.6293\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0431 - val_loss: 0.5939\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0439 - val_loss: 0.6871\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0440 - val_loss: 0.5755\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0425 - val_loss: 0.6233\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0413 - val_loss: 0.5993\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0416 - val_loss: 0.5721\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0415 - val_loss: 0.5777\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0402 - val_loss: 0.5764\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0403 - val_loss: 0.5741\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0406 - val_loss: 0.6377\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0406 - val_loss: 0.5497\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0397 - val_loss: 0.6457\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0396 - val_loss: 0.5437\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0386 - val_loss: 0.5684\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0385 - val_loss: 0.5682\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0393 - val_loss: 0.5656\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0380 - val_loss: 0.5718\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0381 - val_loss: 0.5480\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0375 - val_loss: 0.5609\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0375 - val_loss: 0.5362\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0373 - val_loss: 0.5693\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0380 - val_loss: 0.5691\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0387 - val_loss: 0.5724\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0378 - val_loss: 0.5668\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0363 - val_loss: 0.5469\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0364 - val_loss: 0.5526\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0362 - val_loss: 0.5695\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0360 - val_loss: 0.5489\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0357 - val_loss: 0.5572\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0364 - val_loss: 0.5773\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0355 - val_loss: 0.5574\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0350 - val_loss: 0.5441\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0352 - val_loss: 0.5488\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0351 - val_loss: 0.5282\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0353 - val_loss: 0.5613\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0345 - val_loss: 0.5392\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0339 - val_loss: 0.5421\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0340 - val_loss: 0.5275\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.0339 - val_loss: 0.5290\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.0337 - val_loss: 0.5373\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0335 - val_loss: 0.5447\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0346 - val_loss: 0.5403\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 0.0352 - val_loss: 0.5206\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0332 - val_loss: 0.5323\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0342 - val_loss: 0.5301\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 5s 493us/step - loss: 0.0325 - val_loss: 0.5115\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0329 - val_loss: 0.5496\n",
      "Epoch 155/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 5s 509us/step - loss: 0.0326 - val_loss: 0.5384\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 5s 530us/step - loss: 0.0330 - val_loss: 0.5520\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0336 - val_loss: 0.5173\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 5s 523us/step - loss: 0.0334 - val_loss: 0.5173\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0322 - val_loss: 0.5260\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0320 - val_loss: 0.5238\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0321 - val_loss: 0.5078\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0321 - val_loss: 0.5589\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0319 - val_loss: 0.5220\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0325 - val_loss: 0.5606\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0328 - val_loss: 0.5297\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 566us/step - loss: 0.0304 - val_loss: 0.5216\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 5s 538us/step - loss: 0.0317 - val_loss: 0.5157\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0314 - val_loss: 0.5157\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0311 - val_loss: 0.5167\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0308 - val_loss: 0.5002\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0300 - val_loss: 0.5208\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0310 - val_loss: 0.4975\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0305 - val_loss: 0.4883\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0303 - val_loss: 0.5096\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 5s 513us/step - loss: 0.0297 - val_loss: 0.4828\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0298 - val_loss: 0.4953\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0297 - val_loss: 0.4967\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0293 - val_loss: 0.5181\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0295 - val_loss: 0.5005\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0298 - val_loss: 0.5079\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0290 - val_loss: 0.6056\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0293 - val_loss: 0.4865\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 5s 535us/step - loss: 0.0290 - val_loss: 0.5081\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0287 - val_loss: 0.4913\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0290 - val_loss: 0.5040\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0291 - val_loss: 0.5007\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.0285 - val_loss: 0.4864\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0286 - val_loss: 0.4960\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0290 - val_loss: 0.4865\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0285 - val_loss: 0.5164\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0281 - val_loss: 0.4719\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0284 - val_loss: 0.4926\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 6s 616us/step - loss: 0.0278 - val_loss: 0.4852\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0278 - val_loss: 0.4876\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0279 - val_loss: 0.4947\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.0275 - val_loss: 0.4809\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 5s 478us/step - loss: 0.0274 - val_loss: 0.5100\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0273 - val_loss: 0.4931\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0277 - val_loss: 0.5000\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 6s 636us/step - loss: 0.0273 - val_loss: 0.4846\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0272 - val_loss: 0.4767\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0275 - val_loss: 0.4913\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0271 - val_loss: 0.4694\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0266 - val_loss: 0.4813\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0264 - val_loss: 0.4871\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 6s 603us/step - loss: 0.0264 - val_loss: 0.4916\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0266 - val_loss: 0.4758\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0265 - val_loss: 0.5456\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 5s 495us/step - loss: 0.0264 - val_loss: 0.4869\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 5s 544us/step - loss: 0.0266 - val_loss: 0.4643\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0262 - val_loss: 0.4852\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0259 - val_loss: 0.4695\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0263 - val_loss: 0.4769\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.0258 - val_loss: 0.4810\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0260 - val_loss: 0.4938\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0260 - val_loss: 0.4871\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.0256 - val_loss: 0.4815\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0257 - val_loss: 0.4920\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0262 - val_loss: 0.4931\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 5s 479us/step - loss: 0.0261 - val_loss: 0.4642\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 5s 547us/step - loss: 0.0265 - val_loss: 0.4812\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0255 - val_loss: 0.4896\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0252 - val_loss: 0.4832\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0253 - val_loss: 0.4887\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.0251 - val_loss: 0.4791\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0250 - val_loss: 0.4699\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 6s 625us/step - loss: 0.0250 - val_loss: 0.4743\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0254 - val_loss: 0.4727\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0249 - val_loss: 0.4696\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0251 - val_loss: 0.4649\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0253 - val_loss: 0.4572\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0255 - val_loss: 0.4665\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0249 - val_loss: 0.4703\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0245 - val_loss: 0.4646\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0245 - val_loss: 0.4686\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0246 - val_loss: 0.4643\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.0244 - val_loss: 0.4799\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0240 - val_loss: 0.4682\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 5s 499us/step - loss: 0.0237 - val_loss: 0.4637\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0241 - val_loss: 0.4733\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0235 - val_loss: 0.4721\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0239 - val_loss: 0.4719\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0243 - val_loss: 0.4746\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0241 - val_loss: 0.4573\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0241 - val_loss: 0.4723\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0236 - val_loss: 0.4768\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0236 - val_loss: 0.4541\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0237 - val_loss: 0.4553\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 635us/step - loss: 0.0237 - val_loss: 0.4593\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0237 - val_loss: 0.4644\n"
     ]
    }
   ],
   "source": [
    "modelhe = Sequential()\n",
    "modelhe.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu')) #uniform\n",
    "modelhe.add(Dense(256,  kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.00008)\n",
    "modelhe.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historye = modelhe.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resulte= pd.DataFrame(historyd.history)\n",
    "resulte.to_csv(\"history2he(he-uniform).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) ¿Qué es lo que sucede con la red más profunda? ¿El modelo logra convergencia en su entrenamiento? Modifique aspectos estructurales (funciones de activación, inicializadores, regularización, momentum, variación de tasa de aprendizaje, entre otros) de la red profunda de 6 capas definida anteriormente (no modifique la profundidad ni el número de neuronas) para lograr un error cuadrático medio (mse) similar o menor al de una red no profunda, como la definida en b) en esta sección, sobre el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 8s 783us/step - loss: 15.6048 - val_loss: 13.4087\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 6s 571us/step - loss: 7.3248 - val_loss: 2.0294\n",
      "Epoch 3/250\n",
      "1056/9745 [==>...........................] - ETA: 4s - loss: 1.6984"
     ]
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyf = modelf.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resultf= pd.DataFrame(historyd.history)\n",
    "resultf.to_csv(\"history2d.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
