{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "#print(datos)\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste Training Set\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Ajuste Validation Set\n",
    "scaler2 = StandardScaler().fit(df_val)\n",
    "X_val_scaled = pd.DataFrame(scaler2.transform(df_val),columns=df_val.columns)\n",
    "y_val_scaled = X_val_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Ajuste Validation Set\n",
    "scaler3 = StandardScaler().fit(df_test)\n",
    "X_test_scaled = pd.DataFrame(scaler3.transform(df_test),columns=df_test.columns)\n",
    "y_test_scaled = X_test_scaled.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "yTest = df_test.pop('Eat').values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Para el mismo problema definido anteriormente (sección 1) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (loss) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de gradients (link) en el backend de Keras. Deberá generar un histograma para todos los pesos de cada capa antes y despues del entrenamiento con 250 epochs. Comente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradientss = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradientss = [gradient/len(y_train) for gradient in evaluated_gradientss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/ipykernel_launcher.py:5: MatplotlibDeprecationWarning: scipy.stats.norm.pdf\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "mu, sigma = 0, 1\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(evaluated_gradientss, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "plt.xlabel('Weights')\n",
    "plt.ylabel('Gradient')\n",
    "plt.title(r'$\\mathrm{Histogram}$')\n",
    "plt.axis([-0.9, 0.1, 0, 10])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 6s 594us/step - loss: 1.3981 - val_loss: 1.1174\n",
      "\n",
      "Testing loss: 1.0213046483710793\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.6079 - val_loss: 0.7701\n",
      "\n",
      "Testing loss: 1.0680099724133483\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 4s 452us/step - loss: 0.5140 - val_loss: 1.2703\n",
      "\n",
      "Testing loss: 0.5319029072732968\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.4367 - val_loss: 1.4253\n",
      "\n",
      "Testing loss: 0.42704916593477676\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.3747 - val_loss: 0.7951\n",
      "\n",
      "Testing loss: 0.6872603961151548\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.3240 - val_loss: 0.8578\n",
      "\n",
      "Testing loss: 0.6725279192822753\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 4s 422us/step - loss: 0.2902 - val_loss: 1.1685\n",
      "\n",
      "Testing loss: 0.40661746382737773\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.2619 - val_loss: 1.2700\n",
      "\n",
      "Testing loss: 0.5176210641053841\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.2302 - val_loss: 0.8728\n",
      "\n",
      "Testing loss: 0.5701929297562647\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.2016 - val_loss: 0.6489\n",
      "\n",
      "Testing loss: 0.7543570784745668\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.1811 - val_loss: 0.7321\n",
      "\n",
      "Testing loss: 0.5939809734037679\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 5s 506us/step - loss: 0.1648 - val_loss: 0.7069\n",
      "\n",
      "Testing loss: 0.6147522990288772\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.1498 - val_loss: 0.8421\n",
      "\n",
      "Testing loss: 0.6312406303320549\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 5s 537us/step - loss: 0.1350 - val_loss: 0.8235\n",
      "\n",
      "Testing loss: 0.5547157930432649\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 5s 465us/step - loss: 0.1221 - val_loss: 0.7493\n",
      "\n",
      "Testing loss: 0.6755109398052934\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.1135 - val_loss: 0.6321\n",
      "\n",
      "Testing loss: 0.7669186184139197\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.1071 - val_loss: 0.9091\n",
      "\n",
      "Testing loss: 0.6151965593259551\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0990 - val_loss: 0.5715\n",
      "\n",
      "Testing loss: 0.7252952894031586\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: 0.0928 - val_loss: 0.7311\n",
      "\n",
      "Testing loss: 0.7445123149177605\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0889 - val_loss: 0.9600\n",
      "\n",
      "Testing loss: 0.6839984167255629\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.0840 - val_loss: 0.5946\n",
      "\n",
      "Testing loss: 0.740855094077502\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.0815 - val_loss: 0.5980\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history2 = model2.fit(X_train_scaled, \n",
    "                      y_train, epochs=250, \n",
    "                      verbose=1, \n",
    "                      validation_data=(X_val_scaled, y_val),\n",
    "                      callbacks=[TestCallback((X_test_scaled.values, yTest))])\n",
    "#history2=pd.read_csv(\"history2b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/ipykernel_launcher.py:13: MatplotlibDeprecationWarning: scipy.stats.norm.pdf\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#history2=pd.read_csv(\"history2b.csv\")\n",
    "loss = keras.losses.mean_squared_error(model2.output,y_train_scaled)\n",
    "listOfVariableTensors = model2.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients2 = sess.run(gradients,feed_dict={model2.input:X_train_scaled.values})\n",
    "evaluated_gradients2 = [gradient/len(y_train) for gradient in evaluated_gradients2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "mu, sigma = 0, 1\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(evaluated_gradients2, 50, normed=1, facecolor='blue', alpha=0.75)\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "plt.xlabel('Evaluated Gradients')\n",
    "plt.ylabel('')\n",
    "plt.title(r'$\\mathrm{Histogram}$')\n",
    "#plt.axis([-0.9, 0.1, 0, 10])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento pero ahora entrenando una red mucho más profunda de 6 capas, 5 capas escondidas y 1 de salida. Utilice el inicializador de pesos *uniform* el cual inicializa mediante una distribución uniforme entre $-1/\\sqrt{N}$ y $1/\\sqrt{N}$ para cada capa, con $N$ el número de neuronas de la capa anterior. Por simplicidad visualice las 3-4 primeras capas de la red. Comente si observa el efecto del *gradiente desvaneciente* antes y/o después de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = Sequential()\n",
    "modelc.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelc.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "#historyc = model2.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "#result= pd.DataFrame(historyc.history)\n",
    "#result.to_csv(\"history2c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc2 = Sequential()\n",
    "modelc2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelc2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelc2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyc=pd.read_csv(\"history2c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/ipykernel_launcher.py:12: MatplotlibDeprecationWarning: scipy.stats.norm.pdf\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = keras.losses.mean_squared_error(modelc.output,y_train_scaled)\n",
    "listOfVariableTensors = modelc.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradientss = sess.run(gradients,feed_dict={modelc.input:X_train_scaled.values})\n",
    "evaluated_gradientss = [gradient/len(y_train) for gradient in evaluated_gradientss]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "mu, sigma = 0, 1\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(evaluated_gradientss, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "plt.xlabel('Evaluated Gradients')\n",
    "plt.ylabel('')\n",
    "plt.title(r'$\\mathrm{Histogram}$')\n",
    "plt.axis([-0.6, 0.4, 0, 20])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/ipykernel_launcher.py:12: MatplotlibDeprecationWarning: scipy.stats.norm.pdf\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = keras.losses.mean_squared_error(modelc2.output,y_train_scaled)\n",
    "listOfVariableTensors = modelc2.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradientss = sess.run(gradients,feed_dict={modelc2.input:X_train_scaled.values})\n",
    "evaluated_gradientss = [gradient/len(y_train) for gradient in evaluated_gradientss]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "mu, sigma = 0, 1\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(evaluated_gradientss, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "plt.xlabel('Evaluated Gradients')\n",
    "plt.ylabel('')\n",
    "plt.title(r'$\\mathrm{Histogram}$')\n",
    "plt.axis([-0.4, 0.4, 0, 10])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">d) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento, pero ahora entrenando la red profunda con el inicializador de Glorot [[1]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/(N_{in}+N_{out})}$  y $\\sqrt{6/(N_{in}+N_{out})}$ . Por simplicidad visualice las 3-4 primeras capas de la red. Comente si el efecto del *gradiente desvaneciente* se amortigua antes y/o después de entrenar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 14.9281 - val_loss: 13.8282\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 7s 750us/step - loss: 6.7519 - val_loss: 2.2967\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 7s 742us/step - loss: 2.1486 - val_loss: 1.4162\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 7s 750us/step - loss: 1.2964 - val_loss: 1.6880\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 7s 756us/step - loss: 1.0518 - val_loss: 4.5584\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 7s 740us/step - loss: 0.9103 - val_loss: 0.6060\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 7s 731us/step - loss: 0.7514 - val_loss: 0.6716\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.7099 - val_loss: 0.9440\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.5713 - val_loss: 0.8302\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 8s 778us/step - loss: 0.5167 - val_loss: 1.1006\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.4584 - val_loss: 1.5044\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 9s 890us/step - loss: 0.4435 - val_loss: 1.2956\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 8s 796us/step - loss: 0.3794 - val_loss: 1.6159\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 7s 742us/step - loss: 0.3505 - val_loss: 0.4889\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.3224 - val_loss: 0.9592\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 7s 722us/step - loss: 0.3255 - val_loss: 0.3005\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.2565 - val_loss: 1.5791\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 8s 770us/step - loss: 0.2720 - val_loss: 1.0645\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 8s 810us/step - loss: 0.2246 - val_loss: 0.2721\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.2487 - val_loss: 0.2581\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 7s 686us/step - loss: 0.2110 - val_loss: 0.7677\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 7s 737us/step - loss: 0.1836 - val_loss: 1.8782\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.1951 - val_loss: 0.5504\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 7s 680us/step - loss: 0.1576 - val_loss: 0.8232\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 7s 715us/step - loss: 0.1742 - val_loss: 0.3973\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 7s 757us/step - loss: 0.1778 - val_loss: 1.2262\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 8s 846us/step - loss: 0.1331 - val_loss: 0.1630\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 7s 763us/step - loss: 0.1628 - val_loss: 0.4737\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.1189 - val_loss: 0.7734\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 7s 672us/step - loss: 0.1466 - val_loss: 1.4394\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.1419 - val_loss: 0.3829\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.1095 - val_loss: 0.4324\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 7s 707us/step - loss: 0.1145 - val_loss: 0.1259\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.1176 - val_loss: 0.3292\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 8s 772us/step - loss: 0.1007 - val_loss: 0.9655\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.1038 - val_loss: 0.3824\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 7s 761us/step - loss: 0.1061 - val_loss: 0.9348\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 7s 754us/step - loss: 0.1009 - val_loss: 1.0208\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0947 - val_loss: 0.8821\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 8s 843us/step - loss: 0.0919 - val_loss: 0.8160\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.0899 - val_loss: 0.9434\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 8s 790us/step - loss: 0.0907 - val_loss: 0.9584\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0899 - val_loss: 0.5758\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 7s 740us/step - loss: 0.0821 - val_loss: 0.4818\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 7s 715us/step - loss: 0.0819 - val_loss: 0.3247\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 7s 761us/step - loss: 0.0753 - val_loss: 0.5010\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 7s 731us/step - loss: 0.0727 - val_loss: 1.0400\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 7s 764us/step - loss: 0.0820 - val_loss: 0.2500\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 7s 748us/step - loss: 0.0761 - val_loss: 0.8401\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 8s 776us/step - loss: 0.0769 - val_loss: 0.4142\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0733 - val_loss: 0.4943\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0736 - val_loss: 0.3432\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 0.0722 - val_loss: 0.9100\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0761 - val_loss: 0.6596\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 7s 724us/step - loss: 0.0613 - val_loss: 0.5508\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.0685 - val_loss: 0.5221\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 7s 683us/step - loss: 0.0684 - val_loss: 0.5168\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.0619 - val_loss: 0.4338\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.0609 - val_loss: 0.3156\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 7s 752us/step - loss: 0.0710 - val_loss: 0.2252\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0632 - val_loss: 0.5474\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0660 - val_loss: 0.4331\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0600 - val_loss: 0.1881\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 7s 762us/step - loss: 0.0635 - val_loss: 1.0319\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0517 - val_loss: 0.4310\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.0601 - val_loss: 0.9362\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 7s 750us/step - loss: 0.0615 - val_loss: 0.6207\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 8s 798us/step - loss: 0.0556 - val_loss: 0.4684\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 7s 744us/step - loss: 0.0543 - val_loss: 0.4388\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 7s 732us/step - loss: 0.0519 - val_loss: 0.6535\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.0563 - val_loss: 0.5024\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 8s 859us/step - loss: 0.0529 - val_loss: 0.6930\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 8s 787us/step - loss: 0.0568 - val_loss: 0.7226\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 8s 801us/step - loss: 0.0551 - val_loss: 0.4403\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.0464 - val_loss: 0.8543\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 9s 953us/step - loss: 0.0507 - val_loss: 0.4855\n",
      "Epoch 77/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0533 - val_loss: 0.6487\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 9s 956us/step - loss: 0.0512 - val_loss: 0.5380\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.0469 - val_loss: 0.8048\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 7s 742us/step - loss: 0.0519 - val_loss: 0.3476\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.0550 - val_loss: 0.7506\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.0450 - val_loss: 0.5468\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 7s 718us/step - loss: 0.0456 - val_loss: 0.5941\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 8s 863us/step - loss: 0.0433 - val_loss: 0.6253\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 7s 758us/step - loss: 0.0519 - val_loss: 0.4212\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 9s 968us/step - loss: 0.0451 - val_loss: 0.5738\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.0475 - val_loss: 0.3763\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.0505 - val_loss: 1.0599\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 692us/step - loss: 0.0472 - val_loss: 0.6043\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0400 - val_loss: 0.6175\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.0423 - val_loss: 0.5116\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.0429 - val_loss: 0.3986\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0458 - val_loss: 0.4909\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 8s 811us/step - loss: 0.0445 - val_loss: 0.4934\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 8s 832us/step - loss: 0.0399 - val_loss: 0.7158\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 8s 856us/step - loss: 0.0386 - val_loss: 0.7153\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 9s 894us/step - loss: 0.0476 - val_loss: 0.8804\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.0450 - val_loss: 0.5002\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 8s 813us/step - loss: 0.0375 - val_loss: 0.3368\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 8s 785us/step - loss: 0.0426 - val_loss: 0.5901\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 8s 839us/step - loss: 0.0363 - val_loss: 0.5172\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 8s 772us/step - loss: 0.0377 - val_loss: 0.6461\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 8s 856us/step - loss: 0.0418 - val_loss: 0.6202\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0470 - val_loss: 0.6843\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0398 - val_loss: 0.3672\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 7s 713us/step - loss: 0.0385 - val_loss: 0.6956\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0409 - val_loss: 0.5256\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.0378 - val_loss: 0.5825\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 8s 816us/step - loss: 0.0390 - val_loss: 0.6575\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 8s 804us/step - loss: 0.0357 - val_loss: 0.5728\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 7s 722us/step - loss: 0.0492 - val_loss: 0.4325\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 6s 660us/step - loss: 0.0352 - val_loss: 0.7758\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 6s 646us/step - loss: 0.0341 - val_loss: 0.5645\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0366 - val_loss: 0.7271\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0468 - val_loss: 0.5770\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 6s 649us/step - loss: 0.0337 - val_loss: 0.3373\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0349 - val_loss: 0.4552\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0338 - val_loss: 0.5444\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0342 - val_loss: 0.5072\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 7s 680us/step - loss: 0.0362 - val_loss: 0.5198\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 7s 716us/step - loss: 0.0341 - val_loss: 0.5384\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 0.0364 - val_loss: 0.4206\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 6s 660us/step - loss: 0.0312 - val_loss: 0.6072\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 6s 661us/step - loss: 0.0344 - val_loss: 0.4911\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 7s 670us/step - loss: 0.0316 - val_loss: 0.4427\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0354 - val_loss: 0.6065\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0323 - val_loss: 0.6248\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 7s 740us/step - loss: 0.0344 - val_loss: 0.3384\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 7s 736us/step - loss: 0.0315 - val_loss: 0.5035\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.0316 - val_loss: 0.4117\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 7s 680us/step - loss: 0.0311 - val_loss: 0.3985\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.0356 - val_loss: 0.4393\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.0361 - val_loss: 0.6318\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 7s 679us/step - loss: 0.0334 - val_loss: 0.6795\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 0.0361 - val_loss: 0.7378\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.0310 - val_loss: 0.3446\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 7s 692us/step - loss: 0.0285 - val_loss: 0.2119\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0320 - val_loss: 0.6023\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.0296 - val_loss: 0.5065\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0282 - val_loss: 0.5038\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 6s 665us/step - loss: 0.0310 - val_loss: 0.9616\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 7s 696us/step - loss: 0.0290 - val_loss: 0.8316\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 7s 718us/step - loss: 0.0280 - val_loss: 0.7307\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.0307 - val_loss: 0.7881\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 7s 679us/step - loss: 0.0320 - val_loss: 0.5028\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0297 - val_loss: 0.6209\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 0.0267 - val_loss: 0.5343\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 7s 715us/step - loss: 0.0298 - val_loss: 0.4680\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 7s 677us/step - loss: 0.0296 - val_loss: 0.4490\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.0282 - val_loss: 0.6972\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 7s 745us/step - loss: 0.0271 - val_loss: 0.6731\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 7s 679us/step - loss: 0.0331 - val_loss: 0.6120\n",
      "Epoch 153/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 7s 718us/step - loss: 0.0287 - val_loss: 0.6201\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0285 - val_loss: 0.7060\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0291 - val_loss: 0.4799\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0271 - val_loss: 0.4840\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.0286 - val_loss: 0.5323\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 6s 599us/step - loss: 0.0261 - val_loss: 0.1698\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0301 - val_loss: 0.6212\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 6s 604us/step - loss: 0.0300 - val_loss: 0.3687\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 6s 615us/step - loss: 0.0312 - val_loss: 0.5639\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 6s 618us/step - loss: 0.0244 - val_loss: 0.5015\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0333 - val_loss: 0.5157\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0263 - val_loss: 0.4994\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0267 - val_loss: 0.6372\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0258 - val_loss: 0.4773\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0274 - val_loss: 0.8073\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.0267 - val_loss: 0.6136\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 6s 616us/step - loss: 0.0294 - val_loss: 0.4974\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 6s 646us/step - loss: 0.0281 - val_loss: 0.6024\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.0254 - val_loss: 0.4276\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0280 - val_loss: 0.6708\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0260 - val_loss: 0.6556\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0268 - val_loss: 0.5871\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 6s 578us/step - loss: 0.0286 - val_loss: 0.8378\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0267 - val_loss: 0.4292\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0255 - val_loss: 0.5389\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 7s 741us/step - loss: 0.0254 - val_loss: 0.5019\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 6s 634us/step - loss: 0.0277 - val_loss: 0.8711\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0277 - val_loss: 0.4596\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0240 - val_loss: 0.9060\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0251 - val_loss: 0.4529\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 6s 608us/step - loss: 0.0276 - val_loss: 0.6131\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0246 - val_loss: 0.6753\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0256 - val_loss: 0.5768\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0253 - val_loss: 1.0696\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.0263 - val_loss: 0.6286\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 6s 660us/step - loss: 0.0233 - val_loss: 0.6207\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0245 - val_loss: 0.5537\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0277 - val_loss: 0.5063\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0257 - val_loss: 0.6195\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 7s 750us/step - loss: 0.0236 - val_loss: 0.5078\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 8s 832us/step - loss: 0.0243 - val_loss: 0.7260\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 8s 822us/step - loss: 0.0247 - val_loss: 0.6464\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 8s 798us/step - loss: 0.0266 - val_loss: 0.4340\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 9s 900us/step - loss: 0.0245 - val_loss: 0.5808\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.0239 - val_loss: 0.2631\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0249 - val_loss: 0.3789\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0255 - val_loss: 0.2399\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0262 - val_loss: 0.4818\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 8s 791us/step - loss: 0.0247 - val_loss: 0.7111\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0229 - val_loss: 0.5240\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0238 - val_loss: 0.4932\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.0263 - val_loss: 0.7096\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0219 - val_loss: 0.4669\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 9s 939us/step - loss: 0.0236 - val_loss: 0.5719\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 7s 727us/step - loss: 0.0232 - val_loss: 0.5343\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 8s 778us/step - loss: 0.0258 - val_loss: 0.4499\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 8s 821us/step - loss: 0.0225 - val_loss: 0.5355\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.0240 - val_loss: 0.6836\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.0220 - val_loss: 0.5466\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 7s 714us/step - loss: 0.0229 - val_loss: 0.4318\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 6s 663us/step - loss: 0.0218 - val_loss: 0.5232\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0233 - val_loss: 0.6963\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 9s 920us/step - loss: 0.0227 - val_loss: 0.6765\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 8s 811us/step - loss: 0.0217 - val_loss: 0.4754\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 8s 850us/step - loss: 0.0233 - val_loss: 0.7073\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 8s 859us/step - loss: 0.0223 - val_loss: 0.5013\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.0211 - val_loss: 0.6455\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0226 - val_loss: 0.5501\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0220 - val_loss: 0.5252\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0228 - val_loss: 0.5161\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 7s 719us/step - loss: 0.0225 - val_loss: 0.5291\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 6s 654us/step - loss: 0.0216 - val_loss: 0.5545\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0215 - val_loss: 0.4292\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 7s 735us/step - loss: 0.0232 - val_loss: 0.6901\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 0.0226 - val_loss: 0.6387\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.0222 - val_loss: 0.3649\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 6s 649us/step - loss: 0.0219 - val_loss: 0.4926\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.0208 - val_loss: 0.5170\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0202 - val_loss: 0.5930\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0207 - val_loss: 0.5910\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 6s 660us/step - loss: 0.0230 - val_loss: 0.6330\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 7s 670us/step - loss: 0.0207 - val_loss: 0.4627\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.0216 - val_loss: 0.5004\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.0250 - val_loss: 0.6280\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 9s 885us/step - loss: 0.0198 - val_loss: 0.6629\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 8s 867us/step - loss: 0.0205 - val_loss: 0.7622\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0219 - val_loss: 0.5212\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0202 - val_loss: 0.6032\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0220 - val_loss: 0.3384\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 7s 731us/step - loss: 0.0214 - val_loss: 0.5660\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 9s 939us/step - loss: 0.0210 - val_loss: 0.5827\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 9s 940us/step - loss: 0.0220 - val_loss: 0.4819\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 8s 796us/step - loss: 0.0227 - val_loss: 0.5843\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0228 - val_loss: 0.5357\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0204 - val_loss: 0.5408\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.0205 - val_loss: 0.5432\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 7s 742us/step - loss: 0.0209 - val_loss: 0.4050\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 7s 705us/step - loss: 0.0225 - val_loss: 0.6130\n"
     ]
    }
   ],
   "source": [
    "modeld = Sequential()\n",
    "modeld.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeld.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modeld.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyd = modeld.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resultd= pd.DataFrame(historyd.history)\n",
    "resultd.to_csv(\"history2d.csv\")\n",
    "\n",
    "loss = keras.losses.mean_squared_error(modeld.output,y_train_scaled)\n",
    "listOfVariableTensors = modeld.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradientsd = sess.run(gradients,feed_dict={modeld.input:X_train_scaled.values})\n",
    "evaluated_gradientsd = [gradient/len(y_train) for gradient in evaluated_gradientsd]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/ipykernel_launcher.py:5: MatplotlibDeprecationWarning: scipy.stats.norm.pdf\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "mu, sigma = 0, 1\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(evaluated_gradientsd, 50, normed=1, facecolor='blue', alpha=0.75)\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "plt.xlabel('Evaluated Gradients')\n",
    "plt.ylabel('')\n",
    "plt.title(r'$\\mathrm{Histogram}$')\n",
    "#plt.axis([-0.1, 0.7, 0, 10])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "/home/portilla/.conda/envs/redesneuronales/lib/python3.6/site-packages/ipykernel_launcher.py:22: MatplotlibDeprecationWarning: scipy.stats.norm.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEcCAYAAADXxE9kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xl4U2XC/vFv0ja0QBvSpoAtuEBhsEKRRQFlqVJRFhUZZFxgxB8jo8Iw6MgALuCog1VAFi3CoLjAMAr6isuISFkHGQRFZmSRTdCR5e1KN1rSJOf3RyFDbaGhS0rOe3+uqxfkyXNO7qbJndPTkxOLYRgGIiIS9Kz1HUBERGqHCl1ExCRU6CIiJqFCFxExCRW6iIhJqNBFRExChS4iYhIqdBERk1Chy0Vv5MiRpKSk1HcMkYueRe8UlfoycuRIfvrpJ9LT0ytcZ7FYWLx4McOHDycvLw+v14vD4fBrvSkpKbRo0YI333yzlhOLXNxC6zuASFXsdnt9Rzgnl8uFzWar7xgigHa5SBD4+S6XTZs2cf311xMZGUlkZCQdO3Zk1apVvrlr1qzhrbfewmKxYLFYWL9+PaWlpUyaNIn4+HhsNhuJiYksXbq03O0UFxczevRo7HY7DoeDhx9+mMmTJ5OQkOCbk5yczKhRo3jqqae45JJLiI+PB2D16tUkJycTHR2N3W6nT58+bN26tdz6zyz75JNP0rRpU5o0acITTzyB1+vlmWeeoVmzZsTGxvLEE0/U1V0pJqdCl6Di8Xi47bbb6NatG9u3b2f79u08/fTTNGzYEIA5c+bQq1cvhg0bxrFjxzh27BjXXXcdjz/+OAsXLmT27Nns3LmT4cOHM3z4cNasWeNb98SJE/nwww9ZvHgxW7ZswW63M2/evAoZli1bRmZmJmvWrGHt2rUAFBYWMmbMGLZs2cLmzZtp06YNt9xyC9nZ2eWWfe+99ygtLWXTpk289NJLTJs2jUGDBlFYWMg//vEPZsyYwbRp01i5cmUd3otiWoZIPbnvvvuMkJAQo1GjRhW+AGPx4sW+eX379jUMwzBycnIMwFi3bt0519u3b1/jvvvu810uKioybDabkZaWVm7e4MGDjRtuuMEwDMMoLCw0bDab8dprr5Wb061bN6N169a+y3369DHatGljeDye835vHo/HaNKkibFkyZJyy3bs2LHcvMTERKN9+/blxpKSkow//OEP512/SGW0hS71qlu3buzYsaPC17k4HA5+85vfcPPNN9O/f39SU1PZu3fveW/jwIEDuFwuevfuXW68T58+7Nq1q9yc7t27l5vTo0ePCuvr0qULVmv5p86hQ4cYMWIECQkJREVFERUVRV5eHj/88EO5eR07dix3uXnz5iQlJVUYy8jIOO/3JFIZFbrUq4iICBISEip8nc/ChQv5+uuvuemmm9iwYQPt27dnwYIFVd6WxWIpd9kwjApjP79cmUaNGlUYGzRoED/++CNpaWls2bKFHTt20LRpU1wuV7l5YWFhFW6vsjGv11tlDpGfU6FLUGrfvj2PPvooK1euZNSoUfzlL3/xXWez2fB4PL7LCQkJNGjQgA0bNpRbx8aNG7nqqqt8c2w2G//85z/LzdmyZUuVWbKzs9m9ezeTJk3i5ptvJjExkfDwcG1lS8DpsEUJKgcOHGDhwoXceuuttGzZkqNHj/KPf/yDzp07++ZcccUVrFu3joMHD2K327Hb7YwbN46nnnqK2NhYrr76apYvX86HH37I6tWrgbKt7t/+9rc8+eSTNGvWjLZt2/LWW2+xZ88eYmNjz5vJ4XAQGxvLwoULad26NdnZ2fzxj38kIiKiTu8LkZ9ToUtQadSoEfv37+euu+4iMzOTmJgYBg4cyIwZM3xz/vCHP/Dtt9/SsWNHioqKWLduHX/+85+xWq2MHz+ezMxMEhISWLJkCX379vUt98ILL1BSUsI999yD1Wrlnnvu8R0GeT5Wq5Xly5czbtw4kpKSuOyyy5g2bRoTJ06ss/tBpDJ6p6jIedx44404HA7ef//9+o4iUiVtoYuc9u2337J9+3Z69OiBy+Vi8eLFrFu3jk8//bS+o4n4RYUucprFYuHVV19l3LhxeL1e2rVrxwcffED//v3rO5qIX7TLRUTEJHTYooiISajQRURMIuD70I8ePVrustPpJCsrK9AxqkVZ604w5Q2mrBBceYMpKwQub1xcnF/ztIUuImISKnQREZNQoYuImIQKXUTEJFToIiImoUIXETEJFbqIiEmo0EVETEKFLiJiElW+U9TlcjF16lTcbjcej4fu3bszbNgw0tLS2L17Nw0bNgRgzJgxXH755XWdt5yhnwzlvUHvBfQ2RUSgrH9sYTaW3ry0vqP4VFnoYWFhTJ06lfDwcNxuN1OmTOHqq68GYMSIERU+JV1EROpHlbtcLBYL4eHhAHg8Hjwej1+fjC4iIoHl1z50r9fLhAkT+M1vfkOHDh1o06YNAH/729947LHHePPNNyktLa3ToCIicn4X9AEXRUVFzJgxg/vvv5/IyEiaNGmC2+1mwYIFNG/enKFDh1ZYJj09nfT0dABSU1NxuVzlrg8NDcXtdlcrfL+/9uPzez+v1rLVUZOsgRZMWSG48gZTVgiuvMGUtd9f+2GxWFh1z6o6vy2bzebXvAs6fW6jRo1ITExkx44d3HbbbUDZPvYbbriBjz/+uNJlUlJSSElJ8V3++akma3L6SVepK6Cn2gymU3sGU1YIrrzBlBWCK28wZXWVurCF2YLr9Ln5+fkUFRUBZUe8fPvtt8THx5ObmwuAYRhs27aNli1b1iCuiIjUVJVb6Lm5uaSlpeH1ejEMgx49etClSxf+9Kc/kZ+fD8Bll13G6NGj6zysiIicW5WFftlll/Hiiy9WGJ86dWqdBBIRCSYX0/th9E5RERGTUKGLiJiECl1ExCRU6CIiJqFCFxExCRW6iIhJqNBFRExChS4iYhIqdBERk1Chi4iYhApdRMQkVOgiIiahQhcRMQkVuoiISajQRURMQoUuImISKnQREZNQoYuImIQKXUTEJFToIiImUeWHRLtcLqZOnYrb7cbj8dC9e3eGDRtGRkYGs2fPprCwkCuuuILf/e53hIZWuToREakjVTZwWFgYU6dOJTw8HLfbzZQpU7j66qv55JNPGDhwINdffz1/+ctfWLt2Lf369QtEZhERqUSVu1wsFgvh4eEAeDwePB4PFouFXbt20b17dwCSk5PZtm1b3SYVEZHz8msfidfrZeLEiRw/fpybb76ZZs2a0bBhQ0JCQgCIjo4mJyenToOKiMj5+VXoVquV6dOnU1RUxIwZMzhy5IjfN5Cenk56ejoAqampOJ3O8gFCQyuM+csWZqv2stVRk6yBFkxZIbjyBlNWCK68wZTVFmbDYrEEvIfO54L+itmoUSMSExPZv38/J0+exOPxEBISQk5ODtHR0ZUuk5KSQkpKiu9yVlZWueudTmeFMX+5Sl3VXrY6apI10IIpKwRX3mDKCsGVN5iyukpd2MJsAemhuLg4v+ZVuQ89Pz+foqIioOyIl2+//Zb4+HiuuuoqtmzZAsD69evp2rVrDeKKiEhNVbmFnpubS1paGl6vF8Mw6NGjB126dKFFixbMnj2bd955hyuuuIIbb7wxEHlFROQcqiz0yy67jBdffLHCeLNmzXj++efrJJSIiFw4vVNURMQkVOgiIiahQhcRMQkVuoiISajQRURMQoUuImISKnQREZNQoYuImIQKXUTEJFToIiImoUIXETEJFbqIiEmo0EVETEKFLiJiEip0ERGTUKGLiJiECl1ExCRU6CIiJqFCFxExCRW6iIhJVPkh0VlZWaSlpXHixAksFgspKSkMGDCAZcuWsWbNGqKiogC4++676dy5c50HFhGRylVZ6CEhIYwYMYJWrVpRXFzMpEmTSEpKAmDgwIHcdtttdR5SRESqVmWhOxwOHA4HABEREcTHx5OTk1PnwURE5MJc0D70jIwMDh06REJCAgCrVq3iscceY968eRQWFtZJQBER8Y/FMAzDn4klJSVMnTqVIUOG0K1bN06cOOHbf/7uu++Sm5vLww8/XGG59PR00tPTAUhNTcXlcpW7PjQ0FLfbXa3w/f7aj8/v/bxay1ZHTbIGWjBlheDKG0xZIbjyBlPWfn/th8ViwTCMOu8hm83m17wqd7kAuN1uZs6cSa9evejWrRsATZo08V3ft29fXnjhhUqXTUlJISUlxXc5Kyur3PVOp7PCmL9cpa5qL1sdNckaaMGUFYIrbzBlheDKG0xZXaUubGG2gPRQXFycX/Oq3OViGAbz588nPj6eQYMG+cZzc3N9/9+6dSstW7asRkwREaktVW6h7927l40bN3LppZcyYcIEoOwQxS+++ILDhw9jsViIjY1l9OjRdR5WRETOrcpCb9euHcuWLaswrmPORUQuLnqnqIiISajQRURMQoUuImISKnQREZNQoYuImIQKXUTEJFToIiImoUIXETEJFbqIiEmo0EVETEKFLiJiEip0ERGTUKGLiJiECl1ExCRU6CIiJqFCFxGpoaGfDK3vCIAKXUTENFToIiImoUIXETEJFbqIiEmo0EVETCK0qglZWVmkpaVx4sQJLBYLKSkpDBgwgMLCQmbNmkVmZiaxsbE88sgjNG7cOBCZRUSkElUWekhICCNGjKBVq1YUFxczadIkkpKSWL9+PR06dGDw4MGsWLGCFStWMHz48EBkFhGRSlS5y8XhcNCqVSsAIiIiiI+PJycnh23bttGnTx8A+vTpw7Zt2+o2qYiInFeVW+hny8jI4NChQyQkJJCXl4fD4QDKSj8/P7/SZdLT00lPTwcgNTUVp9NZPkBoaIUxf9nCbNVetjpqkjXQgikrBFfeYMoKwZU3mLLawmxYLBZsYTaAiyK334VeUlLCzJkzGTlyJA0bNvT7BlJSUkhJSfFdzsrKKne90+msMOYvV6mr2stWR02yBlowZYXgyhtMWSG48gZTVlepC1uYDVepC6jYbbUpLi7Or3l+HeXidruZOXMmvXr1olu3bgDY7XZyc3MByM3NJSoqqppRRUSkNlRZ6IZhMH/+fOLj4xk0aJBvvGvXrmzYsAGADRs2cM0119RdShERqVKVu1z27t3Lxo0bufTSS5kwYQIAd999N4MHD2bWrFmsXbsWp9PJo48+WudhRUTk3Kos9Hbt2rFs2bJKr5syZUqtBxIRkerRO0VFRExChS4iYhIqdBERk1Chi4iYhApdRMQkVOgiIiahQhcRMQkVuoiISajQRURMQoUuImISKnQREZNQoYuImIQKXUTEJFToIiImoUIXETEJFbqIiEmo0EVETEKFLiJiEip0ERGTUKGLiJhElR8SPW/ePLZv347dbmfmzJkALFu2jDVr1hAVFQXA3XffTefOnes2qYiInFeVhZ6cnMwtt9xCWlpaufGBAwdy22231VkwERG5MFXucklMTKRx48aByCIiIjVQ5Rb6uaxatYqNGzfSqlUrfv3rX5+z9NPT00lPTwcgNTUVp9NZPkBoaIUxf9nCbNVetjpqkjXQgikrBFfeYMoKwZU3mLLawmxYLBZsYTaAiyJ3tQq9X79+DB06FIB3332Xt99+m4cffrjSuSkpKaSkpPguZ2Vllbve6XRWGPOXq9RV7WWroyZZAy2YskJw5Q2mrBBceYMpq6vUhS3MhqvUBVTsttoUFxfn17xqHeXSpEkTrFYrVquVvn37cvDgweqsRkREalG1Cj03N9f3/61bt9KyZctaCyQiItVT5S6X2bNns3v3bgoKCnjwwQcZNmwYu3bt4vDhw1gsFmJjYxk9enQgsoqIyHlUWejjx4+vMHbjjTfWSRgREak+vVNURMQkVOgiIiahQhcRMQkVuoiISajQRURMQoUuImISKnQREZNQoYuImIQKXUTEJFToIiJ+GvrJ0PqOcF4qdBERk1Chi4iYhApdRMQkVOgiIiahQhcRMQkVuoiISajQRURMQoUuImISKnQREZNQoYuImESVHxI9b948tm/fjt1uZ+bMmQAUFhYya9YsMjMziY2N5ZFHHqFx48Z1HlZERM6tyi305ORkHn/88XJjK1asoEOHDsydO5cOHTqwYsWKOgsoIiL+qbLQExMTK2x9b9u2jT59+gDQp08ftm3bVjfpRETEb1XucqlMXl4eDocDAIfDQX5+/jnnpqenk56eDkBqaipOp7N8gNDQCmP+soXZqr1sddQka6AFU1YIrrzBlBWCK+/FnvXszrGF2bBYLNjCbAAXRe5qFfqFSElJISUlxXc5Kyur3PVOp7PCmL9cpa5qL1sdNckaaMGUFYIrbzBlheDKe7FnPbtzXKUubGE2XKUuoGK31aa4uDi/5lXrKBe73U5ubi4Aubm5REVFVWc1IiJSi6pV6F27dmXDhg0AbNiwgWuuuaZWQ4mIyIWrcpfL7Nmz2b17NwUFBTz44IMMGzaMwYMHM2vWLNauXYvT6eTRRx8NRFYRETmPKgt9/PjxlY5PmTKl1sOIiEj16Z2iIiImoUIXETEJFbqIiEmo0EVETEKFLiJiEip0ERGTUKGLiJiECl1ExCRU6CIiJqFCFxExCRW6iIhJqNBFRExChS4iYhIqdBERk1Chi4iYhApdRMQkVOgiIiahQhcRMQkVuoiISajQRURMosoPiT6fMWPGEB4ejtVqJSQkhNTU1NrKJSJSb4Z+MpT3Br1X3zEuWI0KHWDq1KlERUXVRhYREakB7XIRETGJGm+h//nPfwbgpptuIiUlpcL16enppKenA5CamorT6SwfIDS0wpi/bGG2ai9bHTXJGmjBlBWCK28wZYXgynuxZD1Xt5w9bguzYbFYsIXZAC6K3DUq9GeffZbo6Gjy8vJ47rnniIuLIzExsdyclJSUckWflZVV7nqn01lhzF+uUle1l62OmmQNtGDKCsGVN5iyQnDlvViynqtbzh53lbqwhdlwlbqAit1Wm+Li4vyaV6NdLtHR0QDY7XauueYaDhw4UJPViYhIDVS70EtKSiguLvb9/9///jeXXnpprQUTEZELU+1dLnl5ecyYMQMAj8dDz549ufrqq2stmIiIXJhqF3qzZs2YPn16bWYREZEa0GGLIiImoUIXETEJFbqIiEmo0EVETEKFLiJiEip0ERGTUKGLiJiECl0CaugnQ+s7gohpqdBFLlC/v/ar7wgilVKhi4iYhApdRMQkVOgitUR/H5D6pkKXi8L/xf3SegGQ2qZCFxGpI4F+0VahS41oK1Pk4qFCFxHxQzBsvKjQpU4Ew4M/UHRfSKCo0EUuMnoBkOpSoYtfVDIiF79qf6aoiMhFzzDKvs6wWMr+9XrLj1mt4Hb7hkI8p5fxeHzLh3gMPNbTy3q9hHgMQqwGFq+BYaHc8lgsEBKC1WuUjVutZV91rEaFvmPHDt544w28Xi99+/Zl8ODBtZVLzjwQLZayf10uMAwshoERGgo2G5b8fPB4ysZCQjDsdix5eViKi6GkBGt2Nt5LLsFSUoI1M7PsgWgYeGNjMRo1InT//rJ1e70YUVF4WrQg5OBBrAUFvgdxaadOWI8fJ+lAPrYtW8DrpTQxERo0wLZtG912naBBo3V44uNxt22L7YsvsBYW0vubbBo0Xs+p5GRCd+8m9MABMAxu+joTS+98LKdO0WDdOt/32aqwEICId9/FcuoUeL14mzalZMAAGqxfT+i+fb65RSNHEvrTT4R/9pkvf0m/frgTEoicO9c3r7RtW0puv52GS5cScvgwFq8XIySEgsmTsf3zn4SvXFl2X3u9FI0cieFwEDl9um/5U9dfT8nttxOZmkrI8eNl911MDHSDiBUraLB6NZbT92nelClcduwkjgce8C1f/MtfUjJgAM8s3Ef0uyMAcLdpQ/6UKTROS2P2B7uJeXsYADlvvEHYjh1EvvQS87J2EfP6HRSOG8epHj1wDh3qe0yc6tmTgsmTsU+aRNg335T97G02sj75hIj336fxvHm+2z8xYwbe5s0Ju+EGmrrdYBicvPNOCn//e6LvuYfQQ4fAMPDEx5P9/vs0njuXRm+/7fvZZy1bhjUvj+hRo3zrLPz97yn6f/+PptdfjyUvr+wx0rkzOW+/jX3iRCI+/ti3/P9++SUNNm2iyfjxvrG86dMpvvlmmrdvz+lqpbh/f0688grR995L2ObNXGIYGA0acHzvXhq9/jpRf/qT72mRvXQpnhYtaNqzJ5bT6ywcNYr8Z54hNjmZsP37y+7nyy4jY/NmIqdNo/Grr/qWz1y7FuuJE8T88pe+sYLJkyl8+GGad+yIpaAAgLmtGsLt0OTRR4n44AMA1hkeBs64hvC//x3HmDGsMzxYgOfuS2Bd52guadXKt86SgQPJffVVXpq7h0vGtqLwoYcomDy5Ok1wQapd6F6vl9dff50nn3ySmJgYJk+eTNeuXWnRokVt5vOJnDaNkGPHyp5U0dHkP/MM/b7MxPHRaN+DLf9Pf8JSVETUn//sGzt5552U3HYbjt/8pqyovF5K27Uj/9lnaTx7Ng3Wry97YHi9ZC9dStg33xD1/PPg9WIxDPInTOBU797E9u9PiNVKbGkpp/r0If+ZZ2gybhy2bdvKbissjIx//IOGf/sbkTNm+Mozd/58PPHxxN5yi698ikaMoOCJJ3DeckvZA9Drxd2yJZkbNxI5bRqRaWm+7ztj7Vos+fk4hw0DqxUDKJgwgaIHHyQ2JQVrUREArs6dyVm8mMjp04n49FOsISHEGgYZa9Zg27YN+1NPlb04WCzkP/kkJTfcgOO3vy3barBYyr6nKVNovHAhYf/6V9ltNWhA9v/8D7atW3nof34kcsMLYLWSP3UqnqZNaTxvHndnHaXRvxZScsstuNu2JeKTT7AeP86A/80kvHAlp5KTCduzh/DPPwerlZ7HcrEUFmI5eZIGmzb5tlycl5wCIGzXrrJCt1hwl5QAYM3KIuSnn3z5Ladf4Cx5eb4xPJ6yO+z0i6BhtUJISNlQw4YYUVEYFkvZiyFgNG6Mp0UL3/dPRARGWBil7dr5Mnni4gAoTUrCfcUVZctFRgLfUZqQ8N/bslgwGjfmRGQYxbff7tviK23bFoBPe8TSqcuvwWLB26QJACU33shfiz6gVfexZeuw2XD/4hcU/OEP/GXLVJ65biLuhASw2ch7+mnf9+l1OAAofOCBsp/9me8VOJWcTOmVV/rmelq2LPueli3jRF4eBmCcvv286dOhtLRs+dP3SdGvf03xL3+JAWW3FRuLx+slc+VK3zqNxo0ByProo7L722LBOH0/50+ZQv6Z0jo9t+Smm/jf7dv/O2azQWgo/7tzZ9n9Cb6fU84bb+CMiSErK8u3JV00ciRFv/71f7esT/+8jv34o2+dZ2SuW1fuMkDBE09Q8MQTFfrk2H/+U2Hs+K5dvv+P+WQo7wEn5szhxJw5wH93O5YMupVjt97K0E+GYguz4Sp1la3zTKazjB+fyHuD3qswXlcshnH27yP+27dvH8uXL+eJ03fWB6dfxe64447zLnf06NFyl51OZ9kPsArhq1aVvXparXgbN+ZUv35MnN+fOXEP+Z6Ap3r1gtJSbF99VbZQSAju1q3xXH45tk2bykrWYsGw2ylNSiL0wAGs2dm+oizt1AlLQQGhP/7oe6J44uMx7HZC9+7FERND7okTeBs3xhsXh/XYsbLyOVMALVqUlVVBge9J7bXbITS0rHxOPxgNmw0iIsq2pAHjzK98Npsv488fmBfK3/vVX0M/GVrpA7O2xu9ZdQ9Lb15aO2Hr2Lmy1vV9VF21/VjwR3W/h/rIWpnK8p8p9DPjPy/0uvxZxp3euKhKtQt9y5Yt7NixgwcffBCAjRs3sn//fkaNGlVuXnp6Ounp6QCkpqZW56ZERMQP1d5LX9nrgKWSrcqUlBRSU1PPWeaTJk2qboSAU9a6E0x5gykrBFfeYMoKF1/eahd6TEwM2dnZvsvZ2dk4Tu/fExGRwKt2obdu3Zpjx46RkZGB2+1m8+bNdO3atTaziYjIBQh5+umnn67OglarlebNm/Pyyy/z2Wef0atXL7p3716tEK3OOtznYqesdSeY8gZTVgiuvMGUFS6uvNX+o6iIiFxc9NZ/ERGTUKGLiJhEwM/l8s9//pPly5dz5MgRpk2bRuvWrc851+v1MmnSJKKjo+vl8CB/smZlZZGWlsaJEyewWCykpKQwYMCAizIrXDynaygsLGTWrFlkZmYSGxvLI488QuPT70I825IlS9i+fTuGYdChQwfuv//+Sg+PvRiyZmVlMX/+fN/RX5MnT6Zp06YBzQr+5wU4efIkjzzyCNdee22F95AEgj9ZDx8+zMKFCykuLsZqtTJkyBCuu+66gGWs6jlTWlrKK6+8wvfff09kZCTjx4+vl587AEaA/ec//zGOHDliTJ061Thw4MB553788cfG7Nmzjeeffz5A6crzJ2tOTo5x8OBBwzAM4+TJk8a4ceOM//znP4GMaRiGf1k9Ho8xduxY4/jx40Zpaanx2GOP1UtWwzCMxYsXGx988IFhGIbxwQcfGIsXL64w57vvvjOefPJJw+PxGB6Px3j88ceNnTt3BjqqX1kNwzCmTp1q/Otf/zIMwzCKi4uNkpKSgGU8m795DcMwFi1aZMyePdt47bXXAhWvHH+yHjlyxDh69KhhGIaRnZ1tPPDAA0ZhYWFA8vnznPnss8+MBQsWGIZhGJs2bTJeeumlgGSrTMB3ubRo0cKvt7FmZ2ezfft2+vbtG4BUlfMnq8Ph8P2VOyIigvj4eHJycgIRrxx/sh44cIDmzZvTrFkzQkNDue6669i2bVuAEpa3bds2+vTpA0CfPn0qzWGxWHC5XLjdbkpLS/F4PNjt9kBH9SvrTz/9hMfjISkpCYDw8HAaNGgQ0Jxn+JMX4PvvvycvL4+OHTsGMl45/mSNi4vjkksuASA6Ohq73U5+fn5A8vnznPnqq69ITk4GoHv37uzcubPSN14GwkV7+tw333yT4cOHU3z6fCfBICMjg0OHDpGQkFDfUSqVk5NDTEyM73JMTAz7T5+dLtDy8vJ8b0RzOByVPkHbtm3LVVddxejRozEMg1tuuaXOTv52Pv5kPXr0KI0aNWLGjBlkZGT5hWDXAAAL00lEQVTQoUMH7r33XqwBOGXqz/mT1+v18vbbbzN27Fh2nj5RVn3wJ+vZDhw4gNvtplmzZoGI59dz5uw5ISEhNGzYkIKCAqKiogKS8Wx1UujPPvssJ06cqDB+1113cc0111S5/Ndff43dbqdVq1bsOusMaHWhplnPKCkpYebMmYwcOZKGDRvWZkSfmmatbKuhLvdHny+vP44fP86RI0eYP3++b327d+8mMTGxVnOeWXdNsnq9Xvbs2cOLL76I0+lk1qxZrF+/nhtvvLG2owI1z/v555/TqVMnnE5nbUeroKZZz8jNzeXll19mzJgxAXuh9Oc5E+jn1fnUSaE/9dRTNVp+7969fPXVV3zzzTe4XC6Ki4uZO3cu48aNq6WE/1XTrABut5uZM2fSq1cvunXrVgupKlfTrIE+XcP58trtdnJzc3E4HOTm5la6NbN161batGlDeHg4AJ06dWL//v11Uug1zRodHc0VV1zh23K89tpr2bdvX50Vek3z7tu3jz179vD5559TUlKC2+0mPDyce++996LLCmV/vE1NTeWuu+6i7enTEgeCP8+ZM3NiYmLweDycPHnynH+ErmsX5WGL99xzD/PnzyctLY3x48fTvn37Oinz2mAYBvPnzyc+Pp5BgwbVd5zzuphO19C1a1c2bNgAwIYNGyr9DcPpdLJnzx48Hg9ut5vdu3cTHx8f6Kh+ZU1ISKCoqMi3y2Dnzp31snsI/Ms7btw4Xn31VdLS0hgxYgS9e/eukzKvij9Z3W43M2bMoHfv3vTo0SOg+fx5znTp0oX169cDZWehveqqq+ptC73ab/2vrq1bt/Lss89y9OhRtm7dyrfffkvv3r3Jyclh9uzZ9OrVq9z8zMxM9u3bR8+ePQMZ0++se/fu5c033+TUqVOsXr2a1atX43Q6fX/EuZiy1ubpGmqqVatWfPjhh7z//vsUFBRw//33Y7PZOHjwIMuWLaNr167Ex8ezb98+li5dyurVq2nXrl29HBLqT1aLxULz5s2ZO3cuq1atwuFwMGzYsHrZh+5P3rMdPnyY3NxcOnfufFFm3bRpEytXriQnJ8f3HGvbti1NTn9YR10613Pm3XffpaSkhLi4OC699FI2bdrE0qVLOXz4MKNHj663LXS99V9ExCQuyl0uIiJy4VToIiImoUIXETEJFbqIiEmo0EVETEKFLgH19NNPs2bNmvqOAcCYMWP497//Xa8Z0tLSeOeddwDYs2cPv//97+s1jwS3i/ZcLlK/xowZw4kTJ8odR52cnFwvp1itzK5du3j55Zd9pwWoCwcPHmT58uXs3bsXwzBwOBxce+213HrrrXVynPGVV17JnDlzamVdY8aM4be//a3vZGHyf4MKXc5p4sSJ/2cLYe/evTz33HMMGTKEBx98kCZNmpCVlcXatWv54YcfuOqqqyos4/F4CAkJqYe0ImVU6HJBSktLeeCBB3jmmWe49NJLAcjPz+ehhx5i3rx5hISE8Morr7B//368Xi+/+MUveOCBB8qdse6MZcuWcfz4cd9pHTIyMhg7dix/+9vfCAkJYd26dXz00UdkZ2cTFRXF7bffzk033URJSQnTpk3D7XYzYsQIAObMmUOTJk346KOPWLNmDUVFRbRv377cu/Y2btzIO++8Q0lJSZWnaViyZAk33HADd9xxh2/M6XQybNgw3+X169ezZs0aWrduzYYNG7j55ptJTk5mwYIF/PDDD1gsFjp27MioUaNo1KgRAIcOHWL+/PkcO3aMTp06lXuL+M9/68jJyWHRokXs2bOH8PBwBg4c6Hun7LJly/jpp5+w2Wxs3boVp9PJmDFjaN26NS+//DJZWVm88MILWK1Whg4dSv/+/Zk/fz47duzA6/VyySWXMHHixIC821ICR/vQ5YKEhYVx7bXX8sUXX/jGNm/eTGJiIna7HcMwSE5OZt68ecybNw+bzcbrr79erduy2+1MnDiRt956i4cffpi33nqL77//nvDwcB5//HEcDgeLFy9m8eLFREdHs3LlSrZt28bTTz/NggULaNy4Ma+99hpQdr7yhQsXMnbsWBYsWEBBQUG5ky6draSkhH379vl1orX9+/fTrFkzXnvtNYYMGQLAHXfcwYIFC5g1axbZ2dksX74cKDsnyfTp0+nVqxeLFi2iR48efPnll5Wu1+v18sILL3D55ZezYMECpkyZwqeffsqOHTt8c77++muuu+463nzzTbp27cqiRYsA+N3vfofT6WTixIksXryY22+/nQ0bNnDy5EleffVVFi1axAMPPIDNZvP/hyFBQYUu5zR9+nRGjhzp+0pPTwegZ8+e5Qr9iy++8J1rJzIyku7du9OgQQMiIiIYMmQIe/bsqdbtd+7cmebNm2OxWEhMTCQpKYnvvvvunPPT09O56667iImJISwsjDvvvJMvv/wSj8fDli1b6NKlC4mJiYSFhfGrX/3qnCdQKioqwjCMcluvS5YsYeTIkYwYMYL333/fN+5wOOjfvz8hISHYbDaaN29OUlISYWFhREVFMXDgQHbv3g2UneHQ4/EwcOBAQkND6d69+zk/KvDgwYPk5+czdOhQQkNDadasGX379mXz5s2+Oe3ataNz585YrVZ69+7N4cOHz3nfhISEUFhYyPHjx7FarbRq1arOTvMs9Ue7XOScJkyYUOk+9Pbt2+Nyudi/fz9NmjTh8OHDXHvttQCcOnWKt956ix07dlBUVARAcXExXq/3gk9U9c033/Dee+9x9OhRDMPg1KlTvt08lcnMzGTGjBnlitpqtZKXl1fhgwrCw8OJjIysdD2NGjXCYrGQm5vrO7vj8OHDGT58OHPnzsXj8fjm/vx84nl5ebzxxhvs2bOHkpISvF6vb5dPbm4u0dHR5fKd63zkmZmZ5ObmMnLkSN+Y1+vlyiuv9F0++9ObbDab71OdKtuP37t3b7Kzs5k9ezYnT56kV69e3HXXXYSGqgLMRD9NuWBWq5UePXrwxRdfYLfb6dy5MxEREQB8/PHHHD16lGnTpvnK/o9//GOlHwIQHh6Oy+XyXT77QxBKS0uZOXMmY8eOpWvXroSGhvLiiy/6rq9s6zomJoaHHnqIdu3aVbjO4XBw5MgR3+VTp05RUFBQ6fcXHh5OmzZt2Lp1K+3bt/fjHvmvpUuXAjBjxgwiIyPZunWrb1eIw+EgJycHwzB8+bOzs2nevHmF9TidTpo2bcrcuXMv6PbPJTQ0lDvvvJM777yTjIwMnn/+eeLi4ursfO1SP7TLRaqlZ8+ebN68mU2bNpU7tXFJSQk2m42GDRtSWFjo239cmcsvv5w9e/aQlZXFyZMnWbFihe+6M58jGhUVRUhICN988025Y8btdjsFBQWcPHnSN3bTTTfxzjvvkJmZCZT9sfbM5z92796dr7/+mu+++w63282777573s99HD58OOvWrWPFihXk5eUBZeV7Zt3nUlxcTHh4OI0aNSInJ4ePP/7Yd13btm2xWq2sXLkSj8fDl19+yYEDBypdT0JCAhEREaxYsQKXy4XX6+XHH3885/yfa9KkCRkZGb7LO3fu5Mcff8Tr9dKwYUNCQ0Pr5dS+Ure0hS7ndOYoiTOSkpKYMGECAG3atKFBgwbk5OTQqVMn35wBAwYwd+5cRo0aRXR0NIMGDTrnhxQnJSXRo0cPHnvsMSIjI7n99tv56quvgLIP3L7//vuZNWsWpaWldOnSpdx5vOPj47n++usZO3YsXq+Xl156yXcEyHPPPUdubi52u50ePXpwzTXX0LJlS0aNGsWcOXM4deoUgwYNqvTImzPatWvHlClTeO+993wvNDExMXTt2pX+/fufc7k777yTV155hfvuu4/mzZvTu3dv/v73vwNlW8mPPfYYCxYs4J133qFTp06+XVU/Z7VamThxIm+//TZjxozB7XYTFxfHr371q3Pe9tkGDx7MokWLWLJkCUOGDCE6OpqFCxeSk5NDeHg4PXr0qPDZAxL8dD50ERGT0O9cIiImoUIXETEJFbqIiEmo0EVETEKFLiJiEip0ERGTUKGLiJiECl1ExCT+P5KDuts9qLVmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modeldo = Sequential()\n",
    "modeldo.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeldo.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeldo.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeldo.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeldo.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modeldo.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modeld.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "loss = keras.losses.mean_squared_error(modeldo.output,y_train_scaled)\n",
    "listOfVariableTensors = modeldo.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradientsdo = sess.run(gradients,feed_dict={modeldo.input:X_train_scaled.values})\n",
    "evaluated_gradientsdo = [gradient/len(y_train) for gradient in evaluated_gradientsdo]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "mu, sigma = 0, 1\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(evaluated_gradientsdo, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "plt.xlabel('Evaluated Gradients')\n",
    "plt.ylabel('')\n",
    "plt.title(r'$\\mathrm{Histogram}$')\n",
    "#plt.axis([-0.1, 0.7, 0, 10])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Vuelva a repetir la experimentación ahora cambiando la función de activación por ReLU, es decir, deberá visualizar los gradientes de los pesos de cada capa antes y después del entrenamiento, con inicialización *uniform* y comparar con la inicialización de He [[2]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/N_{in}}$ y $\\sqrt{6/N_{in}} $. Comente si ocurre el mismo fenómeno anterior (para función sigmoidal) sobre el efecto del *gradiente desvaneciente* para la función ReLU. Explique la importancia de la inicialización de los pesos dependiendo de la arquitectura.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "...\n",
    "or\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='relu')) #he\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 27.4344 - val_loss: 2.8600\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.9867 - val_loss: 1.1283\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.5372 - val_loss: 1.5091\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.4210 - val_loss: 1.1276\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 6s 634us/step - loss: 0.3344 - val_loss: 0.7306\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.2691 - val_loss: 0.8697\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.2534 - val_loss: 0.8071\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.2207 - val_loss: 0.6698\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.1865 - val_loss: 0.6007\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.2037 - val_loss: 0.5931\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.1855 - val_loss: 0.5772\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.1759 - val_loss: 0.5993\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 6s 588us/step - loss: 0.1535 - val_loss: 0.4717\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.1516 - val_loss: 1.5862\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1355 - val_loss: 0.7435\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.1242 - val_loss: 0.5200\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.1102 - val_loss: 0.5257\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.1243 - val_loss: 0.4474\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.1129 - val_loss: 0.4343\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 6s 630us/step - loss: 0.1053 - val_loss: 0.4304\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.1081 - val_loss: 0.4470\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.1094 - val_loss: 0.4190\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0940 - val_loss: 0.4378\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 6s 604us/step - loss: 0.1187 - val_loss: 0.4998\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0962 - val_loss: 0.4856\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 0.0947 - val_loss: 0.5427\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 6s 620us/step - loss: 0.0903 - val_loss: 0.4063\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0858 - val_loss: 0.4867\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.0885 - val_loss: 0.5411\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0777 - val_loss: 0.4092\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0929 - val_loss: 0.3641\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0882 - val_loss: 0.4405\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0752 - val_loss: 0.4195\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.0726 - val_loss: 0.5243\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.0658 - val_loss: 0.5708\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0750 - val_loss: 0.5328\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0667 - val_loss: 0.8558\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.0766 - val_loss: 0.3785\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 7s 677us/step - loss: 0.0634 - val_loss: 0.3520\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0649 - val_loss: 0.3878\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 9s 878us/step - loss: 0.0668 - val_loss: 0.3400\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0639 - val_loss: 0.3678\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0636 - val_loss: 0.3416\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.0623 - val_loss: 0.3334\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 9s 896us/step - loss: 0.0600 - val_loss: 0.3361\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 0.0554 - val_loss: 0.3354\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.0555 - val_loss: 0.3388\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 6s 621us/step - loss: 0.0562 - val_loss: 0.3784\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.0508 - val_loss: 0.3581\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0562 - val_loss: 0.7160\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0604 - val_loss: 0.4663\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0562 - val_loss: 0.3887\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 8s 857us/step - loss: 0.0516 - val_loss: 0.4434\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0507 - val_loss: 0.3382\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0509 - val_loss: 0.3520\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 0.0490 - val_loss: 0.3652\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0466 - val_loss: 0.3178\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 6s 576us/step - loss: 0.0527 - val_loss: 0.3269\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 6s 614us/step - loss: 0.0479 - val_loss: 0.3419\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 6s 593us/step - loss: 0.0516 - val_loss: 0.4133\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0444 - val_loss: 0.3155\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 6s 600us/step - loss: 0.0462 - val_loss: 0.3955\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 7s 692us/step - loss: 0.0461 - val_loss: 0.3159\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0430 - val_loss: 0.3517\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0457 - val_loss: 0.3141\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0456 - val_loss: 0.3167\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0432 - val_loss: 0.3203\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 7s 705us/step - loss: 0.0873 - val_loss: 0.4329\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0456 - val_loss: 0.3465\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0425 - val_loss: 0.3079\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0417 - val_loss: 0.3443\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 7s 747us/step - loss: 0.0405 - val_loss: 0.3154\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 0.0461 - val_loss: 0.3097\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0411 - val_loss: 0.3205\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0374 - val_loss: 0.3316\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0381 - val_loss: 0.2913\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 6s 624us/step - loss: 0.0374 - val_loss: 0.3275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0413 - val_loss: 0.2902\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0421 - val_loss: 0.2978\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 6s 654us/step - loss: 0.0385 - val_loss: 0.2980\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 5s 508us/step - loss: 0.0478 - val_loss: 0.3138\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0373 - val_loss: 0.3032\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 7s 734us/step - loss: 0.0355 - val_loss: 0.3118\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0383 - val_loss: 0.3205\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.0370 - val_loss: 0.3064\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 9s 951us/step - loss: 0.0382 - val_loss: 0.2913\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 9s 965us/step - loss: 0.0437 - val_loss: 0.3126\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 8s 783us/step - loss: 0.0419 - val_loss: 0.2976\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 769us/step - loss: 0.0363 - val_loss: 0.2939\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.0385 - val_loss: 0.2963\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 8s 795us/step - loss: 0.0372 - val_loss: 0.3050\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 5s 539us/step - loss: 0.0313 - val_loss: 0.3327\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0356 - val_loss: 0.2937\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 7s 730us/step - loss: 0.0317 - val_loss: 0.3130\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 8s 852us/step - loss: 0.0374 - val_loss: 0.2817\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0329 - val_loss: 0.3970\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0346 - val_loss: 0.3024\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0316 - val_loss: 0.2884\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0320 - val_loss: 0.3166\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.0366 - val_loss: 0.2922\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0336 - val_loss: 0.2925\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 8s 847us/step - loss: 0.0337 - val_loss: 0.2882\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0358 - val_loss: 0.2943\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0342 - val_loss: 0.2870\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0346 - val_loss: 0.2862\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.0341 - val_loss: 0.3093\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.0341 - val_loss: 0.2774\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.0307 - val_loss: 0.3988\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 9s 886us/step - loss: 0.0306 - val_loss: 0.2922\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 9s 960us/step - loss: 0.0312 - val_loss: 0.3106\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 9s 928us/step - loss: 0.0331 - val_loss: 0.3761\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0316 - val_loss: 0.2781\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0305 - val_loss: 0.2788\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.0327 - val_loss: 0.2860\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 8s 790us/step - loss: 0.0283 - val_loss: 0.2877\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.0275 - val_loss: 0.2787\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 8s 813us/step - loss: 0.0284 - val_loss: 0.3140\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.0270 - val_loss: 0.2882\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0288 - val_loss: 0.2799\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.0308 - val_loss: 0.2899\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0284 - val_loss: 0.2917\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 9s 974us/step - loss: 0.0297 - val_loss: 0.2869\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0293 - val_loss: 0.3062\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 9s 974us/step - loss: 0.0306 - val_loss: 0.2817\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0287 - val_loss: 0.3759\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0297 - val_loss: 0.3010\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 10s 996us/step - loss: 0.0290 - val_loss: 0.2729\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.0261 - val_loss: 0.2851\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 8s 839us/step - loss: 0.0253 - val_loss: 0.2880\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0283 - val_loss: 0.3004\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 9s 895us/step - loss: 0.0346 - val_loss: 0.2711\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0301 - val_loss: 0.2846\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 8s 824us/step - loss: 0.0321 - val_loss: 0.2990\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 7s 729us/step - loss: 0.0288 - val_loss: 0.3015\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 7s 686us/step - loss: 0.0277 - val_loss: 0.2872\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 6s 640us/step - loss: 0.0297 - val_loss: 0.2836\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0283 - val_loss: 0.2909\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0614 - val_loss: 0.3006\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 7s 676us/step - loss: 0.0261 - val_loss: 0.2880\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 8s 864us/step - loss: 0.0287 - val_loss: 0.4402\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 7s 715us/step - loss: 0.0265 - val_loss: 0.2881\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.0270 - val_loss: 0.2956\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 7s 719us/step - loss: 0.0294 - val_loss: 0.3241\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 7s 696us/step - loss: 0.0249 - val_loss: 0.2890\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0249 - val_loss: 0.2772\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0258 - val_loss: 0.2895\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0250 - val_loss: 0.2931\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0237 - val_loss: 0.2888\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.0227 - val_loss: 0.2916\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0290 - val_loss: 0.3055\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0272 - val_loss: 0.2774\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0250 - val_loss: 0.2697\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 6s 599us/step - loss: 0.0252 - val_loss: 0.3183\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0243 - val_loss: 0.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0241 - val_loss: 0.2928\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 9s 890us/step - loss: 0.0217 - val_loss: 0.2834\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 9s 926us/step - loss: 0.0248 - val_loss: 0.2714\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 9s 918us/step - loss: 0.0246 - val_loss: 0.2818\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 9s 971us/step - loss: 0.0245 - val_loss: 0.2760\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 9s 971us/step - loss: 0.0236 - val_loss: 0.2881\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0238 - val_loss: 0.2845\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.0237 - val_loss: 0.2957\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0267 - val_loss: 0.2867\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0242 - val_loss: 0.2822\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 9s 908us/step - loss: 0.0219 - val_loss: 0.2745\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 10s 976us/step - loss: 0.0316 - val_loss: 0.2893\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 9s 940us/step - loss: 0.0247 - val_loss: 0.2940\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 9s 964us/step - loss: 0.0237 - val_loss: 0.2911\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0214 - val_loss: 0.2954\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0245 - val_loss: 0.3320\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0214 - val_loss: 0.2856\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0237 - val_loss: 0.2882\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0254 - val_loss: 0.2994\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 10s 995us/step - loss: 0.0226 - val_loss: 0.3000\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0242 - val_loss: 0.2865\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0237 - val_loss: 0.2910\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 9s 956us/step - loss: 0.0220 - val_loss: 0.2819\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 0.0253 - val_loss: 0.3027\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0242 - val_loss: 0.2898\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0223 - val_loss: 0.2771\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0206 - val_loss: 0.2642\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0209 - val_loss: 0.2813\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0234 - val_loss: 0.2976\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0222 - val_loss: 0.2716\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0225 - val_loss: 0.2995\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 10s 982us/step - loss: 0.0205 - val_loss: 0.2872\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0208 - val_loss: 0.2936\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 7s 738us/step - loss: 0.0242 - val_loss: 0.2910\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 7s 683us/step - loss: 0.0220 - val_loss: 0.2747\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 652us/step - loss: 0.0205 - val_loss: 0.2776\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0216 - val_loss: 0.3495\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 7s 748us/step - loss: 0.0211 - val_loss: 0.2954\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.0196 - val_loss: 0.2783\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 8s 775us/step - loss: 0.0212 - val_loss: 0.2921\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.0204 - val_loss: 0.2715\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0202 - val_loss: 0.2888\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.0209 - val_loss: 0.2803\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.0256 - val_loss: 0.2785\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0217 - val_loss: 0.3037\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0200 - val_loss: 0.2802\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 8s 847us/step - loss: 0.0222 - val_loss: 0.2830\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 8s 826us/step - loss: 0.0202 - val_loss: 0.2890\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0214 - val_loss: 0.2792\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0208 - val_loss: 0.2750\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0236 - val_loss: 0.2807\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.0214 - val_loss: 0.2822\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0206 - val_loss: 0.3176\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.0211 - val_loss: 0.2836\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0188 - val_loss: 0.2816\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 7s 768us/step - loss: 0.0223 - val_loss: 0.2772\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0200 - val_loss: 0.2939\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 6s 639us/step - loss: 0.0200 - val_loss: 0.2999\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0204 - val_loss: 0.2856\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0193 - val_loss: 0.2777\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.0196 - val_loss: 0.2708\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 6s 568us/step - loss: 0.0215 - val_loss: 0.3170\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0191 - val_loss: 0.3173\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 6s 570us/step - loss: 0.0192 - val_loss: 0.2836\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0192 - val_loss: 0.2770\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0195 - val_loss: 0.2764\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 9s 891us/step - loss: 0.0206 - val_loss: 0.3056\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0195 - val_loss: 0.3421\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0212 - val_loss: 0.2895\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.0184 - val_loss: 0.2782\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.0196 - val_loss: 0.2962\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0176 - val_loss: 0.2808\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0192 - val_loss: 0.2891\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0229 - val_loss: 0.2814\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0201 - val_loss: 0.2831\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 8s 825us/step - loss: 0.0198 - val_loss: 0.2883\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0179 - val_loss: 0.2940\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0193 - val_loss: 0.2943\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.0178 - val_loss: 0.2822\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 7s 669us/step - loss: 0.0198 - val_loss: 0.3247\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0184 - val_loss: 0.2937\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 8s 812us/step - loss: 0.0173 - val_loss: 0.2851\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0170 - val_loss: 0.2815\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 8s 782us/step - loss: 0.0183 - val_loss: 0.2805\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.0177 - val_loss: 0.2922\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.0187 - val_loss: 0.2871\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 7s 698us/step - loss: 0.0185 - val_loss: 0.2867\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 8s 787us/step - loss: 0.0177 - val_loss: 0.2803\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0184 - val_loss: 0.3042\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0183 - val_loss: 0.2983\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0185 - val_loss: 0.2808\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.0184 - val_loss: 0.2899\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.0179 - val_loss: 0.3051\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 9s 937us/step - loss: 0.0206 - val_loss: 0.2841\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 8s 836us/step - loss: 0.0178 - val_loss: 0.2882\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.0175 - val_loss: 0.2826\n"
     ]
    }
   ],
   "source": [
    "modele = Sequential()\n",
    "modele.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "modele.add(Dense(256,  kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modele.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.001)\n",
    "modele.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historye = modele.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resulte= pd.DataFrame(historyd.history)\n",
    "resulte.to_csv(\"history2e(uniform).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 15.9934 - val_loss: 4.1458\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 1.8212 - val_loss: 3.2474\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 1.0384 - val_loss: 2.2443\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.6994 - val_loss: 2.0361\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.5443 - val_loss: 1.9875\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.4534 - val_loss: 1.7690\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.3952 - val_loss: 1.6390\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.3391 - val_loss: 1.6707\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.3057 - val_loss: 1.5853\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2787 - val_loss: 1.4733\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2561 - val_loss: 1.4612\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2421 - val_loss: 1.4548\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2179 - val_loss: 1.3559\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.2071 - val_loss: 1.5436\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1952 - val_loss: 1.2635\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1873 - val_loss: 1.3319\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1782 - val_loss: 1.2781\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1749 - val_loss: 1.2374\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1631 - val_loss: 1.2542\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1561 - val_loss: 1.1899\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 24s 2ms/step - loss: 0.1504 - val_loss: 1.1432\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1455 - val_loss: 1.1136\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1367 - val_loss: 1.0669\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1348 - val_loss: 1.1734\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1304 - val_loss: 1.0922\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 24s 2ms/step - loss: 0.1295 - val_loss: 1.1963\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1275 - val_loss: 1.1409\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1226 - val_loss: 1.0659\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1212 - val_loss: 1.0489\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1144 - val_loss: 1.0916\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 23s 2ms/step - loss: 0.1095 - val_loss: 1.0790\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.1056 - val_loss: 1.0463\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.1034 - val_loss: 0.9995\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.1060 - val_loss: 0.9344\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0998 - val_loss: 1.0236\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0981 - val_loss: 0.9802\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0958 - val_loss: 1.0135\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0973 - val_loss: 0.9425\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0916 - val_loss: 0.9414\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0914 - val_loss: 0.9710\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0882 - val_loss: 0.9522\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0864 - val_loss: 0.9235\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0842 - val_loss: 0.9116\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0847 - val_loss: 0.8926\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0829 - val_loss: 0.8964\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0806 - val_loss: 0.8928\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0784 - val_loss: 0.8807\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0798 - val_loss: 0.8784\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0765 - val_loss: 0.8906\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0737 - val_loss: 0.8457\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0733 - val_loss: 0.8123\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0725 - val_loss: 0.8206\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0719 - val_loss: 0.8238\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0710 - val_loss: 0.8400\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0694 - val_loss: 0.8602\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0681 - val_loss: 0.8122\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0670 - val_loss: 0.8055\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0675 - val_loss: 0.7823\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0675 - val_loss: 0.8352\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0657 - val_loss: 0.7625\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0643 - val_loss: 0.7820\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0636 - val_loss: 0.7937\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0617 - val_loss: 0.7754\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0644 - val_loss: 0.7449\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0631 - val_loss: 0.7995\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0605 - val_loss: 0.7619\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0608 - val_loss: 0.7472\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0608 - val_loss: 0.7568\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0603 - val_loss: 0.8454\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0593 - val_loss: 0.6987\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0581 - val_loss: 0.7549\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0593 - val_loss: 0.7252\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0585 - val_loss: 0.6976\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0569 - val_loss: 0.7052\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0570 - val_loss: 0.7130\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0558 - val_loss: 0.7169\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0548 - val_loss: 0.6749\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0547 - val_loss: 0.6810\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0533 - val_loss: 0.6896\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0535 - val_loss: 0.7163\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0526 - val_loss: 0.6997\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0524 - val_loss: 0.6791\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0519 - val_loss: 0.6661\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0505 - val_loss: 0.6793\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0512 - val_loss: 0.6490\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0503 - val_loss: 0.6646\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0493 - val_loss: 0.6943\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0499 - val_loss: 0.6650\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0493 - val_loss: 0.6501\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0485 - val_loss: 0.6601\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0483 - val_loss: 0.6610\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0477 - val_loss: 0.6312\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0469 - val_loss: 0.6719\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0465 - val_loss: 0.6427\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0494 - val_loss: 0.6144\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0477 - val_loss: 0.6825\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0457 - val_loss: 0.6240\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0451 - val_loss: 0.6385\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0450 - val_loss: 0.6267\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0450 - val_loss: 0.5997\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0464 - val_loss: 0.6270\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0440 - val_loss: 0.6455\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0467 - val_loss: 0.6051\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0474 - val_loss: 0.6112\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0452 - val_loss: 0.6054\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0440 - val_loss: 0.6068\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0433 - val_loss: 0.6293\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0431 - val_loss: 0.5939\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0439 - val_loss: 0.6871\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0440 - val_loss: 0.5755\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0425 - val_loss: 0.6233\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0413 - val_loss: 0.5993\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0416 - val_loss: 0.5721\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0415 - val_loss: 0.5777\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0402 - val_loss: 0.5764\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0403 - val_loss: 0.5741\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0406 - val_loss: 0.6377\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0406 - val_loss: 0.5497\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0397 - val_loss: 0.6457\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0396 - val_loss: 0.5437\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0386 - val_loss: 0.5684\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0385 - val_loss: 0.5682\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0393 - val_loss: 0.5656\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0380 - val_loss: 0.5718\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0381 - val_loss: 0.5480\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0375 - val_loss: 0.5609\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0375 - val_loss: 0.5362\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0373 - val_loss: 0.5693\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0380 - val_loss: 0.5691\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0387 - val_loss: 0.5724\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0378 - val_loss: 0.5668\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0363 - val_loss: 0.5469\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0364 - val_loss: 0.5526\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0362 - val_loss: 0.5695\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0360 - val_loss: 0.5489\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0357 - val_loss: 0.5572\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0364 - val_loss: 0.5773\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0355 - val_loss: 0.5574\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0350 - val_loss: 0.5441\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0352 - val_loss: 0.5488\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0351 - val_loss: 0.5282\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0353 - val_loss: 0.5613\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0345 - val_loss: 0.5392\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0339 - val_loss: 0.5421\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0340 - val_loss: 0.5275\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 7s 702us/step - loss: 0.0339 - val_loss: 0.5290\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 6s 633us/step - loss: 0.0337 - val_loss: 0.5373\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0335 - val_loss: 0.5447\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0346 - val_loss: 0.5403\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 0.0352 - val_loss: 0.5206\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0332 - val_loss: 0.5323\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0342 - val_loss: 0.5301\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 5s 493us/step - loss: 0.0325 - val_loss: 0.5115\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0329 - val_loss: 0.5496\n",
      "Epoch 155/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 5s 509us/step - loss: 0.0326 - val_loss: 0.5384\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 5s 530us/step - loss: 0.0330 - val_loss: 0.5520\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0336 - val_loss: 0.5173\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 5s 523us/step - loss: 0.0334 - val_loss: 0.5173\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0322 - val_loss: 0.5260\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0320 - val_loss: 0.5238\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0321 - val_loss: 0.5078\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0321 - val_loss: 0.5589\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0319 - val_loss: 0.5220\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0325 - val_loss: 0.5606\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0328 - val_loss: 0.5297\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 566us/step - loss: 0.0304 - val_loss: 0.5216\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 5s 538us/step - loss: 0.0317 - val_loss: 0.5157\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0314 - val_loss: 0.5157\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.0311 - val_loss: 0.5167\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0308 - val_loss: 0.5002\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0300 - val_loss: 0.5208\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0310 - val_loss: 0.4975\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0305 - val_loss: 0.4883\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0303 - val_loss: 0.5096\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 5s 513us/step - loss: 0.0297 - val_loss: 0.4828\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0298 - val_loss: 0.4953\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0297 - val_loss: 0.4967\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0293 - val_loss: 0.5181\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0295 - val_loss: 0.5005\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0298 - val_loss: 0.5079\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0290 - val_loss: 0.6056\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0293 - val_loss: 0.4865\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 5s 535us/step - loss: 0.0290 - val_loss: 0.5081\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0287 - val_loss: 0.4913\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0290 - val_loss: 0.5040\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.0291 - val_loss: 0.5007\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.0285 - val_loss: 0.4864\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 6s 573us/step - loss: 0.0286 - val_loss: 0.4960\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0290 - val_loss: 0.4865\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0285 - val_loss: 0.5164\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0281 - val_loss: 0.4719\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0284 - val_loss: 0.4926\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 6s 616us/step - loss: 0.0278 - val_loss: 0.4852\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0278 - val_loss: 0.4876\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0279 - val_loss: 0.4947\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.0275 - val_loss: 0.4809\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 5s 478us/step - loss: 0.0274 - val_loss: 0.5100\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0273 - val_loss: 0.4931\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0277 - val_loss: 0.5000\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 6s 636us/step - loss: 0.0273 - val_loss: 0.4846\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0272 - val_loss: 0.4767\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0275 - val_loss: 0.4913\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0271 - val_loss: 0.4694\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0266 - val_loss: 0.4813\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0264 - val_loss: 0.4871\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 6s 603us/step - loss: 0.0264 - val_loss: 0.4916\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0266 - val_loss: 0.4758\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0265 - val_loss: 0.5456\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 5s 495us/step - loss: 0.0264 - val_loss: 0.4869\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 5s 544us/step - loss: 0.0266 - val_loss: 0.4643\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0262 - val_loss: 0.4852\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0259 - val_loss: 0.4695\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0263 - val_loss: 0.4769\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.0258 - val_loss: 0.4810\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0260 - val_loss: 0.4938\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0260 - val_loss: 0.4871\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 6s 619us/step - loss: 0.0256 - val_loss: 0.4815\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0257 - val_loss: 0.4920\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0262 - val_loss: 0.4931\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 5s 479us/step - loss: 0.0261 - val_loss: 0.4642\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 5s 547us/step - loss: 0.0265 - val_loss: 0.4812\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0255 - val_loss: 0.4896\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0252 - val_loss: 0.4832\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0253 - val_loss: 0.4887\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.0251 - val_loss: 0.4791\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0250 - val_loss: 0.4699\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 6s 625us/step - loss: 0.0250 - val_loss: 0.4743\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0254 - val_loss: 0.4727\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0249 - val_loss: 0.4696\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 6s 605us/step - loss: 0.0251 - val_loss: 0.4649\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 528us/step - loss: 0.0253 - val_loss: 0.4572\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0255 - val_loss: 0.4665\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0249 - val_loss: 0.4703\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0245 - val_loss: 0.4646\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0245 - val_loss: 0.4686\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0246 - val_loss: 0.4643\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.0244 - val_loss: 0.4799\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0240 - val_loss: 0.4682\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 5s 499us/step - loss: 0.0237 - val_loss: 0.4637\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0241 - val_loss: 0.4733\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0235 - val_loss: 0.4721\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0239 - val_loss: 0.4719\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 6s 583us/step - loss: 0.0243 - val_loss: 0.4746\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0241 - val_loss: 0.4573\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0241 - val_loss: 0.4723\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0236 - val_loss: 0.4768\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0236 - val_loss: 0.4541\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0237 - val_loss: 0.4553\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 635us/step - loss: 0.0237 - val_loss: 0.4593\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0237 - val_loss: 0.4644\n"
     ]
    }
   ],
   "source": [
    "modelhe = Sequential()\n",
    "modelhe.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu')) #uniform\n",
    "modelhe.add(Dense(256,  kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelhe.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.00008)\n",
    "modelhe.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historye = modelhe.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "resulte= pd.DataFrame(historyd.history)\n",
    "resulte.to_csv(\"history2he(he-uniform).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>f) ¿Qué es lo que sucede con la red más profunda? ¿El modelo logra convergencia en su entrenamiento? Modifique aspectos estructurales (funciones de activación, inicializadores, regularización, momentum, variación de tasa de aprendizaje, entre otros) de la red profunda de 6 capas definida anteriormente (no modifique la profundidad ni el número de neuronas) para lograr un error cuadrático medio (mse) similar o menor al de una red no profunda, como la definida en b) en esta sección, sobre el conjunto de pruebas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 10.1803 - val_loss: 2.0508\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 2.5345 - val_loss: 1.5179\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 1.4274 - val_loss: 5.7192\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 1.0272 - val_loss: 0.7788\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.8274 - val_loss: 0.9646\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.6861 - val_loss: 0.8756\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.5210 - val_loss: 1.1036\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.4704 - val_loss: 2.7827\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.4209 - val_loss: 1.7655\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.3845 - val_loss: 0.3054\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.3298 - val_loss: 0.7431\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.3261 - val_loss: 0.9655\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.2710 - val_loss: 0.3975\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.2776 - val_loss: 0.7837\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.2016 - val_loss: 0.4784\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.2439 - val_loss: 0.5800\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.2116 - val_loss: 0.2056\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.2128 - val_loss: 0.9804\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 15s 1ms/step - loss: 0.1795 - val_loss: 0.6510\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1773 - val_loss: 0.4807\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1635 - val_loss: 0.4915\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.1540 - val_loss: 0.5670\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.1307 - val_loss: 0.8168\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1341 - val_loss: 1.1925\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1333 - val_loss: 0.7628\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.1309 - val_loss: 0.6497\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1254 - val_loss: 0.7087\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.1121 - val_loss: 0.6492\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1206 - val_loss: 0.5186\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 15s 1ms/step - loss: 0.1110 - val_loss: 0.5373\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.1057 - val_loss: 0.1475\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.1037 - val_loss: 0.8316\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0994 - val_loss: 0.7372\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0880 - val_loss: 0.2902\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0974 - val_loss: 0.5261\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0941 - val_loss: 0.4900\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0836 - val_loss: 0.6475\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0858 - val_loss: 0.7319\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0888 - val_loss: 0.1894\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0819 - val_loss: 0.7557\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0664 - val_loss: 0.4618\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0862 - val_loss: 0.7268\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0690 - val_loss: 0.8346\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0750 - val_loss: 0.4152\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0779 - val_loss: 0.4452\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0676 - val_loss: 0.4688\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0621 - val_loss: 0.6631\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0732 - val_loss: 0.3829\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0698 - val_loss: 0.3996\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0703 - val_loss: 0.5463\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0683 - val_loss: 0.8058\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0635 - val_loss: 0.4675\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0713 - val_loss: 0.8036\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0668 - val_loss: 0.5758\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0591 - val_loss: 0.5412\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0607 - val_loss: 0.5384\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0609 - val_loss: 0.2590\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0714 - val_loss: 0.2616\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0532 - val_loss: 0.5044\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0515 - val_loss: 0.5384\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0542 - val_loss: 0.8664\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0543 - val_loss: 0.5379\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0625 - val_loss: 0.5150\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0546 - val_loss: 0.7476\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0521 - val_loss: 0.3991\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0562 - val_loss: 0.8850\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0532 - val_loss: 0.5996\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0537 - val_loss: 0.5653\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0504 - val_loss: 0.8867\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0524 - val_loss: 0.3466\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0478 - val_loss: 0.3240\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0476 - val_loss: 0.1086\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0559 - val_loss: 0.4893\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0511 - val_loss: 0.3574\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0439 - val_loss: 0.7614\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0497 - val_loss: 0.5327\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0501 - val_loss: 0.5979\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0491 - val_loss: 0.3957\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0448 - val_loss: 0.4514\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0483 - val_loss: 0.2472\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0440 - val_loss: 0.5099\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0477 - val_loss: 0.8173\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0409 - val_loss: 0.6475\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0410 - val_loss: 0.5962\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0470 - val_loss: 0.5045\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0414 - val_loss: 0.4835\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0516 - val_loss: 0.9354\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0430 - val_loss: 0.5069\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0433 - val_loss: 0.4786\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0488 - val_loss: 0.6950\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0439 - val_loss: 0.3816\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0399 - val_loss: 0.5785\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0395 - val_loss: 0.5672\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 21s 2ms/step - loss: 0.0442 - val_loss: 0.3988\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 0.0367 - val_loss: 0.4771\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0366 - val_loss: 0.5513\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0375 - val_loss: 0.4593\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0463 - val_loss: 0.8423\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0341 - val_loss: 0.6499\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0386 - val_loss: 0.5801\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0425 - val_loss: 0.5465\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0345 - val_loss: 0.4040\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0386 - val_loss: 0.5942\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0375 - val_loss: 0.4459\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0483 - val_loss: 0.5689\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0346 - val_loss: 0.4514\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0350 - val_loss: 0.6457\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0353 - val_loss: 0.6742\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0357 - val_loss: 0.4250\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0344 - val_loss: 0.7291\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0353 - val_loss: 1.0639\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0383 - val_loss: 0.7327\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0324 - val_loss: 0.7149\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0345 - val_loss: 0.7784\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0413 - val_loss: 0.7392\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0324 - val_loss: 0.6451\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 19s 2ms/step - loss: 0.0336 - val_loss: 0.6645\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0366 - val_loss: 0.4140\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0354 - val_loss: 0.6697\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0332 - val_loss: 0.5518\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0353 - val_loss: 0.5353\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0319 - val_loss: 0.3206\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0393 - val_loss: 0.4406\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0316 - val_loss: 0.6906\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 20s 2ms/step - loss: 0.0309 - val_loss: 0.5223\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0309 - val_loss: 0.7054\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0330 - val_loss: 0.5982\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0313 - val_loss: 0.6726\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0334 - val_loss: 0.6391\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0307 - val_loss: 0.6448\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0322 - val_loss: 0.5527\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0304 - val_loss: 0.5374\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0327 - val_loss: 0.5756\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0354 - val_loss: 0.4827\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0282 - val_loss: 0.4870\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0313 - val_loss: 0.6080\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0281 - val_loss: 0.6509\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0312 - val_loss: 0.4670\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0319 - val_loss: 0.4182\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0311 - val_loss: 0.5536\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0317 - val_loss: 0.6266\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 18s 2ms/step - loss: 0.0286 - val_loss: 0.5183\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0274 - val_loss: 0.5954\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0277 - val_loss: 0.5095\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 17s 2ms/step - loss: 0.0300 - val_loss: 0.6362\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0267 - val_loss: 0.4002\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0279 - val_loss: 0.5055\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.0284 - val_loss: 0.5721\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0289 - val_loss: 0.4187\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0275 - val_loss: 0.3373\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 8s 837us/step - loss: 0.0288 - val_loss: 0.6692\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0317 - val_loss: 0.5724\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 7s 683us/step - loss: 0.0322 - val_loss: 0.5939\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.0257 - val_loss: 0.3264\n",
      "Epoch 155/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0317 - val_loss: 0.3936\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 0.0253 - val_loss: 0.6136\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0295 - val_loss: 0.5685\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 7s 722us/step - loss: 0.0270 - val_loss: 0.4987\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 7s 669us/step - loss: 0.0291 - val_loss: 0.6013\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.0288 - val_loss: 0.4700\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.0283 - val_loss: 0.4105\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0267 - val_loss: 0.7365\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 6s 601us/step - loss: 0.0274 - val_loss: 0.6919\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0263 - val_loss: 0.7663\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0251 - val_loss: 0.4014\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 6s 574us/step - loss: 0.0259 - val_loss: 0.5188\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.0254 - val_loss: 0.4836\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.0253 - val_loss: 0.6273\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0264 - val_loss: 0.5341\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0248 - val_loss: 0.3570\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0271 - val_loss: 0.9269\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 6s 570us/step - loss: 0.0246 - val_loss: 0.5703\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0257 - val_loss: 0.5088\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 6s 625us/step - loss: 0.0253 - val_loss: 0.4276\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0248 - val_loss: 0.4393\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0305 - val_loss: 0.7010\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0248 - val_loss: 0.4718\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.0256 - val_loss: 0.5917\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 6s 612us/step - loss: 0.0249 - val_loss: 0.5153\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.0261 - val_loss: 0.5824\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 6s 644us/step - loss: 0.0252 - val_loss: 0.4423\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.0233 - val_loss: 0.6933\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 6s 666us/step - loss: 0.0255 - val_loss: 0.4356\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0243 - val_loss: 0.3799\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0310 - val_loss: 0.2811\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.0247 - val_loss: 0.4056\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0256 - val_loss: 0.6129\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0269 - val_loss: 0.6142\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 6s 623us/step - loss: 0.0249 - val_loss: 0.6858\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.0228 - val_loss: 0.6141\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 7s 673us/step - loss: 0.0251 - val_loss: 0.5651\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0262 - val_loss: 0.5114\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0233 - val_loss: 0.4038\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.0256 - val_loss: 0.5376\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0251 - val_loss: 0.5300\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.0238 - val_loss: 0.5711\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0239 - val_loss: 0.3656\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 6s 641us/step - loss: 0.0245 - val_loss: 0.4905\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 6s 632us/step - loss: 0.0242 - val_loss: 0.5241\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0226 - val_loss: 0.1698\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 7s 695us/step - loss: 0.0232 - val_loss: 0.6873\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 6s 640us/step - loss: 0.0271 - val_loss: 0.4610\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 7s 708us/step - loss: 0.0246 - val_loss: 0.6259\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0250 - val_loss: 0.3159\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 7s 709us/step - loss: 0.0226 - val_loss: 0.4755\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0228 - val_loss: 0.7126\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0226 - val_loss: 0.4511\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0295 - val_loss: 0.5844\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0218 - val_loss: 0.5403\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 7s 685us/step - loss: 0.0211 - val_loss: 0.4205\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0237 - val_loss: 0.5048\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 6s 611us/step - loss: 0.0211 - val_loss: 0.4836\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0252 - val_loss: 0.5539\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 6s 613us/step - loss: 0.0213 - val_loss: 0.5858\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0226 - val_loss: 0.5799\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 6s 663us/step - loss: 0.0233 - val_loss: 0.4317\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0214 - val_loss: 0.6179\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.0236 - val_loss: 0.4790 - loss: 0.02\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0231 - val_loss: 0.4393\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.0226 - val_loss: 0.4301\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 7s 682us/step - loss: 0.0226 - val_loss: 0.5710\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0224 - val_loss: 0.5829\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0226 - val_loss: 0.5241\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 562us/step - loss: 0.0221 - val_loss: 0.4389\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 6s 580us/step - loss: 0.0218 - val_loss: 0.2945\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0217 - val_loss: 0.5286\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 9s 901us/step - loss: 0.0224 - val_loss: 0.4560\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 6s 590us/step - loss: 0.0215 - val_loss: 0.4920\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0216 - val_loss: 0.4717\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 6s 618us/step - loss: 0.0211 - val_loss: 0.6225\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0217 - val_loss: 0.5869\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0213 - val_loss: 0.6016\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0220 - val_loss: 0.3898\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 6s 585us/step - loss: 0.0226 - val_loss: 0.4159\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 546us/step - loss: 0.0211 - val_loss: 0.6024\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 6s 574us/step - loss: 0.0202 - val_loss: 0.5354\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 5s 536us/step - loss: 0.0223 - val_loss: 0.5792\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 564us/step - loss: 0.0205 - val_loss: 0.5291\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0225 - val_loss: 0.4929\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 563us/step - loss: 0.0212 - val_loss: 0.4722\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 6s 609us/step - loss: 0.0211 - val_loss: 0.4082\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0203 - val_loss: 0.6159\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0199 - val_loss: 0.6871\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0216 - val_loss: 0.5185\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0207 - val_loss: 0.4774\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0213 - val_loss: 0.5681\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.0199 - val_loss: 0.5442\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0212 - val_loss: 0.5004\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0212 - val_loss: 0.5763\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0194 - val_loss: 0.5174\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'historyd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-a055dd2332e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhistoryf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresultf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistoryd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mresultf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"history2f.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnumEpochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'historyd' is not defined"
     ]
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256,  kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyf = modelf.fit(X_train_scaled,\n",
    "                      y_train, epochs=250,\n",
    "                      verbose=1, \n",
    "                      validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "resultf= pd.DataFrame(historyf.history)\n",
    "resultf.to_csv(\"history2f.csv\")\n",
    "numEpochs = 250\n",
    "test_loss[-1] = modelf.evaluate(X_test_scaled, yTest, verbose=0)\n",
    "train_loss=historyf.history['loss']\n",
    "xc = range(numEpochs)\n",
    "plt.figure(1, figsize=(10, 6))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,test_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.title('Training Loss vs Testing Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **g) Experimente con la utilización de una función activación auxiliar (debido a que aproxima) a '**ReLU**'**y que es continua derivable (**softplus**)**¿Cuál es el beneficio de ésta con respecto ReLU? Comente.**\n",
    "\n",
    "Las dos funciones de activación son muy similares, excepto que Softplus es diferenciable es cero. Por otra parte, RElu hace más fáciles los cálculos y su derivada, por lo que los algoritmos forward pass y backward pass son más rápidos. Con la función de activación Relu se favorecen las representaciones distribuidas, por su arquitectura no es derivable en cero, la derivada no es continua. La dura saturación de Relu en el umbral podría bloquear el gradiente en la capa de salida, al tener ceros reales, ya que no pueden recuperarse de esto. Sin embargo, no sucede tan frecuentemente que la suma ponderada sea cero, para esto se puede normalizar la data entre 0 y 1. En casos donde no se tengan estas garantías es mejor usar la versión diferenciable Softplus, que además hace el entrenamiento es más sencillo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 16.0616 - val_loss: 8.6825\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 5.7890 - val_loss: 7.1030\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 4.6571 - val_loss: 6.0007\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 8s 781us/step - loss: 3.9643 - val_loss: 5.2332\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 3.4598 - val_loss: 4.6992\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 8s 789us/step - loss: 3.0641 - val_loss: 4.2025\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 646us/step - loss: 2.7297 - val_loss: 3.8254\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 7s 724us/step - loss: 2.4487 - val_loss: 3.4220\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 7s 761us/step - loss: 2.2064 - val_loss: 3.1768\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 1.9945 - val_loss: 2.9908\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 8s 810us/step - loss: 1.8121 - val_loss: 2.6893\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 7s 744us/step - loss: 1.6537 - val_loss: 2.6236\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 7s 699us/step - loss: 1.5124 - val_loss: 2.3904\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 6s 636us/step - loss: 1.3898 - val_loss: 2.3243\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 1.2834 - val_loss: 2.1637\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 1.1902 - val_loss: 2.1314\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 1.1084 - val_loss: 1.9413\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 7s 730us/step - loss: 1.0362 - val_loss: 1.9118\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 7s 728us/step - loss: 0.9736 - val_loss: 1.8896\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 7s 732us/step - loss: 0.9192 - val_loss: 1.7262\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 7s 705us/step - loss: 0.8703 - val_loss: 1.6959\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.8282 - val_loss: 1.7328\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.7903 - val_loss: 1.5817\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.7572 - val_loss: 1.6300\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.7270 - val_loss: 1.6028\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.6999 - val_loss: 1.5681\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.6760 - val_loss: 1.5445\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 8s 862us/step - loss: 0.6547 - val_loss: 1.5001\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.6350 - val_loss: 1.4926\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 9s 916us/step - loss: 0.6165 - val_loss: 1.4803\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 9s 887us/step - loss: 0.6005 - val_loss: 1.4622\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 8s 843us/step - loss: 0.5850 - val_loss: 1.4372\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.5715 - val_loss: 1.4672\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 8s 821us/step - loss: 0.5584 - val_loss: 1.4759\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.5462 - val_loss: 1.4127\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 9s 953us/step - loss: 0.5346 - val_loss: 1.3903\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 9s 972us/step - loss: 0.5243 - val_loss: 1.4541\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.5146 - val_loss: 1.4152\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.5055 - val_loss: 1.4024\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.4962 - val_loss: 1.4086\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.4880 - val_loss: 1.4022\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.4800 - val_loss: 1.3589\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.4727 - val_loss: 1.4149\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 7s 734us/step - loss: 0.4655 - val_loss: 1.3710\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.4588 - val_loss: 1.3170\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 9s 942us/step - loss: 0.4525 - val_loss: 1.3557\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.4462 - val_loss: 1.3647\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 9s 879us/step - loss: 0.4400 - val_loss: 1.3095\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 8s 865us/step - loss: 0.4346 - val_loss: 1.3446\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 8s 827us/step - loss: 0.4289 - val_loss: 1.3450\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 8s 845us/step - loss: 0.4238 - val_loss: 1.3204\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 9s 897us/step - loss: 0.4185 - val_loss: 1.3268\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 9s 887us/step - loss: 0.4138 - val_loss: 1.3224\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 9s 956us/step - loss: 0.4088 - val_loss: 1.3359\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.4044 - val_loss: 1.3198\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 9s 901us/step - loss: 0.3997 - val_loss: 1.3507\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 8s 851us/step - loss: 0.3957 - val_loss: 1.3585\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 8s 858us/step - loss: 0.3913 - val_loss: 1.3169\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.3874 - val_loss: 1.3502\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 8s 822us/step - loss: 0.3834 - val_loss: 1.3031\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.3794 - val_loss: 1.3546\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 9s 879us/step - loss: 0.3757 - val_loss: 1.3012\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 8s 851us/step - loss: 0.3718 - val_loss: 1.3641\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 8s 795us/step - loss: 0.3689 - val_loss: 1.3153\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 8s 844us/step - loss: 0.3654 - val_loss: 1.3198\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 8s 871us/step - loss: 0.3623 - val_loss: 1.2944\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.3588 - val_loss: 1.3779\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 8s 814us/step - loss: 0.3558 - val_loss: 1.2701\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.3526 - val_loss: 1.3379\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 8s 771us/step - loss: 0.3494 - val_loss: 1.2512\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 8s 841us/step - loss: 0.3466 - val_loss: 1.2616\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.3437 - val_loss: 1.2538\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.3411 - val_loss: 1.2935\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 9s 912us/step - loss: 0.3379 - val_loss: 1.2960\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.3355 - val_loss: 1.2926\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 9s 898us/step - loss: 0.3330 - val_loss: 1.3189\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 9s 937us/step - loss: 0.3302 - val_loss: 1.3223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.3276 - val_loss: 1.2370\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 8s 774us/step - loss: 0.3254 - val_loss: 1.2381\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 7s 758us/step - loss: 0.3231 - val_loss: 1.2668\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 7s 725us/step - loss: 0.3206 - val_loss: 1.2770\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.3184 - val_loss: 1.2878\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 7s 744us/step - loss: 0.3161 - val_loss: 1.2150\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.3141 - val_loss: 1.2763\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.3114 - val_loss: 1.2756\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 8s 778us/step - loss: 0.3096 - val_loss: 1.3462\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 8s 795us/step - loss: 0.3075 - val_loss: 1.2978\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 7s 732us/step - loss: 0.3055 - val_loss: 1.2457\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.3035 - val_loss: 1.2445\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.3014 - val_loss: 1.2712\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.2993 - val_loss: 1.2321\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 7s 728us/step - loss: 0.2975 - val_loss: 1.2364\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.2955 - val_loss: 1.2843\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 7s 743us/step - loss: 0.2941 - val_loss: 1.2602\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 7s 707us/step - loss: 0.2919 - val_loss: 1.2532\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 8s 803us/step - loss: 0.2901 - val_loss: 1.2573\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 7s 736us/step - loss: 0.2886 - val_loss: 1.2428\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.2868 - val_loss: 1.3282\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.2851 - val_loss: 1.2450\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 7s 735us/step - loss: 0.2833 - val_loss: 1.2016\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 7s 695us/step - loss: 0.2818 - val_loss: 1.2499\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.2801 - val_loss: 1.2360\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.2786 - val_loss: 1.2492\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.2770 - val_loss: 1.2174\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 9s 942us/step - loss: 0.2753 - val_loss: 1.2176\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 8s 770us/step - loss: 0.2740 - val_loss: 1.2126\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 8s 788us/step - loss: 0.2724 - val_loss: 1.2239\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 8s 828us/step - loss: 0.2709 - val_loss: 1.2253\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 8s 771us/step - loss: 0.2694 - val_loss: 1.2294\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.2682 - val_loss: 1.2175\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 8s 770us/step - loss: 0.2668 - val_loss: 1.1999\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.2652 - val_loss: 1.2615\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 7s 756us/step - loss: 0.2636 - val_loss: 1.2008\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 7s 755us/step - loss: 0.2624 - val_loss: 1.1913\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 0.2612 - val_loss: 1.1795\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 693us/step - loss: 0.2598 - val_loss: 1.2103\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 7s 739us/step - loss: 0.2585 - val_loss: 1.2666\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 8s 830us/step - loss: 0.2573 - val_loss: 1.2634\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 8s 853us/step - loss: 0.2561 - val_loss: 1.1945\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.2543 - val_loss: 1.2529\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 9s 931us/step - loss: 0.2534 - val_loss: 1.2379\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.2523 - val_loss: 1.2744\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 8s 848us/step - loss: 0.2512 - val_loss: 1.2389\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 8s 782us/step - loss: 0.2499 - val_loss: 1.2059\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 8s 780us/step - loss: 0.2488 - val_loss: 1.2068\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.2475 - val_loss: 1.1776\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 7s 768us/step - loss: 0.2463 - val_loss: 1.2476\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 8s 807us/step - loss: 0.2452 - val_loss: 1.1881\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.2441 - val_loss: 1.2449\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 8s 820us/step - loss: 0.2429 - val_loss: 1.2202\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 8s 789us/step - loss: 0.2420 - val_loss: 1.1801\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 8s 779us/step - loss: 0.2409 - val_loss: 1.2306\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 8s 806us/step - loss: 0.2399 - val_loss: 1.2049\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 8s 856us/step - loss: 0.2387 - val_loss: 1.2201\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2376 - val_loss: 1.1748\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.2366 - val_loss: 1.1689\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 8s 801us/step - loss: 0.2357 - val_loss: 1.2248\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2345 - val_loss: 1.2073\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.2336 - val_loss: 1.2095\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2326 - val_loss: 1.2119\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 8s 775us/step - loss: 0.2317 - val_loss: 1.1777\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 9s 888us/step - loss: 0.2306 - val_loss: 1.2077\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 6s 647us/step - loss: 0.2296 - val_loss: 1.2232\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.2286 - val_loss: 1.2176\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2277 - val_loss: 1.2403\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.2270 - val_loss: 1.1732\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 7s 759us/step - loss: 0.2259 - val_loss: 1.1881\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 6s 649us/step - loss: 0.2251 - val_loss: 1.2096\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 9s 949us/step - loss: 0.2240 - val_loss: 1.2282\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.2231 - val_loss: 1.1839\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 9s 914us/step - loss: 0.2222 - val_loss: 1.2166\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 9s 893us/step - loss: 0.2214 - val_loss: 1.2495\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 9s 874us/step - loss: 0.2206 - val_loss: 1.2417\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 10s 987us/step - loss: 0.2197 - val_loss: 1.1803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 9s 882us/step - loss: 0.2187 - val_loss: 1.1667\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.2179 - val_loss: 1.1592\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 8s 785us/step - loss: 0.2172 - val_loss: 1.1677\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 8s 862us/step - loss: 0.2162 - val_loss: 1.1438\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 8s 863us/step - loss: 0.2155 - val_loss: 1.1664\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 8s 868us/step - loss: 0.2148 - val_loss: 1.1375\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 8s 845us/step - loss: 0.2138 - val_loss: 1.1279\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 7s 713us/step - loss: 0.2130 - val_loss: 1.1662\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 9s 905us/step - loss: 0.2122 - val_loss: 1.2174\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.2113 - val_loss: 1.1960\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 9s 912us/step - loss: 0.2105 - val_loss: 1.1382\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.2100 - val_loss: 1.1482\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.2093 - val_loss: 1.2078\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.2083 - val_loss: 1.1783\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.2077 - val_loss: 1.1923\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.2067 - val_loss: 1.1674\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.2059 - val_loss: 1.1363\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 8s 803us/step - loss: 0.2055 - val_loss: 1.1509\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 9s 906us/step - loss: 0.2046 - val_loss: 1.1573\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 8s 859us/step - loss: 0.2040 - val_loss: 1.1446\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 9s 893us/step - loss: 0.2034 - val_loss: 1.2053\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 8s 792us/step - loss: 0.2027 - val_loss: 1.1623\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 8s 819us/step - loss: 0.2018 - val_loss: 1.1731\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 8s 777us/step - loss: 0.2011 - val_loss: 1.1902\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 8s 804us/step - loss: 0.2006 - val_loss: 1.1492\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 8s 863us/step - loss: 0.1998 - val_loss: 1.1422\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 9s 895us/step - loss: 0.1991 - val_loss: 1.1674\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 9s 959us/step - loss: 0.1982 - val_loss: 1.2042\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 8s 822us/step - loss: 0.1977 - val_loss: 1.1823\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 8s 817us/step - loss: 0.1969 - val_loss: 1.1292\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 8s 840us/step - loss: 0.1965 - val_loss: 1.1643\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 9s 957us/step - loss: 0.1956 - val_loss: 1.1521\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 10s 990us/step - loss: 0.1951 - val_loss: 1.1094\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 9s 903us/step - loss: 0.1944 - val_loss: 1.1797\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.1938 - val_loss: 1.1898\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 9s 889us/step - loss: 0.1931 - val_loss: 1.1982\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 9s 940us/step - loss: 0.1925 - val_loss: 1.1858\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 9s 908us/step - loss: 0.1918 - val_loss: 1.2076\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 9s 925us/step - loss: 0.1913 - val_loss: 1.1350\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 9s 965us/step - loss: 0.1908 - val_loss: 1.1771\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1897 - val_loss: 1.1932\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 9s 889us/step - loss: 0.1895 - val_loss: 1.1770\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.1888 - val_loss: 1.1467\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 8s 842us/step - loss: 0.1884 - val_loss: 1.1341\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.1876 - val_loss: 1.1634\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 9s 919us/step - loss: 0.1869 - val_loss: 1.1508\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 9s 925us/step - loss: 0.1864 - val_loss: 1.1317\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 9s 924us/step - loss: 0.1859 - val_loss: 1.1409\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 9s 927us/step - loss: 0.1852 - val_loss: 1.1718\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.1846 - val_loss: 1.1449\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.1842 - val_loss: 1.1418\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.1834 - val_loss: 1.1048\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 9s 921us/step - loss: 0.1831 - val_loss: 1.1465\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 9s 962us/step - loss: 0.1822 - val_loss: 1.1583\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 9s 892us/step - loss: 0.1820 - val_loss: 1.1269\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1813 - val_loss: 1.1563\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 862us/step - loss: 0.1808 - val_loss: 1.1348\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 9s 950us/step - loss: 0.1800 - val_loss: 1.1543\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 8s 865us/step - loss: 0.1797 - val_loss: 1.1399\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 9s 904us/step - loss: 0.1790 - val_loss: 1.1127\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.1786 - val_loss: 1.1433\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 9s 895us/step - loss: 0.1780 - val_loss: 1.1229\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.1777 - val_loss: 1.1385\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1769 - val_loss: 1.0992\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 9s 882us/step - loss: 0.1764 - val_loss: 1.1950\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 8s 861us/step - loss: 0.1759 - val_loss: 1.1574\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 9s 874us/step - loss: 0.1756 - val_loss: 1.1476\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.1750 - val_loss: 1.1417\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 9s 915us/step - loss: 0.1743 - val_loss: 1.1361\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.1737 - val_loss: 1.1777\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 9s 917us/step - loss: 0.1734 - val_loss: 1.1293\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 8s 771us/step - loss: 0.1727 - val_loss: 1.1489\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 8s 770us/step - loss: 0.1723 - val_loss: 1.0660\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 9s 889us/step - loss: 0.1720 - val_loss: 1.1558\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 8s 869us/step - loss: 0.1714 - val_loss: 1.1271\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 8s 800us/step - loss: 0.1709 - val_loss: 1.1546\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.1705 - val_loss: 1.1171\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 8s 845us/step - loss: 0.1700 - val_loss: 1.1248\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 9s 938us/step - loss: 0.1695 - val_loss: 1.1168\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 7s 712us/step - loss: 0.1690 - val_loss: 1.1544\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 8s 866us/step - loss: 0.1685 - val_loss: 1.1372\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 7s 698us/step - loss: 0.1680 - val_loss: 1.1262\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.1676 - val_loss: 1.1012\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 6s 629us/step - loss: 0.1672 - val_loss: 1.1393\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.1666 - val_loss: 1.1494\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.1662 - val_loss: 1.1278\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 6s 663us/step - loss: 0.1657 - val_loss: 1.1453\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 8s 796us/step - loss: 0.1654 - val_loss: 1.1279\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 7s 760us/step - loss: 0.1648 - val_loss: 1.1490\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 7s 733us/step - loss: 0.1644 - val_loss: 1.1218\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 7s 707us/step - loss: 0.1641 - val_loss: 1.1418\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.1635 - val_loss: 1.1603\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 8s 809us/step - loss: 0.1631 - val_loss: 1.0971\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 9s 914us/step - loss: 0.1626 - val_loss: 1.1078\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 9s 924us/step - loss: 0.1622 - val_loss: 1.1274\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 8s 832us/step - loss: 0.1618 - val_loss: 1.1346\n"
     ]
    }
   ],
   "source": [
    "modelg = Sequential()\n",
    "modelg.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(256, kernel_initializer='he_uniform',activation='softplus')) #uniform\n",
    "modelg.add(Dense(256,  kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelg.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.00001)\n",
    "modelg.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyg = modelg.fit(X_train_scaled, y_train, epochs=250, \n",
    "                      verbose=1, \n",
    "                      validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Pruebe con utilizar una red shallow (poco profunda), es decir, sitúe todas las neuronas en una única capa ¿Qué sucede con la convergencia del algoritmo? ¿Por qué sucede este fenómeno?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 250\n",
    "test_loss = np.zeros(numEpochs)\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss = self.model.evaluate(x, y, verbose=0)\n",
    "        test_loss[epoch-1] = loss\n",
    "        print('\\nTesting loss: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 8s 802us/step - loss: 0.1296 - val_loss: 0.0751\n",
      "\n",
      "Testing loss: 116.09199572763924\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 9s 886us/step - loss: 0.0727 - val_loss: 0.0653\n",
      "\n",
      "Testing loss: 118.33883508581287\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 9s 883us/step - loss: 0.0620 - val_loss: 0.0547\n",
      "\n",
      "Testing loss: 117.31697167047086\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 8s 810us/step - loss: 0.0560 - val_loss: 0.0491\n",
      "\n",
      "Testing loss: 116.37066476326474\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 10s 987us/step - loss: 0.0525 - val_loss: 0.0475\n",
      "\n",
      "Testing loss: 116.00664916290921\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 0.0498 - val_loss: 0.0475\n",
      "\n",
      "Testing loss: 116.4163169062524\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.0475 - val_loss: 0.0462\n",
      "\n",
      "Testing loss: 116.29033153410275\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 635us/step - loss: 0.0458 - val_loss: 0.0442\n",
      "\n",
      "Testing loss: 116.34854355766286\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 9s 885us/step - loss: 0.0444 - val_loss: 0.0422\n",
      "\n",
      "Testing loss: 116.37892872806842\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 9s 942us/step - loss: 0.0431 - val_loss: 0.0472\n",
      "\n",
      "Testing loss: 117.93661607343934\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 9s 942us/step - loss: 0.0419 - val_loss: 0.0405\n",
      "\n",
      "Testing loss: 116.34094581400511\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 9s 930us/step - loss: 0.0410 - val_loss: 0.0396\n",
      "\n",
      "Testing loss: 116.2929275694273\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0401 - val_loss: 0.0376\n",
      "\n",
      "Testing loss: 116.37247275132158\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 9s 879us/step - loss: 0.0390 - val_loss: 0.0383\n",
      "\n",
      "Testing loss: 116.93643601722388\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 8s 808us/step - loss: 0.0383 - val_loss: 0.0368\n",
      "\n",
      "Testing loss: 116.41449321485418\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 8s 810us/step - loss: 0.0375 - val_loss: 0.0382\n",
      "\n",
      "Testing loss: 116.00185596214047\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 9s 875us/step - loss: 0.0368 - val_loss: 0.0372\n",
      "\n",
      "Testing loss: 115.76597638417816\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 7s 713us/step - loss: 0.0362 - val_loss: 0.0374\n",
      "\n",
      "Testing loss: 117.71470724112879\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 7s 710us/step - loss: 0.0356 - val_loss: 0.0361\n",
      "\n",
      "Testing loss: 116.78123546126048\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0350 - val_loss: 0.0341\n",
      "\n",
      "Testing loss: 116.73982779119756\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 7s 760us/step - loss: 0.0344 - val_loss: 0.0358\n",
      "\n",
      "Testing loss: 117.49074257970786\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 8s 813us/step - loss: 0.0339 - val_loss: 0.0339\n",
      "\n",
      "Testing loss: 116.39572529250792\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 9s 928us/step - loss: 0.0335 - val_loss: 0.0340\n",
      "\n",
      "Testing loss: 116.66734881238659\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 9s 883us/step - loss: 0.0330 - val_loss: 0.0340\n",
      "\n",
      "Testing loss: 116.08650466737367\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 8s 789us/step - loss: 0.0325 - val_loss: 0.0333\n",
      "\n",
      "Testing loss: 116.02052276448535\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 7s 764us/step - loss: 0.0321 - val_loss: 0.0342\n",
      "\n",
      "Testing loss: 115.77372044816485\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 8s 786us/step - loss: 0.0317 - val_loss: 0.0327\n",
      "\n",
      "Testing loss: 116.4750150446526\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 8s 812us/step - loss: 0.0314 - val_loss: 0.0329\n",
      "\n",
      "Testing loss: 116.00820708088959\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0309 - val_loss: 0.0324\n",
      "\n",
      "Testing loss: 116.80737570792766\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.0305 - val_loss: 0.0310\n",
      "\n",
      "Testing loss: 116.36648796247468\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 7s 687us/step - loss: 0.0302 - val_loss: 0.0308\n",
      "\n",
      "Testing loss: 116.67206946346766\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0298 - val_loss: 0.0291\n",
      "\n",
      "Testing loss: 116.86807455721086\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 7s 763us/step - loss: 0.0294 - val_loss: 0.0306\n",
      "\n",
      "Testing loss: 116.21190523406645\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0292 - val_loss: 0.0308\n",
      "\n",
      "Testing loss: 116.50880678824142\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0289 - val_loss: 0.0305\n",
      "\n",
      "Testing loss: 116.86794020848828\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 7s 692us/step - loss: 0.0284 - val_loss: 0.0292\n",
      "\n",
      "Testing loss: 116.41838538945088\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 7s 689us/step - loss: 0.0282 - val_loss: 0.0304\n",
      "\n",
      "Testing loss: 116.29490193901954\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 7s 688us/step - loss: 0.0279 - val_loss: 0.0299\n",
      "\n",
      "Testing loss: 116.43627428783468\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.0277 - val_loss: 0.0316\n",
      "\n",
      "Testing loss: 115.81139765423636\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 7s 691us/step - loss: 0.0274 - val_loss: 0.0312\n",
      "\n",
      "Testing loss: 116.21044076599446\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 6s 635us/step - loss: 0.0271 - val_loss: 0.0305\n",
      "\n",
      "Testing loss: 116.20008716105828\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0267 - val_loss: 0.0317\n",
      "\n",
      "Testing loss: 117.48877816558324\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 6s 662us/step - loss: 0.0265 - val_loss: 0.0306\n",
      "\n",
      "Testing loss: 116.10717009558851\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 6s 661us/step - loss: 0.0264 - val_loss: 0.0294\n",
      "\n",
      "Testing loss: 117.02190712548007\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.0260 - val_loss: 0.0293\n",
      "\n",
      "Testing loss: 116.0791223249721\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 6s 649us/step - loss: 0.0258 - val_loss: 0.0281\n",
      "\n",
      "Testing loss: 116.17922510296583\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 6s 666us/step - loss: 0.0256 - val_loss: 0.0296\n",
      "\n",
      "Testing loss: 116.06436841586695\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 7s 694us/step - loss: 0.0253 - val_loss: 0.0293\n",
      "\n",
      "Testing loss: 115.7099768787708\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.0251 - val_loss: 0.0276\n",
      "\n",
      "Testing loss: 116.58855228791924\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0249 - val_loss: 0.0273\n",
      "\n",
      "Testing loss: 116.32556752176328\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 6s 666us/step - loss: 0.0247 - val_loss: 0.0297\n",
      "\n",
      "Testing loss: 115.65842166942899\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0245 - val_loss: 0.0277\n",
      "\n",
      "Testing loss: 116.19758148882103\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0242 - val_loss: 0.0285\n",
      "\n",
      "Testing loss: 115.889015877105\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0239 - val_loss: 0.0265\n",
      "\n",
      "Testing loss: 116.18913885781058\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.0237 - val_loss: 0.0278\n",
      "\n",
      "Testing loss: 116.8068878461848\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0236 - val_loss: 0.0273\n",
      "\n",
      "Testing loss: 115.88455686072223\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0234 - val_loss: 0.0263\n",
      "\n",
      "Testing loss: 116.48822929372932\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0232 - val_loss: 0.0262\n",
      "\n",
      "Testing loss: 116.44129876851743\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0229 - val_loss: 0.0258\n",
      "\n",
      "Testing loss: 116.80421052039722\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 7s 681us/step - loss: 0.0227 - val_loss: 0.0271\n",
      "\n",
      "Testing loss: 116.09694698867516\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 6s 643us/step - loss: 0.0226 - val_loss: 0.0249\n",
      "\n",
      "Testing loss: 116.91396732197269\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 6s 660us/step - loss: 0.0224 - val_loss: 0.0248\n",
      "\n",
      "Testing loss: 116.44477008980809\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.0222 - val_loss: 0.0273\n",
      "\n",
      "Testing loss: 116.06682845574693\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 7s 671us/step - loss: 0.0221 - val_loss: 0.0276\n",
      "\n",
      "Testing loss: 115.909449111256\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 6s 649us/step - loss: 0.0218 - val_loss: 0.0265\n",
      "\n",
      "Testing loss: 117.00886074812823\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 6s 665us/step - loss: 0.0216 - val_loss: 0.0264\n",
      "\n",
      "Testing loss: 116.14297688217695\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 6s 658us/step - loss: 0.0215 - val_loss: 0.0261\n",
      "\n",
      "Testing loss: 116.82086091477899\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.0213 - val_loss: 0.0268\n",
      "\n",
      "Testing loss: 116.43992699273649\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 7s 678us/step - loss: 0.0211 - val_loss: 0.0248\n",
      "\n",
      "Testing loss: 116.61267424197192\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0210 - val_loss: 0.0260\n",
      "\n",
      "Testing loss: 116.83383836773609\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0208 - val_loss: 0.0247\n",
      "\n",
      "Testing loss: 116.41586792092369\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 8s 811us/step - loss: 0.0206 - val_loss: 0.0255\n",
      "\n",
      "Testing loss: 116.14989052241485\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 8s 841us/step - loss: 0.0204 - val_loss: 0.0256\n",
      "\n",
      "Testing loss: 116.5268444676452\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 6s 656us/step - loss: 0.0204 - val_loss: 0.0248\n",
      "\n",
      "Testing loss: 116.24939088191368\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 7s 685us/step - loss: 0.0202 - val_loss: 0.0242\n",
      "\n",
      "Testing loss: 116.22309244975331\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 6s 657us/step - loss: 0.0200 - val_loss: 0.0264\n",
      "\n",
      "Testing loss: 116.60816724524422\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 6s 641us/step - loss: 0.0199 - val_loss: 0.0233\n",
      "\n",
      "Testing loss: 116.48991017738815\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 6s 637us/step - loss: 0.0197 - val_loss: 0.0254\n",
      "\n",
      "Testing loss: 115.94491704252836\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 7s 684us/step - loss: 0.0196 - val_loss: 0.0256\n",
      "\n",
      "Testing loss: 116.23997997210365\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 6s 626us/step - loss: 0.0194 - val_loss: 0.0267\n",
      "\n",
      "Testing loss: 117.36680010082196\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 7s 697us/step - loss: 0.0192 - val_loss: 0.0260\n",
      "\n",
      "Testing loss: 117.41451052108846\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 7s 669us/step - loss: 0.0191 - val_loss: 0.0267\n",
      "\n",
      "Testing loss: 115.92265406856295\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 7s 670us/step - loss: 0.0189 - val_loss: 0.0237\n",
      "\n",
      "Testing loss: 116.5036843745592\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 6s 634us/step - loss: 0.0188 - val_loss: 0.0254\n",
      "\n",
      "Testing loss: 115.74759502199569\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 6s 651us/step - loss: 0.0186 - val_loss: 0.0243\n",
      "\n",
      "Testing loss: 116.17392067954243\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 6s 662us/step - loss: 0.0185 - val_loss: 0.0248\n",
      "\n",
      "Testing loss: 116.03699076092532\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 6s 645us/step - loss: 0.0183 - val_loss: 0.0240\n",
      "\n",
      "Testing loss: 116.04810133203405\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 6s 655us/step - loss: 0.0182 - val_loss: 0.0232\n",
      "\n",
      "Testing loss: 116.46269028843257\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0181 - val_loss: 0.0223\n",
      "\n",
      "Testing loss: 116.22828626730524\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 6s 659us/step - loss: 0.0180 - val_loss: 0.0234\n",
      "\n",
      "Testing loss: 116.37831785055668\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 7s 751us/step - loss: 0.0178 - val_loss: 0.0234\n",
      "\n",
      "Testing loss: 116.66111747231403\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 8s 864us/step - loss: 0.0177 - val_loss: 0.0232\n",
      "\n",
      "Testing loss: 116.53101393645251\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 6s 664us/step - loss: 0.0175 - val_loss: 0.0256\n",
      "\n",
      "Testing loss: 115.85675033801004\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0174 - val_loss: 0.0238\n",
      "\n",
      "Testing loss: 116.35414100994475\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 6s 661us/step - loss: 0.0174 - val_loss: 0.0235\n",
      "\n",
      "Testing loss: 116.502159247009\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 7s 706us/step - loss: 0.0172 - val_loss: 0.0243\n",
      "\n",
      "Testing loss: 116.02802285064084\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 7s 752us/step - loss: 0.0170 - val_loss: 0.0249\n",
      "\n",
      "Testing loss: 116.72953313014328\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.0170 - val_loss: 0.0248\n",
      "\n",
      "Testing loss: 116.34177743366227\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0169 - val_loss: 0.0231\n",
      "\n",
      "Testing loss: 116.64525980865265\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0167 - val_loss: 0.0230\n",
      "\n",
      "Testing loss: 117.17847596341878\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 9s 890us/step - loss: 0.0166 - val_loss: 0.0231\n",
      "\n",
      "Testing loss: 115.97036484572357\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 7s 737us/step - loss: 0.0165 - val_loss: 0.0224\n",
      "\n",
      "Testing loss: 117.21657638330659\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 7s 688us/step - loss: 0.0164 - val_loss: 0.0224\n",
      "\n",
      "Testing loss: 116.49853391651253\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0163 - val_loss: 0.0231\n",
      "\n",
      "Testing loss: 116.25272947615858\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 7s 700us/step - loss: 0.0162 - val_loss: 0.0221\n",
      "\n",
      "Testing loss: 116.47035952361823\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0160 - val_loss: 0.0239\n",
      "\n",
      "Testing loss: 115.82778933168582\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 6s 667us/step - loss: 0.0159 - val_loss: 0.0223\n",
      "\n",
      "Testing loss: 116.31779585796446\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 7s 686us/step - loss: 0.0158 - val_loss: 0.0247\n",
      "\n",
      "Testing loss: 117.3462912927752\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 7s 668us/step - loss: 0.0157 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 117.29950964250278\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 6s 642us/step - loss: 0.0156 - val_loss: 0.0227\n",
      "\n",
      "Testing loss: 116.5656118788189\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 7s 672us/step - loss: 0.0155 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 116.25821575905194\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 6s 648us/step - loss: 0.0153 - val_loss: 0.0223\n",
      "\n",
      "Testing loss: 116.13490066872691\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 6s 638us/step - loss: 0.0153 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.67780030649315\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 7s 676us/step - loss: 0.0152 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.70294086767798\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 7s 690us/step - loss: 0.0151 - val_loss: 0.0219\n",
      "\n",
      "Testing loss: 116.58161571253798\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0149 - val_loss: 0.0229\n",
      "\n",
      "Testing loss: 116.1730241740293\n",
      "Epoch 117/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 7s 726us/step - loss: 0.0148 - val_loss: 0.0219\n",
      "\n",
      "Testing loss: 116.3290523799854\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 8s 787us/step - loss: 0.0147 - val_loss: 0.0244\n",
      "\n",
      "Testing loss: 115.54656230439697\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0146 - val_loss: 0.0204\n",
      "\n",
      "Testing loss: 116.47550310049577\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0146 - val_loss: 0.0233\n",
      "\n",
      "Testing loss: 116.06921471872435\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0144 - val_loss: 0.0236\n",
      "\n",
      "Testing loss: 115.86526118588829\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0144 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.57715404136385\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0143 - val_loss: 0.0221\n",
      "\n",
      "Testing loss: 116.01581017795918\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 16s 2ms/step - loss: 0.0141 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 116.29778134201607\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 14s 1ms/step - loss: 0.0141 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.39345723035935\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0140 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 116.84043595005265\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0139 - val_loss: 0.0222\n",
      "\n",
      "Testing loss: 116.75211422825916\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0138 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 117.08284027262403\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0137 - val_loss: 0.0224\n",
      "\n",
      "Testing loss: 116.656701846743\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 9s 893us/step - loss: 0.0136 - val_loss: 0.0203\n",
      "\n",
      "Testing loss: 116.62638428585912\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 9s 944us/step - loss: 0.0135 - val_loss: 0.0229\n",
      "\n",
      "Testing loss: 116.70859602410431\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 9s 890us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "\n",
      "Testing loss: 116.04455552631389\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 9s 911us/step - loss: 0.0133 - val_loss: 0.0224\n",
      "\n",
      "Testing loss: 116.4903089220425\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0133 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.45575104573734\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 8s 835us/step - loss: 0.0131 - val_loss: 0.0236\n",
      "\n",
      "Testing loss: 116.22019088175338\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 7s 766us/step - loss: 0.0131 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.58662244869541\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 7s 746us/step - loss: 0.0130 - val_loss: 0.0226\n",
      "\n",
      "Testing loss: 115.95974864811011\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 8s 843us/step - loss: 0.0129 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.5787684425351\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 8s 848us/step - loss: 0.0128 - val_loss: 0.0203\n",
      "\n",
      "Testing loss: 116.67271063941202\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 9s 928us/step - loss: 0.0127 - val_loss: 0.0219\n",
      "\n",
      "Testing loss: 116.75616408118373\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 9s 909us/step - loss: 0.0127 - val_loss: 0.0226\n",
      "\n",
      "Testing loss: 116.12542280683176\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 7s 734us/step - loss: 0.0126 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 116.77292031658737\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 7s 675us/step - loss: 0.0125 - val_loss: 0.0228\n",
      "\n",
      "Testing loss: 117.33485981123732\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 7s 685us/step - loss: 0.0125 - val_loss: 0.0213\n",
      "\n",
      "Testing loss: 116.61547725084232\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 6s 666us/step - loss: 0.0124 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.73462841987218\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 9s 961us/step - loss: 0.0123 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.32703636648424\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0122 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 116.32496945487215\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0121 - val_loss: 0.0223\n",
      "\n",
      "Testing loss: 116.08367459090167\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0120 - val_loss: 0.0222\n",
      "\n",
      "Testing loss: 116.25899590455855\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0119 - val_loss: 0.0241\n",
      "\n",
      "Testing loss: 115.75333191343738\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0119 - val_loss: 0.0196\n",
      "\n",
      "Testing loss: 116.25100121323966\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0118 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.11703927623574\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0118 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.7778797799268\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0117 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 116.55278410850903\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0117 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 116.50531773494804\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0116 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 116.5613094512977\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0115 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.78023470509575\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0114 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.36249295228028\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0113 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.49675024734641\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0113 - val_loss: 0.0204\n",
      "\n",
      "Testing loss: 116.45364035511291\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 8s 846us/step - loss: 0.0112 - val_loss: 0.0204\n",
      "\n",
      "Testing loss: 116.84006555656649\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0112 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 116.50321844615162\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0111 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 116.0521223954994\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0110 - val_loss: 0.0199\n",
      "\n",
      "Testing loss: 116.63235633295973\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0109 - val_loss: 0.0203\n",
      "\n",
      "Testing loss: 116.60431853033354\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0109 - val_loss: 0.0205\n",
      "\n",
      "Testing loss: 116.34934584938898\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0108 - val_loss: 0.0207\n",
      "\n",
      "Testing loss: 116.58237076901392\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0108 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.62485695433118\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0107 - val_loss: 0.0203\n",
      "\n",
      "Testing loss: 116.64205133675257\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0107 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.80244386318428\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0106 - val_loss: 0.0228\n",
      "\n",
      "Testing loss: 115.89854513175379\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "\n",
      "Testing loss: 116.54117335341824\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0105 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 116.99830786478437\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0104 - val_loss: 0.0215\n",
      "\n",
      "Testing loss: 116.73241923899253\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 15s 2ms/step - loss: 0.0103 - val_loss: 0.0196\n",
      "\n",
      "Testing loss: 117.01666439464947\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0103 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 116.40735573312715\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0102 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.4691416067719\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0102 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.47569974915729\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 10s 1ms/step - loss: 0.0102 - val_loss: 0.0209\n",
      "\n",
      "Testing loss: 116.49311884338854\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0101 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.50963058753439\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0101 - val_loss: 0.0214\n",
      "\n",
      "Testing loss: 116.5367479602186\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0100 - val_loss: 0.0223\n",
      "\n",
      "Testing loss: 116.38868481084661\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 13s 1ms/step - loss: 0.0099 - val_loss: 0.0213\n",
      "\n",
      "Testing loss: 116.44351519998293\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0098 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.63804750438591\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0098 - val_loss: 0.0196\n",
      "\n",
      "Testing loss: 116.73933952247016\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 11s 1ms/step - loss: 0.0097 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 116.39562216262911\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0097 - val_loss: 0.0193\n",
      "\n",
      "Testing loss: 116.43034149282626\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 8s 870us/step - loss: 0.0097 - val_loss: 0.0205\n",
      "\n",
      "Testing loss: 116.43045554241294\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0096 - val_loss: 0.0197\n",
      "\n",
      "Testing loss: 116.48702397461939\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 7s 762us/step - loss: 0.0096 - val_loss: 0.0183\n",
      "\n",
      "Testing loss: 116.91491599664074\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 7s 757us/step - loss: 0.0095 - val_loss: 0.0215\n",
      "\n",
      "Testing loss: 116.88110380364482\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 8s 781us/step - loss: 0.0095 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.69138432292601\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 8s 818us/step - loss: 0.0094 - val_loss: 0.0199\n",
      "\n",
      "Testing loss: 116.93827385916101\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 7s 749us/step - loss: 0.0094 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.61014487071266\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 8s 783us/step - loss: 0.0094 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.55127996257646\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 8s 858us/step - loss: 0.0093 - val_loss: 0.0186\n",
      "\n",
      "Testing loss: 116.62970871078122\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 7s 731us/step - loss: 0.0092 - val_loss: 0.0204\n",
      "\n",
      "Testing loss: 116.39332413389685\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 7s 765us/step - loss: 0.0092 - val_loss: 0.0213\n",
      "\n",
      "Testing loss: 116.43005310359136\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 8s 793us/step - loss: 0.0091 - val_loss: 0.0215\n",
      "\n",
      "Testing loss: 116.30288083933884\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 9s 955us/step - loss: 0.0091 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.4420689397725\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 9s 933us/step - loss: 0.0090 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.47533558566501\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 9s 912us/step - loss: 0.0090 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.23769052761423\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 7s 758us/step - loss: 0.0090 - val_loss: 0.0196\n",
      "\n",
      "Testing loss: 116.679132623559\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 7s 711us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "\n",
      "Testing loss: 117.3835718579893\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 7s 756us/step - loss: 0.0089 - val_loss: 0.0205\n",
      "\n",
      "Testing loss: 116.34493277216721\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 9s 930us/step - loss: 0.0088 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.39921968669054\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 7s 753us/step - loss: 0.0088 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.77622064802556\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.0088 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.79076722669073\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 8s 849us/step - loss: 0.0087 - val_loss: 0.0186\n",
      "\n",
      "Testing loss: 116.72704732501218\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 8s 805us/step - loss: 0.0087 - val_loss: 0.0188\n",
      "\n",
      "Testing loss: 116.62089162896196\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 8s 863us/step - loss: 0.0086 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.72256080273678\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 10s 981us/step - loss: 0.0086 - val_loss: 0.0195\n",
      "\n",
      "Testing loss: 116.86487214624955\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 9s 884us/step - loss: 0.0085 - val_loss: 0.0190\n",
      "\n",
      "Testing loss: 116.68452100475939\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 8s 816us/step - loss: 0.0085 - val_loss: 0.0201\n",
      "\n",
      "Testing loss: 116.70159381151102\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 8s 815us/step - loss: 0.0085 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 117.19538817407657\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 7s 701us/step - loss: 0.0084 - val_loss: 0.0187\n",
      "\n",
      "Testing loss: 116.59167181178591\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 7s 674us/step - loss: 0.0084 - val_loss: 0.0191\n",
      "\n",
      "Testing loss: 116.91431640199231\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 7s 720us/step - loss: 0.0084 - val_loss: 0.0199\n",
      "\n",
      "Testing loss: 116.40603476779893\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 7s 679us/step - loss: 0.0084 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.27813110852466\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 7s 714us/step - loss: 0.0083 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.8790110266008\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 7s 677us/step - loss: 0.0083 - val_loss: 0.0213\n",
      "\n",
      "Testing loss: 116.49297240409491\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 7s 721us/step - loss: 0.0082 - val_loss: 0.0211\n",
      "\n",
      "Testing loss: 116.44347226624612\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 7s 742us/step - loss: 0.0082 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 116.16612945544118\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 7s 695us/step - loss: 0.0082 - val_loss: 0.0204\n",
      "\n",
      "Testing loss: 116.4554350253838\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 8s 804us/step - loss: 0.0081 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.8562359909665\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 9s 874us/step - loss: 0.0081 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.65129965311394\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 9s 881us/step - loss: 0.0081 - val_loss: 0.0218\n",
      "\n",
      "Testing loss: 116.50743171901648\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 8s 829us/step - loss: 0.0080 - val_loss: 0.0216\n",
      "\n",
      "Testing loss: 116.41791834653182\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 8s 784us/step - loss: 0.0080 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.36344521467345\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 8s 786us/step - loss: 0.0080 - val_loss: 0.0215\n",
      "\n",
      "Testing loss: 116.41000446981712\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 9s 957us/step - loss: 0.0079 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.39357461721916\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 8s 820us/step - loss: 0.0079 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.63082917048689\n",
      "Epoch 233/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 12s 1ms/step - loss: 0.0079 - val_loss: 0.0204\n",
      "\n",
      "Testing loss: 116.6047821984117\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 10s 997us/step - loss: 0.0078 - val_loss: 0.0198\n",
      "\n",
      "Testing loss: 116.5870820907574\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 8s 813us/step - loss: 0.0078 - val_loss: 0.0207\n",
      "\n",
      "Testing loss: 116.3749682990361\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 8s 785us/step - loss: 0.0078 - val_loss: 0.0202\n",
      "\n",
      "Testing loss: 116.4347351725394\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 8s 834us/step - loss: 0.0077 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 116.5001489500751\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 9s 877us/step - loss: 0.0077 - val_loss: 0.0213\n",
      "\n",
      "Testing loss: 116.6932070813905\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 9s 882us/step - loss: 0.0077 - val_loss: 0.0212\n",
      "\n",
      "Testing loss: 116.30333716291162\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 8s 838us/step - loss: 0.0076 - val_loss: 0.0200\n",
      "\n",
      "Testing loss: 116.4311823854693\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 8s 865us/step - loss: 0.0076 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.61390099977439\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 8s 794us/step - loss: 0.0076 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 116.7025035408906\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 8s 790us/step - loss: 0.0075 - val_loss: 0.0204\n",
      "\n",
      "Testing loss: 116.68031389301412\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 7s 767us/step - loss: 0.0075 - val_loss: 0.0192\n",
      "\n",
      "Testing loss: 117.22564793689651\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 7s 769us/step - loss: 0.0075 - val_loss: 0.0180\n",
      "\n",
      "Testing loss: 117.01765806302214\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 8s 839us/step - loss: 0.0075 - val_loss: 0.0205\n",
      "\n",
      "Testing loss: 116.82086698824038\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 9s 946us/step - loss: 0.0074 - val_loss: 0.0197\n",
      "\n",
      "Testing loss: 116.68456269249742\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 9s 934us/step - loss: 0.0074 - val_loss: 0.0208\n",
      "\n",
      "Testing loss: 116.23295216331435\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 9s 886us/step - loss: 0.0074 - val_loss: 0.0196\n",
      "\n",
      "Testing loss: 116.97838316092884\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 9s 898us/step - loss: 0.0073 - val_loss: 0.0210\n",
      "\n",
      "Testing loss: 116.20511092169147\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.001)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "historyh = model.fit(X_train_scaled.values,\n",
    "                     y_train_scaled, epochs=250, \n",
    "                     verbose=1, \n",
    "                     validation_data=(X_val_scaled.values, y_val_scaled),\n",
    "                     callbacks=[TestCallback((X_test_scaled.values, yTest))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss[-1] = model.evaluate(X_test_scaled.values, yTest, verbose=0)\n",
    "train_loss=historyh.history['loss']\n",
    "xc = range(numEpochs)\n",
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,test_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.title('Training Loss vs Testing Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
