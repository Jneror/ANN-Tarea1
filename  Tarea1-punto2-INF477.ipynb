{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n",
      "       Unnamed: 0           0          1          2          3          4  \\\n",
      "0               0   73.516695  17.817765  12.469551  12.458130  12.454607   \n",
      "1               1   73.516695  20.649126  18.527789  17.891535  17.887995   \n",
      "2               2   73.516695  17.830377  12.512263  12.404775  12.394493   \n",
      "3               3   73.516695  17.875810  17.871259  17.862402  17.850920   \n",
      "4               4   73.516695  17.883818  17.868256  17.864221  17.818540   \n",
      "5               5   53.358707  17.038820  16.981436  16.167446  16.137631   \n",
      "6               6   53.358707  17.040919  16.975955  16.168874  16.131888   \n",
      "7               7   53.358707  15.190748  15.134397  15.078282  13.721467   \n",
      "8               8   73.516695  20.648642  18.559611  17.674347  16.152675   \n",
      "9               9   73.516695  17.563342  17.562598  12.653657  12.540799   \n",
      "10             10   73.516695  18.593826  17.902116  17.791648  17.709349   \n",
      "11             11   73.516695  17.893322  17.892435  17.891329  17.835056   \n",
      "12             12   73.516695  17.892036  17.835452  17.796401  12.544441   \n",
      "13             13  388.023441  66.102901  35.415029  35.414463  21.068417   \n",
      "14             14   73.516695  19.145558  17.855414  17.779134  12.927104   \n",
      "15             15   73.516695  20.686211  18.654076  18.029737  16.104124   \n",
      "16             16   73.516695  23.621854  23.620385  20.741845  18.712919   \n",
      "17             17   73.516695  20.882498  20.704776  18.830990  18.239768   \n",
      "18             18   53.358707  17.152774  16.979009  16.165980  16.131888   \n",
      "19             19   73.516695  20.792001  20.781995  18.699353  15.970705   \n",
      "20             20   73.516695  20.752137  19.148052  18.575154  17.872719   \n",
      "21             21  388.023441  29.794358  29.450794  19.838451  19.640707   \n",
      "22             22   73.516695  20.707815  17.725040  17.690824  12.899468   \n",
      "23             23   53.358707  17.139133  16.062083  15.810563  15.258986   \n",
      "24             24   53.358707  15.192917  15.192453  15.191957  13.696268   \n",
      "25             25   73.516695  20.722774  18.596453  17.866674  17.743940   \n",
      "26             26   73.516695  20.765497  19.143140  18.645819  17.736610   \n",
      "27             27   73.516695  20.800040  12.574005  12.558918  12.517330   \n",
      "28             28  388.023441  46.500552  46.493093  34.676868  29.085904   \n",
      "29             29   53.358707  18.092880  15.249478  15.177545  13.653715   \n",
      "...           ...         ...        ...        ...        ...        ...   \n",
      "16212       16243  388.023441  29.658517  29.655842  23.552305  23.550885   \n",
      "16213       16244   73.516695  18.635498  17.804251  15.228614  13.653764   \n",
      "16214       16245   53.358707  19.148633  13.653248  13.652988  13.652850   \n",
      "16215       16246   73.516695  20.698171  15.924907  15.855703  13.654009   \n",
      "16216       16247   73.516695  18.643484  18.640762  17.802607  17.802277   \n",
      "16217       16248   73.516695  17.870020  15.238941  15.176526  15.176523   \n",
      "16218       16249   53.358707  19.150693  16.405490  15.022080  14.895449   \n",
      "16219       16250   36.858105  14.107138  12.674530  12.544437  12.505901   \n",
      "16220       16251   73.516695  20.698855  20.698855  15.893020  15.892046   \n",
      "16221       16252   53.358707  15.754498  15.753712  13.653954  13.653736   \n",
      "16222       16253   73.516695  17.677578  12.519029  12.479941  12.477739   \n",
      "16223       16254   73.516695  20.743557  20.741361  18.598127  18.595527   \n",
      "16224       16255   73.516695  20.801457  20.801457  18.634187  18.634183   \n",
      "16225       16256   73.516695  20.581963  15.924696  15.751649  15.452521   \n",
      "16226       16257   53.358707  16.359301  16.358704  15.148839  13.825358   \n",
      "16227       16258   53.358707  16.435036  16.433452  15.192854  15.192072   \n",
      "16228       16259   36.858105  14.159026  13.653760  13.653143  13.653143   \n",
      "16229       16260   53.358707  16.525299  16.435115  14.202458  13.827072   \n",
      "16230       16261   73.516695  20.731531  20.731285  16.422832  16.422798   \n",
      "16231       16262   73.516695  20.221994  18.690270  18.632741  18.405014   \n",
      "16232       16263   73.516695  17.860880  17.780884  17.779987  12.511356   \n",
      "16233       16264  388.023441  46.723458  46.722364  28.505545  28.472771   \n",
      "16234       16265   73.516695  23.607539  23.606041  21.378625  17.851697   \n",
      "16235       16266   73.516695  20.643181  20.588968  18.703556  18.673772   \n",
      "16236       16267   73.516695  20.610314  20.554562  18.683667  18.653061   \n",
      "16237       16268   73.516695  20.753166  18.624076  17.872009  17.851690   \n",
      "16238       16269   73.516695  20.724740  18.579933  17.741621  14.716676   \n",
      "16239       16270   53.358707  20.820797  19.150234  19.148721  15.135514   \n",
      "16240       16271   53.358707  15.707759  15.707644  13.653838  13.653570   \n",
      "16241       16272   53.358707  15.708752  15.708094  13.653893  13.653176   \n",
      "\n",
      "               5          6          7          8    ...      1267  1268  \\\n",
      "0      12.447345  12.433065  12.426926  12.387474    ...       0.0   0.0   \n",
      "1      17.871731  17.852586  17.729842  15.864270    ...       0.0   0.0   \n",
      "2      12.391564  12.324461  12.238106  10.423249    ...       0.0   0.0   \n",
      "3      17.850440  12.558105  12.557645  12.517583    ...       0.0   0.0   \n",
      "4      12.508657  12.490519  12.450098  10.597068    ...       0.0   0.0   \n",
      "5      16.053239  15.713944  15.432893  15.421116    ...       0.0   0.0   \n",
      "6      16.073074  15.843838  15.638061  15.160532    ...       0.0   0.0   \n",
      "7      13.720334  13.671396  13.655370  13.654554    ...       0.0   0.0   \n",
      "8      14.266867  13.666125  13.657868  13.642132    ...       0.0   0.0   \n",
      "9      12.539160  12.536825  12.508203  12.489843    ...       0.0   0.0   \n",
      "10     17.659515  17.569676  17.188025  17.152301    ...       0.0   0.0   \n",
      "11     17.797049  17.796887  17.796494  12.519782    ...       0.0   0.0   \n",
      "12     12.520178  12.520042  12.501077  12.205003    ...       0.0   0.0   \n",
      "13     20.972084  18.855496  18.854617  18.833136    ...       0.0   0.0   \n",
      "14     12.337087  12.336829  12.276143  12.205558    ...       0.0   0.0   \n",
      "15     15.732605  15.382404  14.561514  13.653314    ...       0.0   0.0   \n",
      "16     15.755895  15.645016  15.533408  15.392356    ...       0.0   0.0   \n",
      "17     18.026817  14.817389  14.804453  14.229886    ...       0.0   0.0   \n",
      "18     16.122930  16.090475  15.157053  12.520047    ...       0.0   0.0   \n",
      "19     15.420394  14.874331  12.647200  12.647035    ...       0.0   0.0   \n",
      "20     17.850143  17.739271  14.677496  12.976874    ...       0.0   0.0   \n",
      "21     18.080196  15.228163  15.179768  15.179574    ...       0.0   0.0   \n",
      "22     12.809439  12.740222  12.667372  12.576495    ...       0.0   0.0   \n",
      "23     15.258296  13.676434  13.675309  13.654199    ...       0.0   0.0   \n",
      "24     13.695720  13.655091  13.653141  13.653066    ...       0.0   0.0   \n",
      "25     14.701212  13.740966  13.738704  13.654970    ...       0.0   0.0   \n",
      "26     17.699663  17.687518  14.701643  14.216043    ...       0.0   0.0   \n",
      "27     12.384754  12.355934  12.344451  12.316290    ...       0.0   0.0   \n",
      "28     19.000431  18.995270  18.631019  18.630794    ...       0.0   0.0   \n",
      "29     13.653147  13.652852  13.652687  13.652476    ...       0.0   0.0   \n",
      "...          ...        ...        ...        ...    ...       ...   ...   \n",
      "16212  23.446952  23.446603  23.424316  22.152848    ...       0.0   0.0   \n",
      "16213  13.653754  13.653413  13.652802  13.652570    ...       0.0   0.0   \n",
      "16214  13.652754  13.652495  13.652167  12.965136    ...       0.0   0.0   \n",
      "16215  13.653321  13.653014  13.652836  13.652436    ...       0.0   0.0   \n",
      "16216  13.653867  13.653335  13.652740  13.652676    ...       0.0   0.0   \n",
      "16217  12.525824  12.525722  12.507239  12.484568    ...       0.0   0.0   \n",
      "16218  12.521816  12.521367  12.488777  12.488520    ...       0.0   0.0   \n",
      "16219  12.480945  12.469779  12.345929  12.286274    ...       0.0   0.0   \n",
      "16220  15.875667  15.875528  13.653570  13.652995    ...       0.0   0.0   \n",
      "16221  13.653736  13.653482  13.653142  13.653000    ...       0.0   0.0   \n",
      "16222  12.452035  12.426147  12.421406  12.308018    ...       0.0   0.0   \n",
      "16223  14.582338  14.576939  12.993091  12.611977    ...       0.0   0.0   \n",
      "16224  14.566950  14.566947  14.254289  14.252941    ...       0.0   0.0   \n",
      "16225  14.852202  13.654823  13.653847  13.653692    ...       0.0   0.0   \n",
      "16226  13.825283  13.654598  13.653577  13.653275    ...       0.0   0.0   \n",
      "16227  15.191944  13.836318  13.787761  13.744369    ...       0.0   0.0   \n",
      "16228  13.652803  13.652756  13.652713  13.092874    ...       0.0   0.0   \n",
      "16229  13.743711  13.695585  13.694933  12.759605    ...       0.0   0.0   \n",
      "16230  16.136916  16.135040  15.893446  15.853311    ...       0.0   0.0   \n",
      "16231  17.735798  14.707738  13.842602  13.689474    ...       0.0   0.0   \n",
      "16232  12.510997  12.481791  12.475366  10.571260    ...       0.0   0.0   \n",
      "16233  19.796532  19.731292  14.209230  14.144322    ...       0.0   0.0   \n",
      "16234  15.649065  15.422739  13.654045  13.653121    ...       0.0   0.0   \n",
      "16235  14.895437  14.894662  13.684300  13.683890    ...       0.0   0.0   \n",
      "16236  14.895197  14.894752  13.684066  13.683760    ...       0.0   0.0   \n",
      "16237  17.851254  17.742176  14.655754  12.706683    ...       0.0   0.0   \n",
      "16238  13.697829  13.697558  13.653512  13.652942    ...       0.0   0.0   \n",
      "16239  15.123685  12.942704  12.938162  12.488633    ...       0.0   0.0   \n",
      "16240  13.653314  13.652591  13.652585  13.652550    ...       0.0   0.0   \n",
      "16241  13.653120  13.652930  13.652528  13.652322    ...       0.0   0.0   \n",
      "\n",
      "       1269  1270  1271  1272  1273  1274  pubchem_id        Eat  \n",
      "0       0.5   0.0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
      "1       0.0   0.0   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
      "2       0.0   0.0   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
      "3       0.0   0.0   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
      "4       0.0   0.0   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
      "5       0.0   0.0   0.0   0.0   0.0   0.0       25017 -16.169604  \n",
      "6       0.0   0.0   0.0   0.0   0.0   0.0       25019 -17.378477  \n",
      "7       0.0   0.0   0.0   0.0   0.0   0.0       25023 -15.673737  \n",
      "8       0.0   0.0   0.0   0.0   0.0   0.0       25032 -10.427851  \n",
      "9       0.0   0.0   0.0   0.0   0.0   0.0       25042  -8.744178  \n",
      "10      0.0   0.0   0.0   0.0   0.0   0.0       25051 -12.244325  \n",
      "11      0.0   0.0   0.0   0.0   0.0   0.0       25054 -14.149376  \n",
      "12      0.0   0.0   0.0   0.0   0.0   0.0       25055  -8.572029  \n",
      "13      0.0   0.0   0.0   0.0   0.0   0.0       25057 -13.696046  \n",
      "14      0.0   0.0   0.0   0.0   0.0   0.0       25066  -8.293733  \n",
      "15      0.0   0.0   0.0   0.0   0.0   0.0       25072 -11.248055  \n",
      "16      0.0   0.0   0.0   0.0   0.0   0.0       25089 -10.170327  \n",
      "17      0.0   0.0   0.0   0.0   0.0   0.0       25090 -11.116150  \n",
      "18      0.0   0.0   0.0   0.0   0.0   0.0       25093 -11.642478  \n",
      "19      0.0   0.0   0.0   0.0   0.0   0.0       25098  -8.329572  \n",
      "20      0.0   0.0   0.0   0.0   0.0   0.0       25106  -8.170857  \n",
      "21      0.0   0.0   0.0   0.0   0.0   0.0       25118 -12.796349  \n",
      "22      0.0   0.0   0.0   0.0   0.0   0.0       25121 -10.500986  \n",
      "23      0.0   0.0   0.0   0.0   0.0   0.0       25123 -11.520089  \n",
      "24      0.0   0.0   0.0   0.0   0.0   0.0       25125 -11.197458  \n",
      "25      0.0   0.0   0.0   0.0   0.0   0.0       25129 -13.505988  \n",
      "26      0.0   0.0   0.0   0.0   0.0   0.0       25131 -14.299903  \n",
      "27      0.0   0.0   0.0   0.0   0.0   0.0       25132 -11.331558  \n",
      "28      0.0   0.0   0.0   0.0   0.0   0.0       25141  -9.290619  \n",
      "29      0.0   0.0   0.0   0.0   0.0   0.0       25142  -9.586835  \n",
      "...     ...   ...   ...   ...   ...   ...         ...        ...  \n",
      "16212   0.0   0.0   0.0   0.0   0.0   0.0       74895  -6.849553  \n",
      "16213   0.0   0.0   0.0   0.0   0.0   0.0       74896 -10.408032  \n",
      "16214   0.0   0.0   0.0   0.0   0.0   0.0       74897 -10.100940  \n",
      "16215   0.0   0.0   0.0   0.0   0.0   0.0       74903 -11.170072  \n",
      "16216   0.0   0.0   0.0   0.0   0.0   0.0       74904 -11.163202  \n",
      "16217   0.0   0.0   0.0   0.0   0.0   0.0       74905 -15.003575  \n",
      "16218   0.0   0.0   0.0   0.0   0.0   0.0       74906 -11.932200  \n",
      "16219   0.0   0.0   0.0   0.0   0.0   0.0       74910 -11.987681  \n",
      "16220   0.0   0.0   0.0   0.0   0.0   0.0       74911 -11.908604  \n",
      "16221   0.0   0.0   0.0   0.0   0.0   0.0       74912 -12.330795  \n",
      "16222   0.0   0.0   0.0   0.0   0.0   0.0       74917 -13.007444  \n",
      "16223   0.0   0.0   0.0   0.0   0.0   0.0       74918 -15.191880  \n",
      "16224   0.0   0.0   0.0   0.0   0.0   0.0       74919  -6.746408  \n",
      "16225   0.0   0.0   0.0   0.0   0.0   0.0       74921 -11.988657  \n",
      "16226   0.0   0.0   0.0   0.0   0.0   0.0       74922 -10.752393  \n",
      "16227   0.0   0.0   0.0   0.0   0.0   0.0       74927 -12.044004  \n",
      "16228   0.0   0.0   0.0   0.0   0.0   0.0       74928 -10.988833  \n",
      "16229   0.0   0.0   0.0   0.0   0.0   0.0       74935 -10.635767  \n",
      "16230   0.0   0.0   0.0   0.0   0.0   0.0       74947  -8.633728  \n",
      "16231   0.0   0.0   0.0   0.0   0.0   0.0       74953 -11.206702  \n",
      "16232   0.0   0.0   0.0   0.0   0.0   0.0       74956  -6.823134  \n",
      "16233   0.0   0.0   0.0   0.0   0.0   0.0       74963 -11.964498  \n",
      "16234   0.0   0.0   0.0   0.0   0.0   0.0       74968  -8.918597  \n",
      "16235   0.0   0.0   0.0   0.0   0.0   0.0       74971 -11.794917  \n",
      "16236   0.0   0.0   0.0   0.0   0.0   0.0       74974 -11.809176  \n",
      "16237   0.0   0.0   0.0   0.0   0.0   0.0       74976  -8.876123  \n",
      "16238   0.0   0.0   0.0   0.0   0.0   0.0       74977 -13.105268  \n",
      "16239   0.0   0.0   0.0   0.0   0.0   0.0       74978 -16.801464  \n",
      "16240   0.0   0.0   0.0   0.0   0.0   0.0       74979 -13.335088  \n",
      "16241   0.0   0.0   0.0   0.0   0.0   0.0       74980 -13.336696  \n",
      "\n",
      "[16242 rows x 1278 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "print(datos)\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_train)\n",
    "#Transform training\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Transform val\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "y_val_scaled = X_val_scaled.pop('Eat').values.reshape(-1,1)\n",
    "#Transform test\n",
    "X_Test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "y_Test_scaled = X_Test_scaled.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 1.4365 - val_loss: 0.5244\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.6070 - val_loss: 0.4590\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.4867 - val_loss: 0.3586\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 4s 372us/step - loss: 0.4118 - val_loss: 0.3237\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 384us/step - loss: 0.3563 - val_loss: 0.2786\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.3105 - val_loss: 0.3024\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.2740 - val_loss: 0.2355\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.2438 - val_loss: 0.2334\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.2178 - val_loss: 0.2190\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 5s 467us/step - loss: 0.1968 - val_loss: 0.1741\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 0.1802 - val_loss: 0.1613\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.1607 - val_loss: 0.3107\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.1449 - val_loss: 0.1411\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.1334 - val_loss: 0.1351\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.1222 - val_loss: 0.1671\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.1121 - val_loss: 0.1236\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.1027 - val_loss: 0.1252\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.0971 - val_loss: 0.1078\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.0899 - val_loss: 0.0992\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0872 - val_loss: 0.1111\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0819 - val_loss: 0.1282\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 0.0794 - val_loss: 0.0895\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 4s 423us/step - loss: 0.0737 - val_loss: 0.0991\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0716 - val_loss: 0.0909\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0706 - val_loss: 0.1399\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 377us/step - loss: 0.0679 - val_loss: 0.0806\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0651 - val_loss: 0.0760\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0636 - val_loss: 0.0736\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0615 - val_loss: 0.0726\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0590 - val_loss: 0.0741\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0583 - val_loss: 0.0697\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 4s 424us/step - loss: 0.0549 - val_loss: 0.0860\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0546 - val_loss: 0.0707\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0542 - val_loss: 0.0795\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0528 - val_loss: 0.0878\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0509 - val_loss: 0.0944\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 4s 448us/step - loss: 0.0505 - val_loss: 0.0652\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 4s 457us/step - loss: 0.0493 - val_loss: 0.0608\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 5s 464us/step - loss: 0.0488 - val_loss: 0.0682\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0474 - val_loss: 0.0988\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0475 - val_loss: 0.0781\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0446 - val_loss: 0.0863\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 5s 467us/step - loss: 0.0448 - val_loss: 0.0626\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 4s 452us/step - loss: 0.0459 - val_loss: 0.0635\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 5s 542us/step - loss: 0.0424 - val_loss: 0.0618\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 5s 546us/step - loss: 0.0429 - val_loss: 0.0614\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0419 - val_loss: 0.0583\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0433 - val_loss: 0.0624\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0427 - val_loss: 0.0553\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 5s 547us/step - loss: 0.0408 - val_loss: 0.0609\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 5s 530us/step - loss: 0.0406 - val_loss: 0.0557\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0400 - val_loss: 0.0628\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 4s 429us/step - loss: 0.0402 - val_loss: 0.0588\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0388 - val_loss: 0.0599\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0367 - val_loss: 0.0623\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0380 - val_loss: 0.0917\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0365 - val_loss: 0.0720\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0354 - val_loss: 0.0619\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0368 - val_loss: 0.0660\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0365 - val_loss: 0.0503\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0354 - val_loss: 0.0509\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 4s 429us/step - loss: 0.0350 - val_loss: 0.0552\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0352 - val_loss: 0.0560\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 5s 539us/step - loss: 0.0343 - val_loss: 0.0498\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 6s 588us/step - loss: 0.0337 - val_loss: 0.0725\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 0.0339 - val_loss: 0.0505\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 4s 437us/step - loss: 0.0347 - val_loss: 0.1394\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 4s 437us/step - loss: 0.0331 - val_loss: 0.0525\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.0332 - val_loss: 0.0531\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.0321 - val_loss: 0.0501\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 4s 394us/step - loss: 0.0334 - val_loss: 0.0614\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0310 - val_loss: 0.0482\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 5s 485us/step - loss: 0.0313 - val_loss: 0.0505\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 5s 515us/step - loss: 0.0319 - val_loss: 0.0561\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.0311 - val_loss: 0.0468\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0303 - val_loss: 0.0629\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 4s 426us/step - loss: 0.0306 - val_loss: 0.0468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.0305 - val_loss: 0.0509\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 0.0294 - val_loss: 0.0477\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0303 - val_loss: 0.0507\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: 0.0294 - val_loss: 0.0498\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 4s 386us/step - loss: 0.0281 - val_loss: 0.0469\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0282 - val_loss: 0.0500\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0286 - val_loss: 0.0528\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0302 - val_loss: 0.0455\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 4s 448us/step - loss: 0.0284 - val_loss: 0.0482\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0281 - val_loss: 0.0548\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.0283 - val_loss: 0.0702\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.0282 - val_loss: 0.0544\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0279 - val_loss: 0.0529\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0269 - val_loss: 0.0488\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0280 - val_loss: 0.0464\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 5s 470us/step - loss: 0.0276 - val_loss: 0.0459\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0270 - val_loss: 0.0531\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 5s 525us/step - loss: 0.0266 - val_loss: 0.0443\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0258 - val_loss: 0.0647\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 4s 442us/step - loss: 0.0266 - val_loss: 0.0464\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0257 - val_loss: 0.0435\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.0260 - val_loss: 0.0481\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0260 - val_loss: 0.0446\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0255 - val_loss: 0.0486\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 4s 383us/step - loss: 0.0253 - val_loss: 0.0430\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: 0.0256 - val_loss: 0.0449\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0248 - val_loss: 0.0424\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0248 - val_loss: 0.0430\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0257 - val_loss: 0.0577\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.0245 - val_loss: 0.0435\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 4s 396us/step - loss: 0.0249 - val_loss: 0.0459\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0246 - val_loss: 0.0562\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.0240 - val_loss: 0.0487\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0243 - val_loss: 0.0419\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0239 - val_loss: 0.0567\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 5s 470us/step - loss: 0.0237 - val_loss: 0.0570\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 4s 452us/step - loss: 0.0232 - val_loss: 0.0446\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.0239 - val_loss: 0.0442\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0234 - val_loss: 0.0492\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0227 - val_loss: 0.0532\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 0.0235 - val_loss: 0.0431\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0232 - val_loss: 0.0531\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.0235 - val_loss: 0.0492\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.0228 - val_loss: 0.0485\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 6s 566us/step - loss: 0.0220 - val_loss: 0.0471\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 4s 442us/step - loss: 0.0227 - val_loss: 0.0579\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 5s 557us/step - loss: 0.0222 - val_loss: 0.0552\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 4s 448us/step - loss: 0.0221 - val_loss: 0.0531\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0223 - val_loss: 0.0444\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0228 - val_loss: 0.0439\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0221 - val_loss: 0.0810\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0220 - val_loss: 0.0488\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.0220 - val_loss: 0.0416\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 4s 426us/step - loss: 0.0215 - val_loss: 0.0419\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0220 - val_loss: 0.0469\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 5s 524us/step - loss: 0.0211 - val_loss: 0.0553\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 5s 499us/step - loss: 0.0208 - val_loss: 0.0404\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0208 - val_loss: 0.0453\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 5s 514us/step - loss: 0.0215 - val_loss: 0.0430\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 4s 456us/step - loss: 0.0209 - val_loss: 0.0411\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 0.0208 - val_loss: 0.0403\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0213 - val_loss: 0.0413\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0208 - val_loss: 0.0555\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0206 - val_loss: 0.0455\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0206 - val_loss: 0.0456\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 5s 477us/step - loss: 0.0205 - val_loss: 0.0412\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 4s 452us/step - loss: 0.0199 - val_loss: 0.0416\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.0201 - val_loss: 0.0431\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0201 - val_loss: 0.0496\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.0209 - val_loss: 0.0421\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0197 - val_loss: 0.0408\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 4s 394us/step - loss: 0.0201 - val_loss: 0.0400\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.0201 - val_loss: 0.0421\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0195 - val_loss: 0.0447\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 5s 479us/step - loss: 0.0198 - val_loss: 0.0437\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.0196 - val_loss: 0.0435\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0197 - val_loss: 0.0491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 5s 492us/step - loss: 0.0202 - val_loss: 0.0433\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0185 - val_loss: 0.0469\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0196 - val_loss: 0.0512\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0200 - val_loss: 0.0401\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0188 - val_loss: 0.0405\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 4s 440us/step - loss: 0.0191 - val_loss: 0.0394\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 4s 396us/step - loss: 0.0187 - val_loss: 0.0422\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0195 - val_loss: 0.0451\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0189 - val_loss: 0.0416\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0186 - val_loss: 0.0442\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 0.0184 - val_loss: 0.0450\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 4s 458us/step - loss: 0.0185 - val_loss: 0.0480\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 5s 509us/step - loss: 0.0185 - val_loss: 0.0482\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 0.0183 - val_loss: 0.0492\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 4s 429us/step - loss: 0.0186 - val_loss: 0.0401\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.0183 - val_loss: 0.0487\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0183 - val_loss: 0.0391\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0187 - val_loss: 0.0550\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0177 - val_loss: 0.0424\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0179 - val_loss: 0.0407\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.0181 - val_loss: 0.0404\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0181 - val_loss: 0.0561\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0187 - val_loss: 0.0418\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0178 - val_loss: 0.0429\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 4s 426us/step - loss: 0.0177 - val_loss: 0.0391\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 0.0180 - val_loss: 0.0411\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 5s 560us/step - loss: 0.0177 - val_loss: 0.0428\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0175 - val_loss: 0.0456\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 5s 470us/step - loss: 0.0175 - val_loss: 0.0419\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0175 - val_loss: 0.0403\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 7s 757us/step - loss: 0.0169 - val_loss: 0.0452\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0177 - val_loss: 0.0415\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 4s 403us/step - loss: 0.0174 - val_loss: 0.0429\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 4s 424us/step - loss: 0.0175 - val_loss: 0.0389\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0170 - val_loss: 0.0455\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 4s 405us/step - loss: 0.0170 - val_loss: 0.0374\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0168 - val_loss: 0.0376\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0175 - val_loss: 0.0407\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0175 - val_loss: 0.0433\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.0177 - val_loss: 0.0381\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.0167 - val_loss: 0.0375\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 4s 454us/step - loss: 0.0165 - val_loss: 0.0381\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0169 - val_loss: 0.0375\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0169 - val_loss: 0.0454\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0162 - val_loss: 0.0466\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.0170 - val_loss: 0.0378\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0165 - val_loss: 0.0374\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.0162 - val_loss: 0.0398\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 4s 448us/step - loss: 0.0170 - val_loss: 0.0477\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0163 - val_loss: 0.0431\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0166 - val_loss: 0.0378\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.0161 - val_loss: 0.0397\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0164 - val_loss: 0.0398\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0161 - val_loss: 0.0377\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 4s 423us/step - loss: 0.0163 - val_loss: 0.0366\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 4s 372us/step - loss: 0.0162 - val_loss: 0.0384\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 440us/step - loss: 0.0162 - val_loss: 0.0587\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0166 - val_loss: 0.0377\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.0160 - val_loss: 0.0376\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0160 - val_loss: 0.0370\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0156 - val_loss: 0.0406\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0157 - val_loss: 0.0441\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0159 - val_loss: 0.0451\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0161 - val_loss: 0.0374\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0156 - val_loss: 0.0405\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0156 - val_loss: 0.0369\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0159 - val_loss: 0.0720\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0161 - val_loss: 0.0377\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0161 - val_loss: 0.0372\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 523us/step - loss: 0.0155 - val_loss: 0.0380\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 5s 509us/step - loss: 0.0153 - val_loss: 0.0384\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 0.0157 - val_loss: 0.0448\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0151 - val_loss: 0.0429\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0154 - val_loss: 0.0472\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 6s 611us/step - loss: 0.0156 - val_loss: 0.0377\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 4s 372us/step - loss: 0.0157 - val_loss: 0.0364\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0159 - val_loss: 0.0396\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 0.0148 - val_loss: 0.0413\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0150 - val_loss: 0.0492\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0147 - val_loss: 0.0381\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: 0.0148 - val_loss: 0.0435\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: 0.0154 - val_loss: 0.0382\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.0147 - val_loss: 0.0377\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.0153 - val_loss: 0.0405\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 0.0146 - val_loss: 0.0475\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.0148 - val_loss: 0.0436\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 6s 587us/step - loss: 0.0146 - val_loss: 0.0399\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 4s 449us/step - loss: 0.0150 - val_loss: 0.0388\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0146 - val_loss: 0.0367\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0150 - val_loss: 0.0402\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0148 - val_loss: 0.0382\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 4s 458us/step - loss: 0.0146 - val_loss: 0.0388\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0146 - val_loss: 0.0370\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0143 - val_loss: 0.0362\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0146 - val_loss: 0.0388\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0147 - val_loss: 0.0362\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model2.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model2.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history2 = model2.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3X90XHd55/H3g6zEdiRFiSOEcBwbx94EW3ZNlYV22XbjAG0gFNKWGjgtTSkcu7vtOe1CW7vttmNaOAVqmh5qui0t1D4LrWCBAuvAtok8xqHbhdrYxLZcb4xtYitK/CtWNCZKLPvZP+beyZ3RHc2dX/px9XmdM0cz997vd57v873zeHzv3Blzd0REZPZ7yXQHICIijaGCLiKSEiroIiIpoYIuIpISKugiIimhgi4ikhIq6DLjmNlfmtnvJ9x2h5l9sNkxicwGKuhSNzP7HTP7Wsmyx8sse0el/tz9V9z9jxoUm5vZiqlu2yxm9oCZ7TezZ83sjJl91MzmTXdcMjOooEsj7AVea2YtAGb2MqAV+OGSZSuCbSWBMoV6IfAbwC3Aa4DXAb85lXHJzKWCLo3wr+QL+Lrg8Y8DWeBYybLvufuTAGZ2p5k9bGYXzeyYmW0IOys9jGJmv21mw2b2pJm9N+ad801m9pCZjZrZt8zs9qBd+I/Hd80sZ2ZvN7NbzGyXmV0KnvtRM6vqdWBmt5vZbjO7YGbnzeyzZtYZrPstM/tiyfZ/bmZ/Fty/0cw+FYxnyMw+GPlH75fM7J/N7EEzuwhsLX1ud//v7v6ou7/g7kPAZ4HXVhO/pJcKutTN3V8AvkW+aBP8fRT4ZsmyvQBmdgPwMPB3wEuBdwJ/YWarS/s2s3uB9wGvJ/8O/z/FhPBO4APATcBx4ENBXOFz/5C7t7n754D3A2eALqAb+F2g2u+/MOCPgZcDrwSW8GLx/Qxwb6TAzwPeDvyPYP1OYDwYy6uAnwDeG+n7NcAJ8nn5UIJYfhw4UmX8klIq6NIo3+DF4v1j5Av6oyXLvhHcfzNwyt3/1t3H3f07wBeBt8X0uwH4W3c/4u4/IF+4S33J3b/t7uPk37Gui9kmdAXoAZa6+5Xg3W5VBd3dj7v7w+7+vLufA/6U4B8adx8m/w/XzwWb3wucd/f9ZtYNvBH4DXe/7O5ngQeB6HmFJ939z4O8PDdZHGb2buAuYFs18Ut6qaBLo+wF/qOZ3QR0ufvjwP8B/kOwrJcXj58vBV4THPa4ZGaXgJ8HXhbT78uB05HHp2O2eSpy/wdA2yRx/gn5d/H/ZGYnzGxLgrEVMbOXmll/cMjkWfLvym+JbLIT+IXg/i/w4rvzpeQPTQ1Hxv1X5N+Nh+LGFxfD/cCHgTe6+/lqxyDppIIujfIvwI3ARuCfAdz9WeDJYNmT7n4y2PY08A1374zc2tz9P8f0OwzcGnm8pJ4g3X3U3d/v7suBnwLeZ2avq7KbPyZ/mGatu3eQL9oWWf9lYK2Z9ZL/38hng+WngeeBWyLj7nD36KGmiv9bCA5D/TXwU+5+qMrYJcVU0KUhgsMD+8gf7340suqbwbLop1t2Af/OzN5lZq3B7d+b2Stjuv488G4ze6WZLQT+oMrQngaWhw/M7M1mtsLMDHgWuBrcyrnOzOZHbi1AO5ADLpnZYuC3og3cfQz4AvlzBN929yeC5cPAPwEfM7MOM3tJcII17rxALDO7h/w/ED/r7t9O2k7mBhV0aaRvkD988M3IskeDZYWC7u6j5E8GvoP8O/ingI8A15d26O5fBz5O/lMzx8n/TwDy73ST2ArsDA5xbABWAo+QL8j/AvyFu++ZpP0R4LnI7d3kj+P/MDACPAR8KabdTmANLx5uCf0icB0wCDxDvvD3JBwLwO+T/5/Q14JP7uTM7OtVtJcUM/3Ahcwmwbv4w8D1wUnQGcnMbgP+DXhZcOhJpOn0Dl1mPDP7aTO7Lji5+hHgf83wYv4S8oeZ+lXMZSqpoMtssAk4B3yP/PHuuJOnM0LwGftngTcAmWkOR+YYHXIREUkJvUMXEUmJKf2WtltuucWXLVs2lU9Zt8uXL3PDDTdMdxhTSmOeG+bimGF2jnv//v3n3b2r0nZTWtCXLVvGvn37pvIp67Znzx7uvvvu6Q5jSmnMc8NcHDPMznGb2feTbKdDLiIiKaGCLiKSEiroIiIpoYIuIpISKugiIimhgi4ikhIq6CIiKaGCLiKSEiroIiIpoYIuIpISKugiIimhgi4ikhIq6CIiKaGCLiKSEiroIiIpkbigm1mLmR0ws13B41eY2bfM7HEz+5yZXde8MEVEpJJq3qH/OnA08vgjwIPuvhJ4BnhPIwMTEZHqJCroZnYrcB/wN8FjA+4BvhBsshO4vxkBiohIMknfof8Z8NvAteDxIuCSu48Hj88Aixscm4iIVKHib4qa2ZuBs+6+38zuDhfHbOpl2m8ENgJ0d3ezZ8+e2iKdJrlcbtbFXC+NeW6Yi2OGdI87yY9EvxZ4i5m9CZgPdJB/x95pZvOCd+m3Ak/GNXb3TwKfBLjrrrt8tv0462z8Qdl6acxzw1wcM6R73BUPubj777j7re6+DHgHsNvdfx7IAm8LNnsA+ErTohQRkYrq+Rz6ZuB9Znac/DH1TzUmJBERqUWSQy4F7r4H2BPcPwG8uvEhiYhILXSlqIhISqigi4ikhAq6iEhKqKCLiKSECnpKbN26dbpDEJlSy7Y8NN0hzDgq6CIiKaGCLiKSEiroIiIpoYIuIpISKugiIimhgi4ikhIq6CIiKaGCLiKSEiroIiIpoYIuIpISKugiIilRsaCb2Xwz+7aZfdfMjpjZB4LlO8zspJkdDG7rmh+uiIiUk+QXi54H7nH3nJm1At80s68H637L3b/QvPBERCSpigXd3R3IBQ9bg5s3MygREame5et1hY3MWoD9wArgE+6+2cx2AD9K/h38ALDF3Z+PabsR2AjQ3d3d19/f37jop0Aul6OtrW26w6hoeHiYnp6ehvQ1W8bcSBrz7HNoaIQ1i2+sut1sHPf69ev3u/tdFTd098Q3oBPIAr1AD2DA9cBO4A8qte/r6/PZJpvNTncIiWQymYb1NVvG3Ega8+yzdPOumtrNxnED+zxBja7qUy7ufgnYA9zr7sPBcz0P/C3w6mr6EhGRxkryKZcuM+sM7i8AXg/8m5n1BMsMuB843MxARURkckk+5dID7AyOo78E+Ly77zKz3WbWRf6wy0HgV5oYp4iIVJDkUy6PAa+KWX5PUyISmQEGdt/O6+753nSHIQmc2fIot374x6Y7jBlBV4qKiKSECrqISEqooIuIpIQKuohISqigi8xwL8senO4QZJZQQRcRSQkVdBGRlFBBFxFJCRV0EZGUUEEXEUkJFXQRkZRQQRcRSQkVdBGRlFBBFxFJCRV0kRlizc410x1C033s7W+e7hBSTQVdRCQlkvwE3Xwz+7aZfdfMjpjZB4LlrzCzb5nZ42b2OTO7rvnhiohIOUneoT8P3OPuPwSsA+41sx8BPgI86O4rgWeA9zQvTBERqaRiQfe8XPCwNbg5cA/whWD5TvI/FC0iItPE3L3yRvkfiN4PrAA+AfwJ8H/dfUWwfgnwdXfvjWm7EdgI0N3d3dff39+46Bvg3BOjdN3WXnZ9Lpejra0tUV+jo4dpb5+QgikxPDxMT09PXX1cGcrRuritqjEn7bMeT584TvfyFQ2Jp5zSMU/HXA5eGGTVolUTlj82+hxr2xc0/PkaOc9JNXIuDw2NsGbxjVXvY9Mx7nqtX79+v7vfVXFDd098AzqBLPBjwPHI8iXAoUrt+/r6fKbZvmlg0vXZbDZxX48MLK8zmtplMpm6+zi9ea+7VzfmpH3WY9uG+xoQyeRKxzwdc9m7ozd2effuA015vkbOc1KNnMulm3e5e/X72HSMu17APk9Qo6v6lIu7XwL2AD8CdJrZvGDVrcCT1fQlIiKNleRTLl1m1hncXwC8HjhK/p3624LNHgC+0qwgRUSksnmVN6EH2BkcR38J8Hl332Vmg0C/mX0QOAB8qolxiohIBUk+5fKYu7/K3de6e6+7/2Gw/IS7v9rdV7j7z7n7880Pd245s+XR6Q5B6rB169bC/bi5XLbloSmMRqbT0TtfOSXPoytFRURSQgVdRCQlVNBFRFJCBV1EJCVU0AMvyx6c0ucLT4g18utEdZJtakz1viJ5jcz7VJ2knGoq6CIiKaGCLiKSEiroIiIpoYIuIpISs66gl15xN7D79to723pjndEk1+g403wV6Sd+ZXfibT/29jfXl9sY05nbWfebm1P4GpLKZl1BFxGReCroIiIpoYIuIpISKugiIikxZwp6mk8izgTTfvVkDSfnGn0ydaaaihOt0a8KnivW7FwDlN+PpuMEd5JfLFpiZlkzO2pmR8zs14PlW81syMwOBrc3NT9cEREpJ8kvFo0D73f375hZO7DfzB4O1j3o7tuaF56IiCRVsaC7+zAwHNwfNbOjwOJmByYiItWp6hi6mS0DXgV8K1j0a2b2mJl92sxuanBsIiJSBXP3ZBuatQHfAD7k7l8ys27gPODAHwE97v7LMe02AhsBuru7+/r7++sK+MpQjtbFbYXHo6OHaW/vnbDd0yeO0718Rdl2AAwf5NyV2+m6rZ3HRp9jbfuCCf3kcjna2tpi+yxVLpa4dYeGRliz+MbYPsN1hZiHD0LPuqJtouM598Qo4605zl9byJrFtV+5d2Uox9GOFpbb1cKYSw1eGGTVolUTlk/IXxBz0vmKOvfEaNGclMv74IVBFo1cx8KuMdpz4xNyVBpLqWgsuVyOkyNXC3kf6zhVNs5y+8qEpx0epqenB4jf/8J5Lh1P6ViTPl855fKXy+W4fPapfP4qzElZQW6jY602ltI8lBPNw/DwcGF/j31tTyKXyzHv+99n/urVxUOZZAxx+234WkhSg8aOHJnwfNVYv379fne/q+KG7l7xBrQC/wi8r8z6ZcDhSv309fV5vU5v3lv0+JGB5bHbbdtw36Tt3N090+HbNw24u3v37gOx/WSz2bJ9lioXS9y6pZt3le0zXFeIOdMxYZvoeLZvGvBMJlNoV6vTm/d69+4DRWMu1bujN3b5hPwFMSedr6jSOSmX994dvb5tw335PmNyVBpLqWgs2Wy2KO+TxVluX5nwtJlM4X7c/lc6X+F4an2+csrlL5vNvpi/WgW5jY612liS7rfRPET399jX9iSy2awP3nHnhOWTjSEuR+FrIUkNinu+agD7PEGtTvIpFwM+BRx19z+NLI/+U/bTwOHk/96IiEijJfmUy2uBdwGHzCz8sPHvAu80s3XkD7mcAjY1JUIREUmk4jt0d/+mu5u7r3X3dcHta+7+LndfEyx/i+c/DTPrJb0YoJpvBGyERj5feBFQrX3WcsFEeBFGUtN1oUo9P01W7uK1Sn029KK3Wr/9MEG7Rs9Jpf2vEc93Zsujde/vs8mcuVJURCTtVNBFRFJCBV1EJCVU0EVEUkIFfRJJTqIs2/JQ1f1OdrInyYmg8CRbpW84rOZEZK0nA+PaVfMthlV9S2MVJ/wSj72KPus5YVpJs76tstqT0TADvjkzxqz+ttQp/Jk+FXQRkZRQQRcRSQkVdBGRlFBBFxFJiVlV0Cc7GTjZCatmnsyaao0+CVvOx97+5qZfrdmME11bt26tKUfl1Ht1YWksSU9SLtvyUNn9drJ1SSU58TkdP6FWr2rmK3oV6WRmUx5mVUEXEZHyVNBFRFJCBV1EJCVU0EVEUmJOFvRKJzmqudKxkRrx9Z5T+lWtVarn5FK9c9Lwr04tc/Vf0qssk55wLp2vyfIwE3JUdNK31qtwG3BlZa0no2e7OVnQRUTSKMlP0C0xs6yZHTWzI2b268Hym83sYTN7PPh7U/PDFRGRcpK8Qx8H3u/urwR+BPhVM1sFbAEG3H0lMBA8FhGRaZLkJ+iG3f07wf1R4CiwGHgrsDPYbCdwf7OCFBGRyszdk29stgzYC/QCT7h7Z2TdM+4+4bCLmW0ENgJ0d3f39ff31xTooaERVl46Q0vnUo52tLC2fQEAo6OHaW/vZezIEeavXl3Y/ukTx+levgKAsSNHitoV1g0f5NyV2+m6rZ3HRp8rrFvYNUZ7ey/nnhhlwc1GW1vbhD7PPTHKeGuO89cWsmbxjUWxxImuGx4eLrSbrM8rQzlaF7cVxRmXh2i7lZfOFPIweGGQVYtWMTw8TE9PDwBXhnIc7Whh3gsn6bq8hK7b2ifkaLld5fLZp7i24IZCu9DghUEWjVxXNreFsebGoWddYQxhuzC30VjCdqFwPOfaOovnJOizNJboujC3LfOHWLVoVX7DIH/jrbnCeEr7zOVynBy5WsjtWMepwj4Qtgv3scdGn2Nt7lhRLHHjCWMJ+wzzEM5JdJ7DOQlzO9n+furZJUV9xo0H4LHR54r6DHMb7u/hPIft4l4LYbtwPKX7Q2lu4+YkqvS1F7ffMnywMJel+21pboteJ2VeQ6X7ey6XY973vz8ht+HzRWtJ3H4b3f9WLVpV9nUfHevYxdai+lSt9evX73f3uypu6O6JbkAbsB/4meDxpZL1z1Tqo6+vz2u1dPMuH7zjTj+9ea937z5QWP7IwHJ3dx+8486i7bdtuK9wv7RdYV2mw7dvGnB3L1oX9rl904Bns9nYPrdvGvBMJuNLN++aEEuc6Lpou8n6PL1574Q44/IQbRfNQ++O3sLzhcJ2vTt6C32W5iibzfq2DfcVtYv2OVluC2PNdBSNIWwXzUNpu9I8TJiToM/SWKLrwjyEY4/mLzqe0nbZbLYot9F9IGwX5rZ794EJscSNJzon0TxE10XbRXM72f5e2me5HJX2GY4nXBfOc9gu7rUQFbc/lOY2bk6iSl97cfttdC4r5TZcFx1fudyG48tms7G5LZ3nCeMpEc5ludd9dKyl9alawD5PUKcTfcrFzFqBLwKfdfcvBYufNrOeYH0PcLa6f3NERKSRknzKxYBPAUfd/U8jq74KPBDcfwD4SuPDExGRpOYl2Oa1wLuAQ2YWXjXxu8CHgc+b2XuAJ4Cfa06IIiKSRJJPuXzT3c3d17r7uuD2NXe/4O6vc/eVwd+LUxFwNRp5xedkV5rVui7JV6BO11WrlczUuKZas79iuKFquAIzbp5rueqyEV/5OxWiV5hW+9uqR+985bT/9qmuFBURSQkVdBGRlFBBFxFJCRV0EZGUSEVBr/c3JCudyKjq5MjWGyd8DWnha2Mb8LWg06HWk1ll201RHuK+rreRvzc6l4T7dLUnCmVqpaKgi4iICrqISGqooIuIpIQKuohISqSuoDfspE2dJ+5m1RWEk5iKk4hrdq5J/HujDf9t0IQq5WGmXQVZ7xWLSa4Gna1XCx8aGqm57Uyb51KpK+giInOVCrqISEqooIuIpIQKuohISqSyoNfy9Z5JzdYTQbPGNFxN2+gTrc3c/+aiyU5EFq2bon0n9oRzyXOH+8BU14tUFnQRkbkoyU/QfdrMzprZ4ciyrWY2ZGYHg9ubmhumiIhUkuQd+g7g3pjlD0Z/waixYYmISLWS/ATdXmDG/byciIgUM3evvJHZMmCXu/cGj7cCvwQ8C+wD3u/uz5RpuxHYCNDd3d3X399fU6CHhkZYeekMLZ1LOdrRwtr2BQCMjh7m1LNLWHnpDPNXr+ax0eeY98JJFo1cx8KuMdrbexk7cqTQLlzXvXwFDB/k3JXb6WyxQp9Pnzieb5cb59yV21lws3HCWyb0ee6JUcZbc5y/tpCW+UOsWrSK0dHDhXbjrTl6enoYvDD4YrvcOPSsY3h4mPPXFrLy0hlGFlyfjwWK+lyz+EauDOUY6zhV6LPrtvbYPETbhXkAGLwwyKpFqwrPF/YZ5qHr8hK6bmsHKMrRcrvK5bNPcW3BDbF9lsttdE7ac+OMXWylpXMprYvbYvMQjaV0TsZbc5xr64ydkzDmyXLbMn8ots8wD3HzfHLkaiG3Yx2nJsxzdB9bmztWNM+l+1g072GfYR66Li+ZMM+leUiyv4d9AkXjqZTbrtvaeWz0ucI8l+Y27jUU5uFcW2dhfEn39+jrt7C/l8xJdB9j+GBhLktzuzZ3bMJrqJCHmHmO29/PXhyhY3hibqN9Vqol4VxG8zDZ/j52sfXF8dVg/fr1+939roobunvFG7AMOBx53A20kH+H/yHg00n66evr81ot3bzLB++4009v3uvduw8Ulj8ysLywzt29e/cB793R69s23OePDCx3dy9qF65zd/dMh2/fNFDUZ6FdsC6bzcb2uX3TgGcyGV+6eZf37ugtxBK2y2Qy7u7F7TId+acN2g3eceeLsZT06e5+evPeoj7L5SHaLsxD+NzR5wv7DMcT9lmao2w269s23Fe2z3K5jc6JZzoK68rlYbI5yWQyZeckNpaS3JbrM8xD3DxHcxs3z9F9rHSeS/ex0nmO5iFunkvzkGR/D/ssHU+l3IZjCOe5NLeT7e/R8SXd36PjKTcn0X0sOpcT9rGY11AhDzHzHLe/f/wzX47Nbdw8l9vfw7FG8zDZ/l40vhoA+zxBja3pUy7u/rS7X3X3a8BfA6+upR8REWmcmgq6mfVEHv40cLjctiIiMjXmVdrAzP4euBu4xczOABngbjNbBzhwCtjUxBhFRCSBJJ9yeae797h7q7vf6u6fcvd3ufsad1/r7m9x9+GpCFaql+QqSP3O5sw3m68+rearpMMrK6f6a5KTfn1zaKb+tqquFBURSQkVdBGRlFBBFxFJCRV0EZGUUEGfKabha2OrNVNPBEl16v29UZm5VNBFRFJCBV1EJCVU0EVEUkIFXUQkJVTQRWa52XwVqTSWCrqISEqooIuIpIQKuohISqigi4ikhAq6iEhKVCzoZvZpMztrZocjy242s4fN7PHg703NDVNERCpJ8g59B3BvybItwIC7rwQGgsciIjKNkvxi0V7gYsnitwI7g/s7gfsbHJeIiFTJ3L3yRmbLgF3u3hs8vuTunZH1z7h77GEXM9sIbATo7u7u6+/vrynQQ0MjrLx0hpbOpRztaGFt+wIARkcPc+rZJay8dIb5q1fz2OhzzHvhJItGrmNh1xjt7b2MHTlSaBeu616+AoYPcu7K7XS2WKHPp08cz7fLjXPuyu0suNk44S0T+jz3xCjjrTnOX1tIy/whVi1axejo4UK78dYcPT09DF4YfLFdbhx61jE8PMz5awtZeekMIwuuL4ol7HPN4hu5MpRjrONUoc+u29pj8xCNJcwDwOCFQbouL5nQZ5iHrstLYvtcble5fPYpri24oajPJLmN5mHsYistnUtpXdwWm4doLKVzMt6a41xbZ+ycdN3WXhhfudy2zB+K7TPMQ9w8nxy5WsjDWMepCfMczcPa3LGieY7LQ3Seo3mYbE7CmEvnJJrbcH+vNbfh/h7Oc2lu4+Y5zMO5ts4J89yI/T3MbVyf0dyuzR2b0GeYh3KvodL9/ezFETqGi19DT5843tT9fexia+F1WYv169fvd/e7Km7o7hVvwDLgcOTxpZL1zyTpp6+vz2u1dPMuH7zjTj+9ea937z5QWP7IwPLCOnf37t0HvHdHr2/bcJ8/MrDc3b2oXbjO3d0zHb5900BRn4V2wbpsNhvb5/ZNA57JZHzp5l3eu6O3EEvYLpPJuLsXt8t05J82aDd4x50TYgnXubuf3ry3qM9yeYi2C/MQPndcn+F4yvWZzWZ924b7JvSZJLfRPITryuVhsjnJZDJl5yQ6vnK5LddnmIe4eY7mIW6eo3konee4PETbRfMw2ZyEMZfOSTS3cX1Wk9vSeS7N7WT7e9w8N2J/D3Mb12fR/h7TZ5iHcq+h0v3945/58oTXULP39+jrshbAPk9QY2v9lMvTZtYDEPw9W2M/IiLSILUW9K8CDwT3HwC+0phwRESkVkk+tvj3wL8Ad5jZGTN7D/Bh4A1m9jjwhuCxiIhMo3mVNnD3d5ZZ9boGxyIiInXQlaIiIimhgi4ikhIq6CIiKaGCLiKSEiroIiIpoYIuIpISKugiIimhgi4ikhIq6CIiKaGCLiKSEiroIiIpoYIuIpISKugiIimhgi4ikhIq6CIiKaGCLiKSEhV/4GIyZnYKGAWuAuOe5FepRUSkKeoq6IH17n6+Af2IiEgddMhFRCQlzN1rb2x2EngGcOCv3P2TMdtsBDYCdHd39/X399f0XIeGRlh56QwtnUs52tHC2vYFPH3iOAu7xjj17BJWXjrD/NWreWz0Oea9cJJFI9exsGuM9vZexo4cKbQL13UvXwHDBzl35XY6W2xCn+25cc5duZ0FNxsnvGVCn+eeGGW8Ncf5awtpmT/EqkWrGB09XGg33pqjp6eHwQuDL7bLjUPPOoaHhzl/bSErL51hZMH1RbGEfa5ZfCNXhnKMdZwq9Nl1W3tsHqKxhHkAGLwwSNflJRP6DPPQdXlJbJ/L7SqXzz7FtQU3FPWZJLfRPIxdbKWlcymti9ti8xCNpXROxltznGvrjJ2TrtvaC+Mrl9uW+UOxfYZ5iJvnkyNXC3kY6zg1YZ6jeVibO1Y0z3F5iM5zNA+TzUkYc+mcRHMb7u+15jbc38N5Ls1t3DyHeTjX1jlhnhuxv4e5jeszmtu1uWMT+gzzUO41VLq/n704QsfwxFrSzP197GJr4XVZi/Xr1+9PdEjb3Wu+AS8P/r4U+C7w45Nt39fX57VaunmXD95xp5/evNe7dx9wd/dtG+7zRwaWF9a5u3fvPuC9O3oL69y9qF24zt3dMx2+fdNAbJ/humw2G9vn9k0DnslkfOnmXd67o9fdvahdJpNxdy9ul+nIP23QbvCOOyfEEq5zdz+9eW9Rn+XyEG0X5iF87rg+w/GU6zObzfq2DfdN6DNJbqN5CNeVy8Nkc5LJZMrOSXR85XJbrs8wD3HzHM1D3DxH81A6z3F5iLaL5mGyOQljLp2TaG7j+qwmt6XzXJrbyfb3uHluxP4e5jauz6L9PabPMA/lXkOl+/vHP/Pl2FrSzP09+rqsBbDPE9Tkug65uPuTwd+zwD8Ar66nPxERqV3NBd3MbjCz9vA+8BPA4UYFJiIi1annUy7dwD+YWdjP37n7/25IVCIiUrWaC7q7nwB+qIGxiIhIHfSxRRGRlFDKoWE0AAAE5UlEQVRBFxFJCRV0EZGUUEEXEUkJFXQRkZRQQRcRSQkVdBGRlFBBFxFJCRV0EZGUUEEXEUkJFXQRkZRQQRcRSQkVdBGRlFBBFxFJCRV0EZGUUEEXEUmJugq6md1rZsfM7LiZbWlUUCIiUr16flO0BfgE8EZgFfBOM1vVqMBERKQ69bxDfzVw3N1PuPsLQD/w1saEJSIi1TJ3r62h2duAe939vcHjdwGvcfdfK9luI7AxeHgHcKz2cKfFLcD56Q5iimnMc8NcHDPMznEvdfeuShvV/CPRgMUsm/Cvg7t/EvhkHc8zrcxsn7vfNd1xTCWNeW6Yi2OGdI+7nkMuZ4Alkce3Ak/WF46IiNSqnoL+r8BKM3uFmV0HvAP4amPCEhGRatV8yMXdx83s14B/BFqAT7v7kYZFNnPM2sNFddCY54a5OGZI8bhrPikqIiIzi64UFRFJCRV0EZGUUEEHzOxmM3vYzB4P/t5UZrsHgm0eN7MHYtZ/1cwONz/i+tUzZjNbaGYPmdm/mdkRM/vw1EZfnUpfUWFm15vZ54L13zKzZZF1vxMsP2ZmPzmVcdej1jGb2RvMbL+ZHQr+3jPVsdeqnnkO1t9mZjkz+82pirnh3H3O34CPAluC+1uAj8RsczNwIvh7U3D/psj6nwH+Djg83eNp9piBhcD6YJvrgEeBN073mMqMswX4HrA8iPW7wKqSbf4L8JfB/XcAnwvurwq2vx54RdBPy3SPqcljfhXw8uB+LzA03eNp9pgj678I/E/gN6d7PLXe9A49763AzuD+TuD+mG1+EnjY3S+6+zPAw8C9AGbWBrwP+OAUxNooNY/Z3X/g7lkAz3/tw3fIX4cwEyX5iopoLr4AvM7MLFje7+7Pu/tJ4HjQ30xX85jd/YC7h9eTHAHmm9n1UxJ1feqZZ8zsfvJvWGb1J/VU0PO63X0YIPj70phtFgOnI4/PBMsA/gj4GPCDZgbZYPWOGQAz6wR+ChhoUpz1qjiG6DbuPg6MAIsStp2J6hlz1M8CB9z9+SbF2Ug1j9nMbgA2Ax+Ygjibqp5L/2cVM3sEeFnMqt9L2kXMMjezdcAKd/+vpcfkpluzxhzpfx7w98DH3f1E9RFOiSRfUVFum0RfbzED1TPm/Eqz1cBHgJ9oYFzNVM+YPwA86O654A37rDVnCrq7v77cOjN72sx63H3YzHqAszGbnQHujjy+FdgD/CjQZ2anyOfzpWa2x93vZpo1ccyhTwKPu/ufNSDcZknyFRXhNmeCf6RuBC4mbDsT1TNmzOxW4B+AX3T37zU/3IaoZ8yvAd5mZh8FOoFrZjbm7tubH3aDTfdB/JlwA/6E4hOEH43Z5mbgJPmTgjcF928u2WYZs+ekaF1jJn++4IvAS6Z7LBXGOY/8sdFX8OLJstUl2/wqxSfLPh/cX03xSdETzI6TovWMuTPY/menexxTNeaSbbYyi0+KTnsAM+FG/tjhAPB48DcsWncBfxPZ7pfJnxg7Drw7pp/ZVNBrHjP5dz8OHAUOBrf3TveYJhnrm4D/R/5TEL8XLPtD4C3B/fnkP91wHPg2sDzS9veCdseYoZ/kaeSYgf8GXI7M60HgpdM9nmbPc6SPWV3Qdem/iEhK6FMuIiIpoYIuIpISKugiIimhgi4ikhIq6CIiKaGCLiKSEiroIiIp8f8Bo+637MLjpWgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "#history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "###calculate gradients\n",
    "\n",
    "#funcion de perdida \n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "loss = keras.losses.mean_squared_error(model.layers[0].output,y_train_scaled)#el gradiente de la funcion de perdida\n",
    "listOfVariableTensors = model.layers[0].trainable_weights\n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "\n",
    "#model.layers[0].Weight.get_value()\n",
    "#weights0 = model.layers[0].get_weights()[0] \n",
    "#biases0 = model.layers[0].get_weights()[1] \n",
    "#weights1 = model.layers[1].get_weights()[0] \n",
    "#biases1 = model.layers[1].get_weights()[1] \n",
    "#layer0 = K.function([model.layers[0].input],[model.layers[-1].output]) \n",
    "#layer1 = K.function([model.layers[0].input],[model.layers[1].output]) \n",
    "\n",
    "f=tf.keras.backend.gradients(\n",
    "    loss,\n",
    "    listOfVariableTensors\n",
    ")\n",
    "weights0, biases0 = model.layers[-2].get_weights()\n",
    "weights, biases = model.layers[0].get_weights()\n",
    "weights1, biases1 = model.layers[1].get_weights()\n",
    "\n",
    "#print (weights0)\n",
    "#print(len(model.layers))\n",
    "#g=tf.convert_to_tensor(gradients)\n",
    "#\n",
    "#tf.summary.histogram(\"sample1\",g,collections=None,family=None)\n",
    "plt.title('Weights Layer 1')\n",
    "plt.hist(weights0, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.title('Weights Layer 2')\n",
    "plt.hist(weights, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.title('Weights Layer 3')\n",
    "plt.hist(weights1, bins = 60)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Para el mismo problema definido anteriormente ([sección 1](#primero)) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (*loss*) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de *gradients* (__[link](https://www.tensorflow.org/api_docs/python/tf/keras/backend/gradients)__) en el *backend* de Keras. Deberá generar un **histograma** para todos los pesos de cada capa antes y despues del entrenamiento con 250 *epochs*. Comente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
