{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Construya un dataframe con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas**\n",
    "\n",
    "Las columnas 'Unnamed: 0' y 'pubchem_id' se eliminan por no tener datos relevante para los conjuntos de entrenamiento, validación y testeo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imprimir dataset\n",
      "                0          1          2          3          4          5  \\\n",
      "13805   73.516695  20.698877  18.620662  17.810283  15.905550  15.843780   \n",
      "13806   73.516695  20.679672  16.113738  13.684521  13.684316  13.650499   \n",
      "13807   73.516695  20.757186  18.681807  14.912967  13.684817  13.683619   \n",
      "13808  388.023441  46.598629  46.596068  41.499896  28.840634  19.258076   \n",
      "13809   73.516695  18.649283  18.596148  16.906068  16.138929  13.732604   \n",
      "13810   73.516695  20.686502  20.676114  18.387888  18.385507  18.380429   \n",
      "13811   73.516695  23.631201  23.627404  18.594704  17.821936  15.732707   \n",
      "13812  388.023441  46.636408  46.608269  41.480748  28.929084  19.874522   \n",
      "13813   73.516695  20.580008  20.468204  18.667254  14.894277  13.756640   \n",
      "13814  388.023441  46.652934  46.629332  46.550982  46.520705  41.493579   \n",
      "13815  388.023441  46.655242  46.625060  46.547117  46.519552  41.491914   \n",
      "13816   73.516695  20.747524  18.505571  18.490333  14.221084  13.651886   \n",
      "13817   73.516695  23.616991  23.615738  16.159896  16.159153  15.638196   \n",
      "13818   73.516695  20.596410  18.636624  18.613193  17.813534  17.803806   \n",
      "13819   73.516695  20.599469  18.677963  16.499959  16.424990  14.894848   \n",
      "13820  388.023441  46.657450  46.629765  41.493986  28.889938  18.894137   \n",
      "13821  388.023441  46.655745  46.628967  41.490100  28.894946  20.573156   \n",
      "13822  388.023441  40.351262  28.443257  28.441920  28.441802  28.441344   \n",
      "13823   73.516695  18.642788  18.640476  18.627544  13.687691  13.686642   \n",
      "13824   73.516695  21.582268  18.870551  15.830518  13.682661  13.682490   \n",
      "13825   73.516695  20.822747  18.817662  17.161491  16.034285  15.830774   \n",
      "13826   73.516695  20.708771  18.663221  18.318262  18.017573  17.794314   \n",
      "13827   73.516695  20.528676  18.570439  17.736223  14.707800  13.653366   \n",
      "13828   73.516695  20.789417  20.744509  18.700121  18.352461  14.558529   \n",
      "13829   53.358707  15.752977  15.728979  15.728642  13.653581  13.653127   \n",
      "13830   73.516695  18.655499  18.634953  18.634146  13.653712  13.653485   \n",
      "13831  388.023441  46.599563  46.568378  41.450317  28.993301  19.522017   \n",
      "13832   73.516695  18.616684  15.304975  15.153638  15.152248  13.874512   \n",
      "13833  388.023441  46.594731  46.560804  46.550385  46.512764  41.448231   \n",
      "13834   73.516695  18.635917  17.805502  13.874353  13.863637  13.653787   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "16212  388.023441  29.658517  29.655842  23.552305  23.550885  23.446952   \n",
      "16213   73.516695  18.635498  17.804251  15.228614  13.653764  13.653754   \n",
      "16214   53.358707  19.148633  13.653248  13.652988  13.652850  13.652754   \n",
      "16215   73.516695  20.698171  15.924907  15.855703  13.654009  13.653321   \n",
      "16216   73.516695  18.643484  18.640762  17.802607  17.802277  13.653867   \n",
      "16217   73.516695  17.870020  15.238941  15.176526  15.176523  12.525824   \n",
      "16218   53.358707  19.150693  16.405490  15.022080  14.895449  12.521816   \n",
      "16219   36.858105  14.107138  12.674530  12.544437  12.505901  12.480945   \n",
      "16220   73.516695  20.698855  20.698855  15.893020  15.892046  15.875667   \n",
      "16221   53.358707  15.754498  15.753712  13.653954  13.653736  13.653736   \n",
      "16222   73.516695  17.677578  12.519029  12.479941  12.477739  12.452035   \n",
      "16223   73.516695  20.743557  20.741361  18.598127  18.595527  14.582338   \n",
      "16224   73.516695  20.801457  20.801457  18.634187  18.634183  14.566950   \n",
      "16225   73.516695  20.581963  15.924696  15.751649  15.452521  14.852202   \n",
      "16226   53.358707  16.359301  16.358704  15.148839  13.825358  13.825283   \n",
      "16227   53.358707  16.435036  16.433452  15.192854  15.192072  15.191944   \n",
      "16228   36.858105  14.159026  13.653760  13.653143  13.653143  13.652803   \n",
      "16229   53.358707  16.525299  16.435115  14.202458  13.827072  13.743711   \n",
      "16230   73.516695  20.731531  20.731285  16.422832  16.422798  16.136916   \n",
      "16231   73.516695  20.221994  18.690270  18.632741  18.405014  17.735798   \n",
      "16232   73.516695  17.860880  17.780884  17.779987  12.511356  12.510997   \n",
      "16233  388.023441  46.723458  46.722364  28.505545  28.472771  19.796532   \n",
      "16234   73.516695  23.607539  23.606041  21.378625  17.851697  15.649065   \n",
      "16235   73.516695  20.643181  20.588968  18.703556  18.673772  14.895437   \n",
      "16236   73.516695  20.610314  20.554562  18.683667  18.653061  14.895197   \n",
      "16237   73.516695  20.753166  18.624076  17.872009  17.851690  17.851254   \n",
      "16238   73.516695  20.724740  18.579933  17.741621  14.716676  13.697829   \n",
      "16239   53.358707  20.820797  19.150234  19.148721  15.135514  15.123685   \n",
      "16240   53.358707  15.707759  15.707644  13.653838  13.653570  13.653314   \n",
      "16241   53.358707  15.708752  15.708094  13.653893  13.653176  13.653120   \n",
      "\n",
      "               6          7          8          9    ...      1266  1267  \\\n",
      "13805  13.683477  13.683215  13.650338  13.650234    ...       0.0   0.0   \n",
      "13806  13.650317  13.650106  13.649532  13.560742    ...       0.0   0.0   \n",
      "13807  13.650482  13.650174  13.649798  13.649586    ...       0.0   0.0   \n",
      "13808  16.628994  15.724721  14.915059  14.745463    ...       0.0   0.0   \n",
      "13809  13.717282  13.665424  13.657666  13.624561    ...       0.0   0.0   \n",
      "13810  17.958065  14.240169  14.200689  12.866667    ...       0.0   0.0   \n",
      "13811  15.621388  15.422146  13.807226  13.653585    ...       0.0   0.0   \n",
      "13812  18.771014  18.533322  16.500540  16.424234    ...       0.0   0.0   \n",
      "13813  13.748754  13.654333  13.653740  13.653475    ...       0.0   0.0   \n",
      "13814  41.419572  29.077986  28.894877  24.565969    ...       0.0   0.0   \n",
      "13815  41.417644  29.080584  28.896403  19.130711    ...       0.0   0.0   \n",
      "13816  13.650138  13.648415  13.645595  13.645531    ...       0.0   0.0   \n",
      "13817  15.422747  15.329719  13.673531  13.672975    ...       0.0   0.0   \n",
      "13818  13.653941  13.653766  13.653434  13.653064    ...       0.0   0.0   \n",
      "13819  13.798744  13.725709  13.707933  13.682139    ...       0.0   0.0   \n",
      "13820  18.655883  18.254742  16.368112  15.749824    ...       0.0   0.0   \n",
      "13821  18.896057  18.663172  18.630923  18.256377    ...       0.0   0.0   \n",
      "13822  20.621804  20.620490  19.118837  19.109225    ...       0.0   0.0   \n",
      "13823  13.663814  13.662232  13.635523  13.634394    ...       0.0   0.0   \n",
      "13824  13.650100  13.649813  13.649091  13.648563    ...       0.0   0.0   \n",
      "13825  13.653867  13.652930  13.652797  13.651813    ...       0.0   0.0   \n",
      "13826  14.804868  14.206663  13.691990  13.663984    ...       0.0   0.0   \n",
      "13827  13.653283  13.653225  13.653225  13.653025    ...       0.0   0.0   \n",
      "13828  14.220804  12.960713  12.790332  12.691308    ...       0.0   0.0   \n",
      "13829  13.652733  13.652591  13.652591  13.652279    ...       0.0   0.0   \n",
      "13830  13.652592  13.652591  13.652375  13.652148    ...       0.0   0.0   \n",
      "13831  18.735185  18.464155  15.709809  13.653968    ...       0.0   0.0   \n",
      "13832  13.862448  13.654282  13.652398  13.652087    ...       0.0   0.0   \n",
      "13833  41.416834  29.087145  29.005335  24.966900    ...       0.0   0.0   \n",
      "13834  13.652577  13.651831  13.651821  12.770832    ...       0.0   0.0   \n",
      "...          ...        ...        ...        ...    ...       ...   ...   \n",
      "16212  23.446603  23.424316  22.152848  22.017541    ...       0.0   0.0   \n",
      "16213  13.653413  13.652802  13.652570  13.651962    ...       0.0   0.0   \n",
      "16214  13.652495  13.652167  12.965136  12.767064    ...       0.0   0.0   \n",
      "16215  13.653014  13.652836  13.652436  13.651938    ...       0.0   0.0   \n",
      "16216  13.653335  13.652740  13.652676  13.652322    ...       0.0   0.0   \n",
      "16217  12.525722  12.507239  12.484568  12.465571    ...       0.0   0.0   \n",
      "16218  12.521367  12.488777  12.488520  12.442787    ...       0.0   0.0   \n",
      "16219  12.469779  12.345929  12.286274  12.247041    ...       0.0   0.0   \n",
      "16220  15.875528  13.653570  13.652995  13.652591    ...       0.0   0.0   \n",
      "16221  13.653482  13.653142  13.653000  13.652847    ...       0.0   0.0   \n",
      "16222  12.426147  12.421406  12.308018  12.290278    ...       0.0   0.0   \n",
      "16223  14.576939  12.993091  12.611977  12.602461    ...       0.0   0.0   \n",
      "16224  14.566947  14.254289  14.252941  12.900472    ...       0.0   0.0   \n",
      "16225  13.654823  13.653847  13.653692  13.652830    ...       0.0   0.0   \n",
      "16226  13.654598  13.653577  13.653275  13.652769    ...       0.0   0.0   \n",
      "16227  13.836318  13.787761  13.744369  13.695440    ...       0.0   0.0   \n",
      "16228  13.652756  13.652713  13.092874  12.769178    ...       0.0   0.0   \n",
      "16229  13.695585  13.694933  12.759605  12.672241    ...       0.0   0.0   \n",
      "16230  16.135040  15.893446  15.853311  13.748468    ...       0.0   0.0   \n",
      "16231  14.707738  13.842602  13.689474  13.663879    ...       0.0   0.0   \n",
      "16232  12.481791  12.475366  10.571260  10.504954    ...       0.0   0.0   \n",
      "16233  19.731292  14.209230  14.144322  13.364883    ...       0.0   0.0   \n",
      "16234  15.422739  13.654045  13.653121  13.653119    ...       0.0   0.0   \n",
      "16235  14.894662  13.684300  13.683890  13.650207    ...       0.0   0.0   \n",
      "16236  14.894752  13.684066  13.683760  13.650190    ...       0.0   0.0   \n",
      "16237  17.742176  14.655754  12.706683  12.557785    ...       0.0   0.0   \n",
      "16238  13.697558  13.653512  13.652942  13.652387    ...       0.0   0.0   \n",
      "16239  12.942704  12.938162  12.488633  12.488061    ...       0.0   0.0   \n",
      "16240  13.652591  13.652585  13.652550  12.743890    ...       0.0   0.0   \n",
      "16241  13.652930  13.652528  13.652322  12.743688    ...       0.0   0.0   \n",
      "\n",
      "       1268  1269  1270  1271  1272  1273  1274        Eat  \n",
      "13805   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.487859  \n",
      "13806   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.126967  \n",
      "13807   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.684585  \n",
      "13808   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.350362  \n",
      "13809   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.353283  \n",
      "13810   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.476729  \n",
      "13811   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.002668  \n",
      "13812   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.901596  \n",
      "13813   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -14.380355  \n",
      "13814   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.681149  \n",
      "13815   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.683815  \n",
      "13816   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.565406  \n",
      "13817   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -14.356415  \n",
      "13818   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.787811  \n",
      "13819   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.903897  \n",
      "13820   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.570680  \n",
      "13821   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.246608  \n",
      "13822   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.640629  \n",
      "13823   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.889417  \n",
      "13824   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.098784  \n",
      "13825   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.055251  \n",
      "13826   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.408812  \n",
      "13827   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.699291  \n",
      "13828   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.442062  \n",
      "13829   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.388516  \n",
      "13830   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.045125  \n",
      "13831   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.400189  \n",
      "13832   0.0   0.5   0.0   0.0   0.0   0.0   0.0 -20.002401  \n",
      "13833   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.660636  \n",
      "13834   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.163770  \n",
      "...     ...   ...   ...   ...   ...   ...   ...        ...  \n",
      "16212   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -6.849553  \n",
      "16213   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.408032  \n",
      "16214   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.100940  \n",
      "16215   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.170072  \n",
      "16216   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.163202  \n",
      "16217   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -15.003575  \n",
      "16218   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.932200  \n",
      "16219   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.987681  \n",
      "16220   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.908604  \n",
      "16221   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.330795  \n",
      "16222   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.007444  \n",
      "16223   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -15.191880  \n",
      "16224   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -6.746408  \n",
      "16225   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.988657  \n",
      "16226   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.752393  \n",
      "16227   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.044004  \n",
      "16228   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.988833  \n",
      "16229   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.635767  \n",
      "16230   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.633728  \n",
      "16231   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.206702  \n",
      "16232   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -6.823134  \n",
      "16233   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.964498  \n",
      "16234   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.918597  \n",
      "16235   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.794917  \n",
      "16236   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.809176  \n",
      "16237   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.876123  \n",
      "16238   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.105268  \n",
      "16239   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -16.801464  \n",
      "16240   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.335088  \n",
      "16241   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.336696  \n",
      "\n",
      "[2437 rows x 1276 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "#print(datos.shape)\n",
    "#datos.info()\n",
    "#print(datos)\n",
    "#print(datos.describe())\n",
    "...\n",
    "#print(\"Quitando filas \\n\")\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True) # con la propiedad drop elimina columnas\n",
    "#print(datos.shape)\n",
    "#print(datos)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante\n",
    "print (\"Imprimir dataset\")\n",
    "print (df_test)\n",
    "#print (total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explique por qué se aconseja dicho preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "...\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "...\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 522us/step - loss: 1.4786 - val_loss: 0.5836\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 458us/step - loss: 0.5992 - val_loss: 0.4668\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: 0.4922 - val_loss: 0.3604\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.4115 - val_loss: 0.3223\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 400us/step - loss: 0.3560 - val_loss: 0.2935\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: 0.3123 - val_loss: 0.3161\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.2710 - val_loss: 0.2362\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 4s 383us/step - loss: 0.2390 - val_loss: 0.2275\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 0.2154 - val_loss: 0.4119\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.1941 - val_loss: 0.2441\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.1755 - val_loss: 0.1898\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.1598 - val_loss: 0.1934\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: 0.1414 - val_loss: 0.1380\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 5s 508us/step - loss: 0.1311 - val_loss: 0.1291\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 4s 456us/step - loss: 0.1189 - val_loss: 0.1372\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 4s 396us/step - loss: 0.1115 - val_loss: 0.1149\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 4s 384us/step - loss: 0.1029 - val_loss: 0.1188\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0956 - val_loss: 0.1067\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.0897 - val_loss: 0.1318\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0862 - val_loss: 0.0954\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.0802 - val_loss: 0.0902\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 4s 439us/step - loss: 0.0772 - val_loss: 0.0883\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.0754 - val_loss: 0.0891\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0705 - val_loss: 0.0912\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0667 - val_loss: 0.0895\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 442us/step - loss: 0.0653 - val_loss: 0.1052\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0640 - val_loss: 0.1053\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0609 - val_loss: 0.0775\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.0587 - val_loss: 0.0709\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 0.0578 - val_loss: 0.0761\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 0.0558 - val_loss: 0.0717\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 4s 442us/step - loss: 0.0536 - val_loss: 0.0706\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0524 - val_loss: 0.0757\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0522 - val_loss: 0.0621\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 401us/step - loss: 0.0500 - val_loss: 0.0772\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.0498 - val_loss: 0.0828\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0479 - val_loss: 0.0608\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0471 - val_loss: 0.0587\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: 0.0476 - val_loss: 0.0628\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.0456 - val_loss: 0.0678\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 4s 429us/step - loss: 0.0458 - val_loss: 0.0590\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.0442 - val_loss: 0.0618\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0420 - val_loss: 0.0555\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 5s 477us/step - loss: 0.0434 - val_loss: 0.0645\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.0426 - val_loss: 0.0708\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 4s 396us/step - loss: 0.0416 - val_loss: 0.0548\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0408 - val_loss: 0.0629\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 5s 465us/step - loss: 0.0392 - val_loss: 0.0554\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0398 - val_loss: 0.0531\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.0395 - val_loss: 0.0652\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0389 - val_loss: 0.0523\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 4s 457us/step - loss: 0.0382 - val_loss: 0.0741\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0365 - val_loss: 0.0612\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.0367 - val_loss: 0.0579\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0365 - val_loss: 0.0541\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 4s 417us/step - loss: 0.0366 - val_loss: 0.0483\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0353 - val_loss: 0.0521\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 5s 466us/step - loss: 0.0366 - val_loss: 0.0585\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0364 - val_loss: 0.0606\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 4s 462us/step - loss: 0.0342 - val_loss: 0.0498\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.0332 - val_loss: 0.0583\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0333 - val_loss: 0.0506\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 0.0336 - val_loss: 0.0675\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 5s 478us/step - loss: 0.0333 - val_loss: 0.0481\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0325 - val_loss: 0.0527\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0322 - val_loss: 0.0474\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0322 - val_loss: 0.0670\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 0.0321 - val_loss: 0.0473\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: 0.0316 - val_loss: 0.0599\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0318 - val_loss: 0.0489\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 4s 394us/step - loss: 0.0310 - val_loss: 0.0558\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 4s 455us/step - loss: 0.0313 - val_loss: 0.0449\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0307 - val_loss: 0.0459\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0298 - val_loss: 0.0594\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0290 - val_loss: 0.0443\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 5s 514us/step - loss: 0.0295 - val_loss: 0.0485\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0295 - val_loss: 0.0592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0297 - val_loss: 0.0462\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0280 - val_loss: 0.0454\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0296 - val_loss: 0.0426\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 0.0281 - val_loss: 0.0448\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 0.0284 - val_loss: 0.0585\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0279 - val_loss: 0.0445\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0280 - val_loss: 0.0442\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0275 - val_loss: 0.0521\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0277 - val_loss: 0.0429\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.0272 - val_loss: 0.0495\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0263 - val_loss: 0.0442\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0264 - val_loss: 0.0499\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 0.0263 - val_loss: 0.0499\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 4s 380us/step - loss: 0.0265 - val_loss: 0.0437\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0254 - val_loss: 0.0461\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0257 - val_loss: 0.0528\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.0258 - val_loss: 0.0422\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0270 - val_loss: 0.0469\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.0257 - val_loss: 0.0568\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0262 - val_loss: 0.0410\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0258 - val_loss: 0.0462\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: 0.0257 - val_loss: 0.0415\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0262 - val_loss: 0.0424\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0242 - val_loss: 0.0550\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0243 - val_loss: 0.0499\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 4s 441us/step - loss: 0.0239 - val_loss: 0.0399\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0250 - val_loss: 0.0637\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0243 - val_loss: 0.0486\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 4s 384us/step - loss: 0.0242 - val_loss: 0.0420\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0237 - val_loss: 0.0410\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 4s 405us/step - loss: 0.0240 - val_loss: 0.0932\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0240 - val_loss: 0.0392\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0236 - val_loss: 0.0465\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0240 - val_loss: 0.0462\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0244 - val_loss: 0.0391\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.0237 - val_loss: 0.0422\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.0235 - val_loss: 0.0456\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 5s 492us/step - loss: 0.0220 - val_loss: 0.0398\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 5s 539us/step - loss: 0.0227 - val_loss: 0.0772\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 4s 384us/step - loss: 0.0226 - val_loss: 0.0586\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0223 - val_loss: 0.0454\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0223 - val_loss: 0.0387\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0223 - val_loss: 0.0488\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0234 - val_loss: 0.0513\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0221 - val_loss: 0.0390\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0225 - val_loss: 0.0420\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0216 - val_loss: 0.0451\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0223 - val_loss: 0.0383\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0220 - val_loss: 0.0407\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0222 - val_loss: 0.0383\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0208 - val_loss: 0.0425\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0218 - val_loss: 0.0389\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: 0.0215 - val_loss: 0.0579\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0219 - val_loss: 0.0396\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0218 - val_loss: 0.0410\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0206 - val_loss: 0.0385\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0211 - val_loss: 0.0387\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.0211 - val_loss: 0.0392\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0203 - val_loss: 0.0407\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.0215 - val_loss: 0.0412\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0206 - val_loss: 0.0487\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 4s 384us/step - loss: 0.0203 - val_loss: 0.0615\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 4s 392us/step - loss: 0.0198 - val_loss: 0.0392\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0198 - val_loss: 0.0416\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0202 - val_loss: 0.0420\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0203 - val_loss: 0.0366\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0201 - val_loss: 0.0382\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0199 - val_loss: 0.0430\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 5s 508us/step - loss: 0.0204 - val_loss: 0.0486\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 5s 552us/step - loss: 0.0204 - val_loss: 0.0475\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0194 - val_loss: 0.0366\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0200 - val_loss: 0.0545\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0191 - val_loss: 0.0375\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 7s 670us/step - loss: 0.0194 - val_loss: 0.0463\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 6s 604us/step - loss: 0.0195 - val_loss: 0.0401\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 6s 630us/step - loss: 0.0196 - val_loss: 0.0503\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 6s 574us/step - loss: 0.0201 - val_loss: 0.0366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 6s 602us/step - loss: 0.0193 - val_loss: 0.0408\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 6s 607us/step - loss: 0.0193 - val_loss: 0.0476\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.0190 - val_loss: 0.0414\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 5s 557us/step - loss: 0.0195 - val_loss: 0.0415\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0185 - val_loss: 0.0393\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0186 - val_loss: 0.0377\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0186 - val_loss: 0.0369\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0188 - val_loss: 0.0380\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 4s 386us/step - loss: 0.0185 - val_loss: 0.0364\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0190 - val_loss: 0.0464\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0178 - val_loss: 0.0373\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0183 - val_loss: 0.0363\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0182 - val_loss: 0.0388\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0184 - val_loss: 0.0459\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0184 - val_loss: 0.0502\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0177 - val_loss: 0.0364\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0176 - val_loss: 0.0422\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0176 - val_loss: 0.0376\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: 0.0177 - val_loss: 0.0378\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 4s 403us/step - loss: 0.0171 - val_loss: 0.0374\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0178 - val_loss: 0.0396\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0173 - val_loss: 0.0369\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 4s 396us/step - loss: 0.0174 - val_loss: 0.0355\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0171 - val_loss: 0.0387\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.0172 - val_loss: 0.0419\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0171 - val_loss: 0.0408\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 5s 534us/step - loss: 0.0173 - val_loss: 0.0420\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0175 - val_loss: 0.0524\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 4s 458us/step - loss: 0.0168 - val_loss: 0.0358\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0170 - val_loss: 0.0403\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0169 - val_loss: 0.0368\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0176 - val_loss: 0.0424\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0173 - val_loss: 0.0360\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0174 - val_loss: 0.0513\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0171 - val_loss: 0.0386\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0166 - val_loss: 0.0350\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.0176 - val_loss: 0.0379\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0172 - val_loss: 0.0377\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 4s 390us/step - loss: 0.0166 - val_loss: 0.0357\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0179 - val_loss: 0.0374\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0168 - val_loss: 0.0370\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0169 - val_loss: 0.0353\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 5s 546us/step - loss: 0.0168 - val_loss: 0.0349\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 5s 502us/step - loss: 0.0162 - val_loss: 0.0450\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 5s 550us/step - loss: 0.0161 - val_loss: 0.0485\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0162 - val_loss: 0.0349\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.0159 - val_loss: 0.0349\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0161 - val_loss: 0.0526\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.0162 - val_loss: 0.0523\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 5s 515us/step - loss: 0.0163 - val_loss: 0.0385\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 0.0161 - val_loss: 0.0490\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.0166 - val_loss: 0.0373\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 0.0159 - val_loss: 0.0345\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0163 - val_loss: 0.0354\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 6s 617us/step - loss: 0.0158 - val_loss: 0.0418\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 440us/step - loss: 0.0157 - val_loss: 0.0377\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 4s 457us/step - loss: 0.0160 - val_loss: 0.0409\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0157 - val_loss: 0.0359\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.0160 - val_loss: 0.0341\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.0157 - val_loss: 0.0397\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: 0.0154 - val_loss: 0.0354\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0156 - val_loss: 0.0478\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: 0.0156 - val_loss: 0.0345\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: 0.0152 - val_loss: 0.0415\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0157 - val_loss: 0.0352\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0154 - val_loss: 0.0350\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 0.0149 - val_loss: 0.0368\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0149 - val_loss: 0.0340\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0152 - val_loss: 0.0358\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.0156 - val_loss: 0.0357\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 4s 400us/step - loss: 0.0152 - val_loss: 0.0373\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0149 - val_loss: 0.0368\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0151 - val_loss: 0.0349\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0147 - val_loss: 0.0394\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: 0.0151 - val_loss: 0.0352\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0157 - val_loss: 0.0349\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 4s 384us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 4s 449us/step - loss: 0.0151 - val_loss: 0.0410\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0153 - val_loss: 0.0358\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: 0.0147 - val_loss: 0.0358\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0145 - val_loss: 0.0418\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0146 - val_loss: 0.0357\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0147 - val_loss: 0.0372\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0146 - val_loss: 0.0364\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0143 - val_loss: 0.0354\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0147 - val_loss: 0.0393\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0147 - val_loss: 0.0358\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0143 - val_loss: 0.0344\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.0149 - val_loss: 0.0363\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0145 - val_loss: 0.0343\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0143 - val_loss: 0.0405\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0144 - val_loss: 0.0375\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0143 - val_loss: 0.0368\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.0144 - val_loss: 0.0403\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
