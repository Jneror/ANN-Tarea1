{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<H3 align='center'>  Jorge Portilla / John Rodriguez </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Construya un dataframe con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16242, 1276)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1276 entries, 0 to Eat\n",
      "dtypes: float64(1276)\n",
      "memory usage: 158.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "df.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total = len(df)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "\n",
    "dfTrain = df[:int(0.6*total)]\n",
    "dfVal = df[int(0.6*total):int(0.85*total)]    \n",
    "dfTest = df[int(0.85*total)::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos corresponden a un csv compuesto de 1276 columnas correspondientes a las variables, y 16242 filas que corresponden a las diferentes simulaciones.\n",
    "\n",
    "Las columnas 'Unnamed: 0' y 'pubchem_id' se eliminan por no tener datos relevante para los conjuntos de entrenamiento, validación y testeo. 'Unnamed: 0' no es necesaria debido a que corresponde a una enumeración, y el dataframe de panda ya viene con una incluída. 'pubchem_id' no es necesaria tampoco pues corresponde a la ID de la simulación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. Explique por qué se aconseja dicho preprocesamiento**\n",
    "\n",
    "Para un mejor funcionamiento de los algoritmos de Machine learning, hay que normalizar las variables de entrada del algoritmo, Normalizar, hace referencia a extender o comprimir los valores de una variable para estar en un rango definido. Es decir, realiza una ponderación de las caracterisiticas de una mejor manera y ademas se reduce el facor de escala. Sin embargo, realizar una mala eleccion del metodo de normalización puede alterar los resultados del analisis de datos. En este caso, se realiza una estandarización de los datos, debido a que si usamos normalización los outlayers podrían causar un desbalance de los datos en el intervalo, haciendo que muchos de ellos se agrupen cerca del 0.0 y que el outlayer quede como -1 o 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing#\n",
    "\n",
    "#x var's\n",
    "xTrain = pd.DataFrame.copy(dfTrain)\n",
    "xVal = pd.DataFrame.copy(dfVal)\n",
    "xTest = pd.DataFrame.copy(dfTest)\n",
    "\n",
    "#y var's\n",
    "yTrain = xTrain.pop('Eat').values.reshape(-1, 1)\n",
    "yVal = xVal.pop('Eat').values.reshape(-1, 1)\n",
    "yTest = xTest.pop('Eat').values.reshape(-1, 1)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler().fit(xTrain)\n",
    "\n",
    "#All standarized\n",
    "xTrainScaled = pd.DataFrame(scaler.transform(xTrain),columns=xTrain.columns)\n",
    "xValScaled = pd.DataFrame(scaler.transform(xVal),columns=xVal.columns)\n",
    "xTestScaled = pd.DataFrame(scaler.transform(xTest),columns=xTest.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Muestre en un gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de *epochs* de entrenamiento, para una red *feedforward* de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrene la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un la red feedward de 3 capas como se especifica, para luego graficar el error obtenido en el entrenamiento y en las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 250\n",
    "test_loss = np.zeros(numEpochs)\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss = self.model.evaluate(x, y, verbose=0)\n",
    "        test_loss[epoch-1] = loss\n",
    "        print('\\nTesting loss: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 6s 626us/step - loss: 1.4313 - val_loss: 0.6044\n",
      "\n",
      "Testing loss: 0.5724473738027098\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: 0.6157 - val_loss: 0.4303\n",
      "\n",
      "Testing loss: 0.4305726061920969\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.5055 - val_loss: 0.3991\n",
      "\n",
      "Testing loss: 0.40601520367795246\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.4305 - val_loss: 0.4110\n",
      "\n",
      "Testing loss: 0.3675703509058383\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.3684 - val_loss: 0.3072\n",
      "\n",
      "Testing loss: 0.24977736251339297\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.3210 - val_loss: 0.2717\n",
      "\n",
      "Testing loss: 0.23468143503637554\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.2817 - val_loss: 0.2685\n",
      "\n",
      "Testing loss: 0.2246224918403878\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 578us/step - loss: 0.2484 - val_loss: 0.2831\n",
      "\n",
      "Testing loss: 0.2594160707602503\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 6s 577us/step - loss: 0.2206 - val_loss: 0.2691\n",
      "\n",
      "Testing loss: 0.18525364892584015\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.1960 - val_loss: 0.2304\n",
      "\n",
      "Testing loss: 0.18367131335365025\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 5s 499us/step - loss: 0.1754 - val_loss: 0.1669\n",
      "\n",
      "Testing loss: 0.1301412959099672\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.1596 - val_loss: 0.1654\n",
      "\n",
      "Testing loss: 0.12926549163933998\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.1425 - val_loss: 0.1793\n",
      "\n",
      "Testing loss: 0.1298859344461339\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.1292 - val_loss: 0.1413\n",
      "\n",
      "Testing loss: 0.08700618668274773\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: 0.1199 - val_loss: 0.1301\n",
      "\n",
      "Testing loss: 0.08044609221468145\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 6s 565us/step - loss: 0.1102 - val_loss: 0.1254\n",
      "\n",
      "Testing loss: 0.09844637303553891\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 4s 455us/step - loss: 0.1016 - val_loss: 0.1183\n",
      "\n",
      "Testing loss: 0.08484515522224129\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.0970 - val_loss: 0.1887\n",
      "\n",
      "Testing loss: 0.1521987600934569\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 5s 474us/step - loss: 0.0904 - val_loss: 0.1086\n",
      "\n",
      "Testing loss: 0.07165930820969123\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.0850 - val_loss: 0.0965\n",
      "\n",
      "Testing loss: 0.06043838055641572\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0806 - val_loss: 0.0923\n",
      "\n",
      "Testing loss: 0.0673291493508358\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0774 - val_loss: 0.0901\n",
      "\n",
      "Testing loss: 0.06919519554336001\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 4s 441us/step - loss: 0.0753 - val_loss: 0.0913\n",
      "\n",
      "Testing loss: 0.06858113781840625\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 5s 496us/step - loss: 0.0705 - val_loss: 0.0782\n",
      "\n",
      "Testing loss: 0.04976735095148405\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0690 - val_loss: 0.0969\n",
      "\n",
      "Testing loss: 0.08321633649730859\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0670 - val_loss: 0.1139\n",
      "\n",
      "Testing loss: 0.1103766490359457\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 5s 496us/step - loss: 0.0650 - val_loss: 0.0745\n",
      "\n",
      "Testing loss: 0.047473996009404024\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 5s 533us/step - loss: 0.0618 - val_loss: 0.0785\n",
      "\n",
      "Testing loss: 0.054386934970960685\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0595 - val_loss: 0.1087\n",
      "\n",
      "Testing loss: 0.09852817852721138\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0590 - val_loss: 0.0857\n",
      "\n",
      "Testing loss: 0.051981980237111626\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0579 - val_loss: 0.0793\n",
      "\n",
      "Testing loss: 0.05942253020328726\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0548 - val_loss: 0.0881\n",
      "\n",
      "Testing loss: 0.05857679194659545\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0550 - val_loss: 0.0665\n",
      "\n",
      "Testing loss: 0.04748331325546757\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 5s 467us/step - loss: 0.0534 - val_loss: 0.0633\n",
      "\n",
      "Testing loss: 0.041694657024872024\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: 0.0529 - val_loss: 0.0642\n",
      "\n",
      "Testing loss: 0.044138974444784\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 4s 386us/step - loss: 0.0520 - val_loss: 0.0665\n",
      "\n",
      "Testing loss: 0.050554609639286555\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 4s 405us/step - loss: 0.0498 - val_loss: 0.0710\n",
      "\n",
      "Testing loss: 0.055061590588381414\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 4s 401us/step - loss: 0.0482 - val_loss: 0.0633\n",
      "\n",
      "Testing loss: 0.047038898706607365\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 4s 383us/step - loss: 0.0484 - val_loss: 0.0579\n",
      "\n",
      "Testing loss: 0.03989676740761903\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0490 - val_loss: 0.0579\n",
      "\n",
      "Testing loss: 0.038836849958108106\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 4s 454us/step - loss: 0.0460 - val_loss: 0.0747\n",
      "\n",
      "Testing loss: 0.045968500466976786\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0457 - val_loss: 0.0644\n",
      "\n",
      "Testing loss: 0.04073130578806973\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 4s 452us/step - loss: 0.0454 - val_loss: 0.0582\n",
      "\n",
      "Testing loss: 0.04198681549992677\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 5s 478us/step - loss: 0.0446 - val_loss: 0.0538\n",
      "\n",
      "Testing loss: 0.037806302786735566\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0433 - val_loss: 0.1893\n",
      "\n",
      "Testing loss: 0.20393944114361037\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0439 - val_loss: 0.0958\n",
      "\n",
      "Testing loss: 0.09337080953658777\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 5s 470us/step - loss: 0.0418 - val_loss: 0.0591\n",
      "\n",
      "Testing loss: 0.04159791232038948\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 6s 653us/step - loss: 0.0429 - val_loss: 0.0560\n",
      "\n",
      "Testing loss: 0.0370132219467494\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0411 - val_loss: 0.0525\n",
      "\n",
      "Testing loss: 0.03815267005133541\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0404 - val_loss: 0.0581\n",
      "\n",
      "Testing loss: 0.04234772046533431\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 5s 509us/step - loss: 0.0401 - val_loss: 0.0505\n",
      "\n",
      "Testing loss: 0.035524843411981005\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.0395 - val_loss: 0.0522\n",
      "\n",
      "Testing loss: 0.04211284201416413\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 5s 525us/step - loss: 0.0380 - val_loss: 0.0517\n",
      "\n",
      "Testing loss: 0.03548875979461604\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0380 - val_loss: 0.0563\n",
      "\n",
      "Testing loss: 0.03807050990040888\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 5s 495us/step - loss: 0.0383 - val_loss: 0.0758\n",
      "\n",
      "Testing loss: 0.07582590161262108\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: 0.0363 - val_loss: 0.0587\n",
      "\n",
      "Testing loss: 0.04234486023345515\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 5s 503us/step - loss: 0.0364 - val_loss: 0.0534\n",
      "\n",
      "Testing loss: 0.03709412052398903\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0362 - val_loss: 0.0554\n",
      "\n",
      "Testing loss: 0.037975050667189994\n",
      "Epoch 59/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 5s 489us/step - loss: 0.0365 - val_loss: 0.0492\n",
      "\n",
      "Testing loss: 0.034319982104611825\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0363 - val_loss: 0.0694\n",
      "\n",
      "Testing loss: 0.0499818189918787\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 5s 499us/step - loss: 0.0367 - val_loss: 0.0506\n",
      "\n",
      "Testing loss: 0.034187185804668785\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 5s 485us/step - loss: 0.0358 - val_loss: 0.0643\n",
      "\n",
      "Testing loss: 0.044643495670585884\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 4s 455us/step - loss: 0.0370 - val_loss: 0.0570\n",
      "\n",
      "Testing loss: 0.05251148103090109\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0350 - val_loss: 0.0540\n",
      "\n",
      "Testing loss: 0.041686621165551954\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0341 - val_loss: 0.0718\n",
      "\n",
      "Testing loss: 0.06052834684144634\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 4s 386us/step - loss: 0.0340 - val_loss: 0.0529\n",
      "\n",
      "Testing loss: 0.03448285737866364\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 4s 454us/step - loss: 0.0336 - val_loss: 0.0838\n",
      "\n",
      "Testing loss: 0.04869839863428998\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.0331 - val_loss: 0.0684\n",
      "\n",
      "Testing loss: 0.05570200003552104\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0320 - val_loss: 0.0964\n",
      "\n",
      "Testing loss: 0.10249354736936792\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0331 - val_loss: 0.0535\n",
      "\n",
      "Testing loss: 0.03816764718926694\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0318 - val_loss: 0.0563\n",
      "\n",
      "Testing loss: 0.051629859894501855\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 0.0322 - val_loss: 0.0661\n",
      "\n",
      "Testing loss: 0.06893089252218733\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 6s 572us/step - loss: 0.0321 - val_loss: 0.0441\n",
      "\n",
      "Testing loss: 0.033207219162062394\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 5s 530us/step - loss: 0.0317 - val_loss: 0.0443\n",
      "\n",
      "Testing loss: 0.03349352689749597\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0306 - val_loss: 0.0607\n",
      "\n",
      "Testing loss: 0.040373966782661454\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0308 - val_loss: 0.0444\n",
      "\n",
      "Testing loss: 0.03413386669072598\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 5s 514us/step - loss: 0.0309 - val_loss: 0.0450\n",
      "\n",
      "Testing loss: 0.03559340023889276\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 4s 458us/step - loss: 0.0299 - val_loss: 0.0609\n",
      "\n",
      "Testing loss: 0.03890426330357044\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0301 - val_loss: 0.0455\n",
      "\n",
      "Testing loss: 0.03486642203414885\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.0300 - val_loss: 0.0598\n",
      "\n",
      "Testing loss: 0.03941994465185902\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0300 - val_loss: 0.0452\n",
      "\n",
      "Testing loss: 0.03363626237437404\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 5s 514us/step - loss: 0.0296 - val_loss: 0.0467\n",
      "\n",
      "Testing loss: 0.03843768498950871\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0290 - val_loss: 0.0447\n",
      "\n",
      "Testing loss: 0.035400159933684935\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0300 - val_loss: 0.0437\n",
      "\n",
      "Testing loss: 0.0318879361677522\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0288 - val_loss: 0.0476\n",
      "\n",
      "Testing loss: 0.03567343590200975\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 5s 544us/step - loss: 0.0298 - val_loss: 0.0421\n",
      "\n",
      "Testing loss: 0.0336406554126402\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 4s 436us/step - loss: 0.0286 - val_loss: 0.0424\n",
      "\n",
      "Testing loss: 0.034703366375105224\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 5s 493us/step - loss: 0.0291 - val_loss: 0.0464\n",
      "\n",
      "Testing loss: 0.0417156901132586\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.0292 - val_loss: 0.0413\n",
      "\n",
      "Testing loss: 0.030812814715557232\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0280 - val_loss: 0.0487\n",
      "\n",
      "Testing loss: 0.04605512819636978\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 6s 604us/step - loss: 0.0290 - val_loss: 0.0426\n",
      "\n",
      "Testing loss: 0.034632667018636404\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 6s 631us/step - loss: 0.0279 - val_loss: 0.0674\n",
      "\n",
      "Testing loss: 0.05572305010248757\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0281 - val_loss: 0.0483\n",
      "\n",
      "Testing loss: 0.04393426250019766\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 5s 552us/step - loss: 0.0269 - val_loss: 0.0511\n",
      "\n",
      "Testing loss: 0.03989978970644119\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 5s 531us/step - loss: 0.0264 - val_loss: 0.0493\n",
      "\n",
      "Testing loss: 0.03409648428460492\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 5s 534us/step - loss: 0.0261 - val_loss: 0.0470\n",
      "\n",
      "Testing loss: 0.03658013897468357\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 4s 436us/step - loss: 0.0270 - val_loss: 0.0668\n",
      "\n",
      "Testing loss: 0.06679928719624419\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0271 - val_loss: 0.0417\n",
      "\n",
      "Testing loss: 0.030555332320290216\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.0256 - val_loss: 0.0394\n",
      "\n",
      "Testing loss: 0.030318811121574425\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.0268 - val_loss: 0.0568\n",
      "\n",
      "Testing loss: 0.05511277359916042\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 4s 417us/step - loss: 0.0262 - val_loss: 0.0507\n",
      "\n",
      "Testing loss: 0.04473516202555067\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 4s 429us/step - loss: 0.0259 - val_loss: 0.0387\n",
      "\n",
      "Testing loss: 0.03309102829241537\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 4s 400us/step - loss: 0.0254 - val_loss: 0.0407\n",
      "\n",
      "Testing loss: 0.03527162426904572\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 4s 384us/step - loss: 0.0267 - val_loss: 0.0386\n",
      "\n",
      "Testing loss: 0.030073259443130267\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.0256 - val_loss: 0.0547\n",
      "\n",
      "Testing loss: 0.03890156192792168\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0265 - val_loss: 0.0432\n",
      "\n",
      "Testing loss: 0.04158846223188024\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.0240 - val_loss: 0.0420\n",
      "\n",
      "Testing loss: 0.030552898179380485\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.0240 - val_loss: 0.0506\n",
      "\n",
      "Testing loss: 0.03407158550474247\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 5s 533us/step - loss: 0.0250 - val_loss: 0.0407\n",
      "\n",
      "Testing loss: 0.031238618743623874\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 6s 571us/step - loss: 0.0250 - val_loss: 0.0466\n",
      "\n",
      "Testing loss: 0.04472742920262211\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0241 - val_loss: 0.0510\n",
      "\n",
      "Testing loss: 0.04163640724081557\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: 0.0243 - val_loss: 0.0397\n",
      "\n",
      "Testing loss: 0.02958864041947663\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 4s 394us/step - loss: 0.0237 - val_loss: 0.0386\n",
      "\n",
      "Testing loss: 0.02953141262608813\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 0.0242 - val_loss: 0.0370\n",
      "\n",
      "Testing loss: 0.030600807890540566\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0235 - val_loss: 0.0470\n",
      "\n",
      "Testing loss: 0.036285976901081776\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.0239 - val_loss: 0.0457\n",
      "\n",
      "Testing loss: 0.03396711052977432\n",
      "Epoch 117/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0236 - val_loss: 0.0382\n",
      "\n",
      "Testing loss: 0.03021039634371298\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0231 - val_loss: 0.0438\n",
      "\n",
      "Testing loss: 0.034768549179689634\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.0224 - val_loss: 0.0434\n",
      "\n",
      "Testing loss: 0.03111838799537109\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0235 - val_loss: 0.0394\n",
      "\n",
      "Testing loss: 0.03082519530971788\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 6s 610us/step - loss: 0.0232 - val_loss: 0.0365\n",
      "\n",
      "Testing loss: 0.030736748299572524\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0236 - val_loss: 0.0378\n",
      "\n",
      "Testing loss: 0.031963985508088454\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0225 - val_loss: 0.0453\n",
      "\n",
      "Testing loss: 0.03772967126393201\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 6s 569us/step - loss: 0.0235 - val_loss: 0.0432\n",
      "\n",
      "Testing loss: 0.043115584594784186\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: 0.0224 - val_loss: 0.0369\n",
      "\n",
      "Testing loss: 0.031022480567844617\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0221 - val_loss: 0.0636\n",
      "\n",
      "Testing loss: 0.0429483891446044\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 0.0227 - val_loss: 0.0401\n",
      "\n",
      "Testing loss: 0.03713692962713647\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0220 - val_loss: 0.0374\n",
      "\n",
      "Testing loss: 0.03256842519233449\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0221 - val_loss: 0.0459\n",
      "\n",
      "Testing loss: 0.03500277370177775\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0231 - val_loss: 0.0366\n",
      "\n",
      "Testing loss: 0.030359377638178132\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 5s 536us/step - loss: 0.0219 - val_loss: 0.0391\n",
      "\n",
      "Testing loss: 0.029538029119231732\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 5s 523us/step - loss: 0.0219 - val_loss: 0.0369\n",
      "\n",
      "Testing loss: 0.02987371559362212\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 5s 513us/step - loss: 0.0216 - val_loss: 0.0412\n",
      "\n",
      "Testing loss: 0.0334575598662781\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 6s 597us/step - loss: 0.0212 - val_loss: 0.0422\n",
      "\n",
      "Testing loss: 0.03335881485287704\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0214 - val_loss: 0.0434\n",
      "\n",
      "Testing loss: 0.03521313734779079\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 0.0219 - val_loss: 0.0427\n",
      "\n",
      "Testing loss: 0.03111694281432391\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0218 - val_loss: 0.0367\n",
      "\n",
      "Testing loss: 0.029390905882473797\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0207 - val_loss: 0.0396\n",
      "\n",
      "Testing loss: 0.030151403086029902\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 5s 470us/step - loss: 0.0208 - val_loss: 0.0413\n",
      "\n",
      "Testing loss: 0.03438332600532055\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.0210 - val_loss: 0.0365\n",
      "\n",
      "Testing loss: 0.03065659381696862\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.0209 - val_loss: 0.0383\n",
      "\n",
      "Testing loss: 0.030000367025307055\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 0.0211 - val_loss: 0.0502\n",
      "\n",
      "Testing loss: 0.046048831300879535\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: 0.0206 - val_loss: 0.0382\n",
      "\n",
      "Testing loss: 0.029302348881489096\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.0202 - val_loss: 0.0444\n",
      "\n",
      "Testing loss: 0.04064513763560347\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0209 - val_loss: 0.0395\n",
      "\n",
      "Testing loss: 0.03433103277472184\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0207 - val_loss: 0.0740\n",
      "\n",
      "Testing loss: 0.056371965344305844\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 4s 458us/step - loss: 0.0200 - val_loss: 0.0362\n",
      "\n",
      "Testing loss: 0.028893499739489497\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.0198 - val_loss: 0.0355\n",
      "\n",
      "Testing loss: 0.03091617261667598\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0204 - val_loss: 0.0372\n",
      "\n",
      "Testing loss: 0.03263085614163757\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 5s 483us/step - loss: 0.0198 - val_loss: 0.0397\n",
      "\n",
      "Testing loss: 0.03867647416009172\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 5s 561us/step - loss: 0.0200 - val_loss: 0.0350\n",
      "\n",
      "Testing loss: 0.031068696857116287\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0200 - val_loss: 0.0372\n",
      "\n",
      "Testing loss: 0.02875874414550852\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 5s 489us/step - loss: 0.0192 - val_loss: 0.0344\n",
      "\n",
      "Testing loss: 0.028325321838400332\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0197 - val_loss: 0.0351\n",
      "\n",
      "Testing loss: 0.02794814936489564\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 5s 464us/step - loss: 0.0194 - val_loss: 0.0371\n",
      "\n",
      "Testing loss: 0.032155050476114916\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0197 - val_loss: 0.0409\n",
      "\n",
      "Testing loss: 0.040841090325136185\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0202 - val_loss: 0.0379\n",
      "\n",
      "Testing loss: 0.03109672193017635\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 5s 492us/step - loss: 0.0195 - val_loss: 0.0345\n",
      "\n",
      "Testing loss: 0.027888257998879885\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 4s 449us/step - loss: 0.0196 - val_loss: 0.0392\n",
      "\n",
      "Testing loss: 0.0315318569918551\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0188 - val_loss: 0.0433\n",
      "\n",
      "Testing loss: 0.036555750362015575\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0195 - val_loss: 0.0430\n",
      "\n",
      "Testing loss: 0.03217038812005344\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: 0.0194 - val_loss: 0.0347\n",
      "\n",
      "Testing loss: 0.029425354145549826\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.0189 - val_loss: 0.0411\n",
      "\n",
      "Testing loss: 0.032490067696065064\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.0192 - val_loss: 0.0359\n",
      "\n",
      "Testing loss: 0.02943813983866143\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 4s 440us/step - loss: 0.0191 - val_loss: 0.0371\n",
      "\n",
      "Testing loss: 0.03496069962041858\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 0.0193 - val_loss: 0.0348\n",
      "\n",
      "Testing loss: 0.027820762707731445\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 4s 442us/step - loss: 0.0185 - val_loss: 0.0377\n",
      "\n",
      "Testing loss: 0.029183021487853886\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 4s 390us/step - loss: 0.0190 - val_loss: 0.0403\n",
      "\n",
      "Testing loss: 0.03223840693400918\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0191 - val_loss: 0.0363\n",
      "\n",
      "Testing loss: 0.02894679161329793\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.0185 - val_loss: 0.0440\n",
      "\n",
      "Testing loss: 0.03929922033397876\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 4s 394us/step - loss: 0.0181 - val_loss: 0.0356\n",
      "\n",
      "Testing loss: 0.02786024140173422\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.0187 - val_loss: 0.0363\n",
      "\n",
      "Testing loss: 0.02818011135477412\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.0185 - val_loss: 0.0378\n",
      "\n",
      "Testing loss: 0.03570476005745805\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0181 - val_loss: 0.0372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing loss: 0.028655287316302125\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0185 - val_loss: 0.0487\n",
      "\n",
      "Testing loss: 0.04945908504116716\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0185 - val_loss: 0.0346\n",
      "\n",
      "Testing loss: 0.028653779148116898\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.0180 - val_loss: 0.0361\n",
      "\n",
      "Testing loss: 0.029249818138793494\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.0177 - val_loss: 0.0359\n",
      "\n",
      "Testing loss: 0.027514734827089025\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.0178 - val_loss: 0.0345\n",
      "\n",
      "Testing loss: 0.02805130897417732\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 4s 442us/step - loss: 0.0183 - val_loss: 0.0355\n",
      "\n",
      "Testing loss: 0.028749590978839626\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0176 - val_loss: 0.0338\n",
      "\n",
      "Testing loss: 0.02776078171233173\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.0173 - val_loss: 0.0392\n",
      "\n",
      "Testing loss: 0.03316395802084765\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 4s 439us/step - loss: 0.0175 - val_loss: 0.0352\n",
      "\n",
      "Testing loss: 0.029465134083234142\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.0183 - val_loss: 0.0340\n",
      "\n",
      "Testing loss: 0.02695199811489466\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 5s 510us/step - loss: 0.0175 - val_loss: 0.0472\n",
      "\n",
      "Testing loss: 0.04897560439942726\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0178 - val_loss: 0.0373\n",
      "\n",
      "Testing loss: 0.032771477138697734\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 5s 464us/step - loss: 0.0175 - val_loss: 0.0370\n",
      "\n",
      "Testing loss: 0.03354796975970293\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0167 - val_loss: 0.0346\n",
      "\n",
      "Testing loss: 0.027643036060600826\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 4s 455us/step - loss: 0.0178 - val_loss: 0.0476\n",
      "\n",
      "Testing loss: 0.03255046079310546\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0174 - val_loss: 0.0385\n",
      "\n",
      "Testing loss: 0.0335624327865545\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.0172 - val_loss: 0.0334\n",
      "\n",
      "Testing loss: 0.026839939314852155\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0170 - val_loss: 0.0348\n",
      "\n",
      "Testing loss: 0.02719011898070366\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0173 - val_loss: 0.0410\n",
      "\n",
      "Testing loss: 0.030732147068291242\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0174 - val_loss: 0.0328\n",
      "\n",
      "Testing loss: 0.02821647435744669\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0178 - val_loss: 0.0385\n",
      "\n",
      "Testing loss: 0.02942411954257489\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.0168 - val_loss: 0.0379\n",
      "\n",
      "Testing loss: 0.029903008159636767\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0173 - val_loss: 0.0358\n",
      "\n",
      "Testing loss: 0.02727897032089516\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 4s 453us/step - loss: 0.0170 - val_loss: 0.0352\n",
      "\n",
      "Testing loss: 0.02833823221256046\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 5s 504us/step - loss: 0.0170 - val_loss: 0.0358\n",
      "\n",
      "Testing loss: 0.027091302618515823\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.0169 - val_loss: 0.0371\n",
      "\n",
      "Testing loss: 0.02846204850877718\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0166 - val_loss: 0.0361\n",
      "\n",
      "Testing loss: 0.02756519736756506\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0165 - val_loss: 0.0413\n",
      "\n",
      "Testing loss: 0.03432140170237692\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 4s 437us/step - loss: 0.0167 - val_loss: 0.0360\n",
      "\n",
      "Testing loss: 0.0315334352507582\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.0164 - val_loss: 0.0351\n",
      "\n",
      "Testing loss: 0.030661314403333282\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0172 - val_loss: 0.0387\n",
      "\n",
      "Testing loss: 0.031194209905741007\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 5s 502us/step - loss: 0.0161 - val_loss: 0.0348\n",
      "\n",
      "Testing loss: 0.02862526044695897\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 0.0162 - val_loss: 0.0385\n",
      "\n",
      "Testing loss: 0.032827659151228324\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0167 - val_loss: 0.0321\n",
      "\n",
      "Testing loss: 0.026788936765114693\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0163 - val_loss: 0.0351\n",
      "\n",
      "Testing loss: 0.03201588001109852\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0161 - val_loss: 0.0355\n",
      "\n",
      "Testing loss: 0.02998346661186556\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 5s 495us/step - loss: 0.0166 - val_loss: 0.0473\n",
      "\n",
      "Testing loss: 0.032360476498432066\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0163 - val_loss: 0.0428\n",
      "\n",
      "Testing loss: 0.032453348541423635\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0160 - val_loss: 0.0398\n",
      "\n",
      "Testing loss: 0.02989231812804754\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 5s 489us/step - loss: 0.0160 - val_loss: 0.0328\n",
      "\n",
      "Testing loss: 0.026089996536280295\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 5s 464us/step - loss: 0.0165 - val_loss: 0.0376\n",
      "\n",
      "Testing loss: 0.027368429904917516\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 5s 479us/step - loss: 0.0163 - val_loss: 0.0350\n",
      "\n",
      "Testing loss: 0.030332651073369132\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0156 - val_loss: 0.0375\n",
      "\n",
      "Testing loss: 0.02992900775131763\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0157 - val_loss: 0.0350\n",
      "\n",
      "Testing loss: 0.026562422141165252\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0157 - val_loss: 0.0461\n",
      "\n",
      "Testing loss: 0.03385660336645237\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 5s 479us/step - loss: 0.0158 - val_loss: 0.0399\n",
      "\n",
      "Testing loss: 0.029621584250740355\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 5s 538us/step - loss: 0.0161 - val_loss: 0.0341\n",
      "\n",
      "Testing loss: 0.02656251292830126\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 6s 593us/step - loss: 0.0158 - val_loss: 0.0341\n",
      "\n",
      "Testing loss: 0.028175526417434142\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.0157 - val_loss: 0.0339\n",
      "\n",
      "Testing loss: 0.028959509911635494\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 4s 433us/step - loss: 0.0154 - val_loss: 0.0376\n",
      "\n",
      "Testing loss: 0.030019392355340985\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0167 - val_loss: 0.0367\n",
      "\n",
      "Testing loss: 0.027870503321494853\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0154 - val_loss: 0.0387\n",
      "\n",
      "Testing loss: 0.029847509354187857\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 6s 627us/step - loss: 0.0153 - val_loss: 0.0347\n",
      "\n",
      "Testing loss: 0.027015456617960783\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0150 - val_loss: 0.0349\n",
      "\n",
      "Testing loss: 0.026284135962925486\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.0157 - val_loss: 0.0374\n",
      "\n",
      "Testing loss: 0.028369664895368504\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 5s 555us/step - loss: 0.0154 - val_loss: 0.0344\n",
      "\n",
      "Testing loss: 0.026786272633579248\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0153 - val_loss: 0.0335\n",
      "\n",
      "Testing loss: 0.028585259543917538\n",
      "Epoch 232/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 5s 475us/step - loss: 0.0150 - val_loss: 0.0345\n",
      "\n",
      "Testing loss: 0.02669857423500321\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0150 - val_loss: 0.0608\n",
      "\n",
      "Testing loss: 0.04359276318493856\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0153 - val_loss: 0.0387\n",
      "\n",
      "Testing loss: 0.03427487892428528\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 6s 622us/step - loss: 0.0149 - val_loss: 0.0408\n",
      "\n",
      "Testing loss: 0.029138103837149647\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 6s 594us/step - loss: 0.0153 - val_loss: 0.0343\n",
      "\n",
      "Testing loss: 0.02614025501736887\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0157 - val_loss: 0.0499\n",
      "\n",
      "Testing loss: 0.03424548773896782\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 6s 581us/step - loss: 0.0154 - val_loss: 0.0351\n",
      "\n",
      "Testing loss: 0.027773399068028115\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 6s 574us/step - loss: 0.0152 - val_loss: 0.0354\n",
      "\n",
      "Testing loss: 0.02631298732891477\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.0146 - val_loss: 0.0362\n",
      "\n",
      "Testing loss: 0.026256068463096756\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 5s 496us/step - loss: 0.0152 - val_loss: 0.0385\n",
      "\n",
      "Testing loss: 0.028894359360544172\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0151 - val_loss: 0.0355\n",
      "\n",
      "Testing loss: 0.026739872233834384\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0153 - val_loss: 0.0344\n",
      "\n",
      "Testing loss: 0.027180764574601873\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0151 - val_loss: 0.0345\n",
      "\n",
      "Testing loss: 0.026430505021006346\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "\n",
      "Testing loss: 0.025543267442930794\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 4s 440us/step - loss: 0.0142 - val_loss: 0.0339\n",
      "\n",
      "Testing loss: 0.026402830854012408\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 6s 577us/step - loss: 0.0142 - val_loss: 0.0351\n",
      "\n",
      "Testing loss: 0.02635187203057682\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0146 - val_loss: 0.0347\n",
      "\n",
      "Testing loss: 0.026484069813029122\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0142 - val_loss: 0.0357\n",
      "\n",
      "Testing loss: 0.028138919410539986\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 6s 566us/step - loss: 0.0146 - val_loss: 0.0399\n",
      "\n",
      "Testing loss: 0.03068482274763595\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=xTrainScaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\"))\n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error') #, metrics=['accuracy']\n",
    "\n",
    "history = model.fit(xTrainScaled.values,\n",
    "                    yTrain,\n",
    "                    epochs=numEpochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(xValScaled.values, yVal), \n",
    "                    callbacks=[TestCallback((xTestScaled.values, yTest))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3564f48f7be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "xc = range(numEpochs)\n",
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de epochs de entrenamiento, para una red feedforward de 3 capas, con 256 unidades ocultas y **función de activación sigmoidal**. Entrenada la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#El último valor del testing siempre venía erroneo, por lo que se reemplaza con el correcto\n",
    "#evaluando el modelo con 250 epochs (modelo final)\n",
    "test_loss[-1] = model.evaluate(xTestScaled.values, yTest, verbose=0)\n",
    "train_loss=history.history['loss']\n",
    "xc = range(numEpochs)\n",
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,test_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.title('Training Loss vs Testing Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la función de activación sigmoidal, podemos ver que el error va decreciendo a medida que se aumenta el número de epochs. Por lo tanto este metodo converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Faltan comentarios sobre DIVERGENCIA !!!**\n",
    "\n",
    "<img src=\"data:image/png;base64,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\" title=\"Title text\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Repita el paso anterior, utilizado ’ReLU’ como función de activación y compare con lo obtenido en b). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(256, input_dim=xTrainScaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model2.add(Dense(1, kernel_initializer='uniform',activation=\"linear\"))\n",
    "model2.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error') #, metrics=['accuracy']\n",
    "\n",
    "history = model2.fit(xTrainScaled.values,\n",
    "                    yTrain,\n",
    "                    epochs=numEpochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(xValScaled.values, yVal), \n",
    "                    callbacks=[TestCallback((xTestScaled.values, yTest))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loss[-1] = model2.evaluate(xTestScaled.values, yTest, verbose=0)\n",
    "train_loss = history.history['loss']\n",
    "xc = range(numEpochs)\n",
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,test_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.title('Training Loss vs Testing Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "#print(plt.style.available)\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Repita el paso anterior, utilizado ’**ReLU**’ como función de activación y compare con lo obtenido en b).  \n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "#%%El gráfico el error cuadrático (MSE) \n",
    "import matplotlib.pyplot as plt\n",
    "train_loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "xc = range(250)\n",
    "plt.figure(1, figsize=(8,6))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Num of epochs')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['train', 'val'])\n",
    "#print (plt.style.available)\n",
    "plt.style.use(['ggplot'])\n",
    "#%%\n",
    "```\n",
    "\n",
    ">El gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de epochs de entrenamiento, para una red feedforward de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrenada la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación.\n",
    "\n",
    ">**Faltan comentarios!!!**\n",
    ">Modificando solo esas lineas,este codigo:\n",
    "```python\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "```\n",
    "<img src=\"data:image/png;base64,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\" title=\"Title text\" width=\"40%\"/>\n",
    "\n",
    "\n",
    ">Con este codigo:\n",
    "```python\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"relu\")) \n",
    "```\n",
    "<img src=\"data:image/png;base64,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\" title=\"Title text\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Repita b) y c) variando la tasa de aprendizaje (*learning rate*) en un rango sensible. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateOverLearningRate(n_lr, activ):\n",
    "    lear_rate = np.linspace(0,1,n_lr)\n",
    "    histories = np.ndarray((20, ), object)\n",
    "\n",
    "    for i in range (n_lr):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=xTrainScaled.shape[1], kernel_initializer='uniform',activation=activ))\n",
    "        model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\"))\n",
    "        model.compile(optimizer=SGD(lear_rate[i]),loss='mean_squared_error')\n",
    "        histories[i] = model.fit(xTrainScaled.values,\n",
    "                        yTrain,\n",
    "                        epochs=250,\n",
    "                        verbose=1,\n",
    "                        validation_data=(xValScaled.values, yVal), \n",
    "                        callbacks=[TestCallback((xTestScaled.values, yTest))])\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, variamos la tasa de aprendizaje del metodo en b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: 137.0050\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 2/250\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: 137.2623\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 3/250\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: 136.8721\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 4/250\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: 137.2969\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 5/250\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: 137.1578\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 6/250\n",
      "9408/9745 [===========================>..] - ETA: 0s - loss: 137.7898\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 7/250\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: 137.2639\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 8/250\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: 137.4600\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 9/250\n",
      "9536/9745 [============================>.] - ETA: 0s - loss: 137.1992\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 10/250\n",
      "9728/9745 [============================>.] - ETA: 0s - loss: 137.1700\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 11/250\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: 137.1284\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 12/250\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: 137.0704\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 13/250\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: 137.2811\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 14/250\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: 137.1422\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 15/250\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: 137.3458\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 16/250\n",
      "9728/9745 [============================>.] - ETA: 0s - loss: 137.2561\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 17/250\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: 137.1759\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 18/250\n",
      "9408/9745 [===========================>..] - ETA: 0s - loss: 137.2281\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 19/250\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: 137.2766\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 20/250\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: 137.0104\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 21/250\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: 137.1452\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 22/250\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: 137.2186\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 23/250\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: 137.2402\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 24/250\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: 137.1569\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 25/250\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: 137.1500\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 26/250\n",
      "9312/9745 [===========================>..] - ETA: 0s - loss: 137.2427\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 27/250\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: 137.2504\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 28/250\n",
      "9408/9745 [===========================>..] - ETA: 0s - loss: 137.1146\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 29/250\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: 137.0919\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 30/250\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: 136.8415\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 31/250\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: 137.1537\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 32/250\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: 137.3793\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 33/250\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: 137.1964\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 34/250\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: 137.2391\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 35/250\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: 137.3011\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 36/250\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: 137.2950\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 37/250\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: 137.3802\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 38/250\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: 137.2470\n",
      "Testing loss: 122.1185209628838\n",
      "9745/9745 [==============================] - 1s - loss: 137.2130 - val_loss: 157.2504\n",
      "Epoch 39/250\n",
      "2112/9745 [=====>........................] - ETA: 0s - loss: 136.6306"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-af951dbd2778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterateOverLearningRate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-a4f5050b9163>\u001b[0m in \u001b[0;36miterateOverLearningRate\u001b[0;34m(n_lr, activ)\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxValScaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                         callbacks=[TestCallback((xTestScaled.values, yTest))])\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "histories = iterateOverLearningRate(20, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora variamos la tasa de aprendizaje del metodo en c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histories = iterateOverLearningRate(20, \"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Entrene los modelos considerados en b) y c) usando *progressive decay*. Compare y comente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterateOverDecay(n_decay, activ):\n",
    "    lear_decay = np.logspace(-6,0,n_decay)\n",
    "    histories = np.ndarray((n_decay, ), object)\n",
    "    for i in range (n_decay):\n",
    "        sgd = SGD(0.2, lear_decay[i])\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=xTrainScaled.shape[1], kernel_initializer='uniform',activation=activ))\n",
    "        model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\"))\n",
    "        model.compile(optimizer= sgd,loss='mean_squared_error') #, metrics=['accuracy'] \n",
    "        histories[i] = model.fit(xTrainScaled.values,\n",
    "                        yTrain,\n",
    "                        epochs=20,\n",
    "                        verbose=1,\n",
    "                        validation_data=(xValScaled.values, yVal), \n",
    "                        callbacks=[TestCallback((xTestScaled.values, yTest))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varios el progressive decay para el metodo en b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/20\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "9408/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "9536/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "9728/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "9344/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "9728/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/20\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "9504/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "9344/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "9536/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "7200/9745 [=====================>........] - ETA: 0s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-614b9deaa9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterateOverDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-bde113318caa>\u001b[0m in \u001b[0;36miterateOverDecay\u001b[0;34m(n_decay, activ)\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxValScaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                         callbacks=[TestCallback((xTestScaled.values, yTest))])\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "histories = iterateOverDecay(10, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "9376/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "9504/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "9472/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "9664/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "9728/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "9600/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "9280/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "9248/9745 [===========================>..] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/20\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "9632/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "9568/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "9440/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "9696/9745 [============================>.] - ETA: 0s - loss: nan\n",
      "Testing loss: nan\n",
      "9745/9745 [==============================] - 1s - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "3776/9745 [==========>...................] - ETA: 0s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-10374c300c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterateOverDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-bde113318caa>\u001b[0m in \u001b[0;36miterateOverDecay\u001b[0;34m(n_decay, activ)\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxValScaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                         callbacks=[TestCallback((xTestScaled.values, yTest))])\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/ANN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "histories = iterateOverDecay(10, \"relu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
